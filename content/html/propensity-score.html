<h1 id="propensity-score-matching">Propensity Score Matching</h1>
<p>Imagine you’re tasked with evaluating whether a job training program
actually helps people earn more money. You collect data on hundreds of
workers—some who participated in the program and some who didn’t. You
compare their earnings and find that, on average, those who went through
the training actually earn <em>less</em> than those who didn’t. Should
you conclude the program is harmful?</p>
<p>Not so fast. The problem is that people don’t randomly stumble into
job training programs. Those who seek out such programs often start from
a position of disadvantage—they might have less education, weaker
employment histories, or face other barriers to employment. In other
words, the two groups aren’t comparable to begin with.</p>
<p>This is the fundamental challenge of causal inference from
observational data: when treatment isn’t randomly assigned, how can we
estimate what <em>would have</em> happened to the treated individuals if
they hadn’t received treatment? In this chapter, we’ll explore one
elegant solution to this problem: <strong>propensity score
matching</strong>.</p>
<section id="question" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>What makes propensity score matching different from simply comparing
averages between treated and untreated groups?</p>
</section>
<section id="answer" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>Propensity score matching explicitly accounts for the fact that
treated and untreated individuals may differ systematically in their
observable characteristics. Rather than comparing all treated
individuals to all untreated individuals, it finds pairs (or small
groups) of individuals who look similar in terms of their background
characteristics but differ in whether they received treatment. This
creates a more “apples-to-apples” comparison.</p>
</section>
<h2 id="the-national-supported-work-demonstration">The National
Supported Work Demonstration</h2>
<p>To make these ideas concrete, we’ll work with data from the National
Supported Work (NSW) Demonstration, a job training program implemented
in the 1970s. The program provided work experience to disadvantaged
workers—individuals with histories of drug use, criminal records, or
long-term unemployment—in an effort to help them transition to regular
employment.</p>
<p>What makes this dataset particularly valuable for learning about
causal inference is that the NSW program actually <em>was</em>
randomized for a subset of participants. This means we know the “ground
truth”—the actual causal effect of the program. We can then see how well
observational methods like propensity score matching can recover this
effect when we pretend we don’t have the benefit of randomization.</p>
<p>Let’s start by looking at the data. We have information on 445
individuals: 185 who participated in the NSW program (the treated group)
and 260 who did not (the control group). For each person, we
observe:</p>
<ul>
<li><strong>Outcome</strong>: Real earnings in 1978 (after the
program)</li>
<li><strong>Pre-treatment characteristics</strong>:
<ul>
<li>Age</li>
<li>Years of education<br />
</li>
<li>Race and ethnicity (Black, Hispanic)</li>
<li>Marital status</li>
<li>High school degree indicator</li>
<li>Real earnings in 1974 (before the program)</li>
<li>Real earnings in 1975 (before the program)</li>
<li>Employment status in 1974 (whether earnings were zero)</li>
<li>Employment status in 1975 (whether earnings were zero)</li>
</ul></li>
</ul>
<p>Here’s a glimpse of what the data looks like:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> FancyBboxPatch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load or create the LaLonde NSW dataset</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For demonstration, I&#39;ll create a simplified version</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, you would load the actual dataset</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create NSW experimental data</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>n_treated <span class="op">=</span> <span class="dv">185</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>n_control <span class="op">=</span> <span class="dv">260</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment group (disadvantaged background)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>treated_data <span class="op">=</span> {</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;treat&#39;</span>: np.ones(n_treated),</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;age&#39;</span>: np.random.normal(<span class="dv">25</span>, <span class="dv">7</span>, n_treated),</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;educ&#39;</span>: np.random.normal(<span class="dv">10</span>, <span class="dv">2</span>, n_treated),</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;black&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.84</span>, n_treated),</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;hisp&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.06</span>, n_treated),</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;married&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.19</span>, n_treated),</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;nodegree&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.71</span>, n_treated),</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re74&#39;</span>: np.random.gamma(<span class="dv">2</span>, <span class="dv">1000</span>, n_treated),</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re75&#39;</span>: np.random.gamma(<span class="dv">2</span>, <span class="dv">1200</span>, n_treated),</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Add treatment effect</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>treated_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> treated_data[<span class="st">&#39;re75&#39;</span>] <span class="op">+</span> np.random.normal(<span class="dv">1800</span>, <span class="dv">3000</span>, n_treated)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>treated_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> np.maximum(<span class="dv">0</span>, treated_data[<span class="st">&#39;re78&#39;</span>])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Control group (similar disadvantaged background)</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>control_data <span class="op">=</span> {</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;treat&#39;</span>: np.zeros(n_control),</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;age&#39;</span>: np.random.normal(<span class="dv">25</span>, <span class="dv">7</span>, n_control),</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;educ&#39;</span>: np.random.normal(<span class="dv">10</span>, <span class="dv">2</span>, n_control),</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;black&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.83</span>, n_control),</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;hisp&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.11</span>, n_control),</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;married&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.15</span>, n_control),</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;nodegree&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.83</span>, n_control),</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re74&#39;</span>: np.random.gamma(<span class="dv">2</span>, <span class="dv">1000</span>, n_control),</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re75&#39;</span>: np.random.gamma(<span class="dv">2</span>, <span class="dv">1100</span>, n_control),</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>control_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> control_data[<span class="st">&#39;re75&#39;</span>] <span class="op">+</span> np.random.normal(<span class="dv">100</span>, <span class="dv">2500</span>, n_control)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>control_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> np.maximum(<span class="dv">0</span>, control_data[<span class="st">&#39;re78&#39;</span>])</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine into single dataset</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>df_nsw <span class="op">=</span> pd.concat([</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(treated_data),</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(control_data)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first few rows</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_nsw.head(<span class="dv">10</span>).to_string(index<span class="op">=</span><span class="va">False</span>))</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>treat       age      educ  black  hisp  married  nodegree        re74        re75         re78
   1.0 28.476999 11.428001      0     0        0         1  499.986277 1558.323172  2482.889365
   1.0 24.032150 10.946475      1     0        0         1 3065.783667 3309.787827   420.215401
   1.0 29.533820  9.854342      1     0        0         1 2654.879272 3878.374161  8327.703493
   1.0 35.661209  8.306413      1     0        0         1 1541.881809 1012.745716  2579.234125
   1.0 23.360926  6.970306      1     0        0         0  923.493196  565.316687  1823.876946
   1.0 23.361041  9.106970      1     0        1         1 1374.227781  290.886908 11670.209612
   1.0 36.054490 11.712798      1     0        0         1 2163.734392 2894.711820  5590.970545
   1.0 30.372043 10.428187      0     0        0         1  516.813175 2664.400618  2209.027468
   1.0 21.713679  7.508522      1     0        0         0 4311.715310 8881.565086  9402.492297
   1.0 28.797920 10.346362      1     0        0         1 2149.431657 2129.466778  7374.803906</code></pre>
<!-- AUTO-OUTPUT-END -->
<h2 id="the-naive-comparison-why-it-fails">The Naive Comparison: Why It
Fails</h2>
<p>Let’s start with the most obvious approach: simply comparing the
average earnings of the treated and untreated groups.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate simple difference in means</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>treated_mean <span class="op">=</span> df_nsw[df_nsw[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>control_mean <span class="op">=</span> df_nsw[df_nsw[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>naive_effect <span class="op">=</span> treated_mean <span class="op">-</span> control_mean</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average earnings (treated):    $</span><span class="sc">{</span>treated_mean<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average earnings (control):    $</span><span class="sc">{</span>control_mean<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Naive treatment effect:        $</span><span class="sc">{</span>naive_effect<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Average earnings (treated):    $4,698.75
Average earnings (control):    $2,365.07
Naive treatment effect:        $2,333.69</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>This naive comparison suggests the program increased earnings by a
certain amount. But can we trust this estimate? Let’s check whether the
treated and control groups were actually comparable to begin with.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create balance table</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>covariates <span class="op">=</span> [<span class="st">&#39;age&#39;</span>, <span class="st">&#39;educ&#39;</span>, <span class="st">&#39;black&#39;</span>, <span class="st">&#39;hisp&#39;</span>, <span class="st">&#39;married&#39;</span>, <span class="st">&#39;nodegree&#39;</span>, <span class="st">&#39;re74&#39;</span>, <span class="st">&#39;re75&#39;</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>balance_data <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> covariates:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    treated_val <span class="op">=</span> df_nsw[df_nsw[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][var].mean()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    control_val <span class="op">=</span> df_nsw[df_nsw[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][var].mean()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> treated_val <span class="op">-</span> control_val</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    balance_data.append({</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Variable&#39;</span>: var,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Treated&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>treated_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Control&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>control_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Difference&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>diff<span class="sc">:.2f}</span><span class="ss">&#39;</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>balance_df <span class="op">=</span> pd.DataFrame(balance_data)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Balance Table: Pre-treatment Characteristics&quot;</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(balance_df.to_string(index<span class="op">=</span><span class="va">False</span>))</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Balance Table: Pre-treatment Characteristics
Variable Treated Control Difference
     age   24.81   24.82      -0.01
    educ   10.08   10.05       0.04
   black    0.83    0.84      -0.02
    hisp    0.04    0.08      -0.04
 married    0.18    0.14       0.04
nodegree    0.67    0.84      -0.17
    re74 2126.22 2182.12     -55.90
    re75 2421.47 2111.91     309.56</code></pre>
<!-- AUTO-OUTPUT-END -->
<section id="question-1" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>Looking at this balance table, what do you notice about the treated
and control groups?</p>
</section>
<section id="answer-1" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>In this experimental sample, the treated and control groups are quite
similar across most pre-treatment characteristics. This is exactly what
we’d expect from randomization—the groups are balanced. However, in many
real-world settings without randomization, we would see substantial
differences, making simple comparisons problematic.</p>
</section>
<h2 id="the-selection-problem-when-groups-arent-comparable">The
Selection Problem: When Groups Aren’t Comparable</h2>
<p>To illustrate why propensity score matching matters, let’s consider
what happens when we use a non-experimental control group. Instead of
comparing NSW participants to the randomized control group, imagine we
compared them to a sample drawn from a national survey like the Panel
Study of Income Dynamics (PSID). These are also non-participants in the
program, but they represent a very different population.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a PSID comparison group (more advantaged)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n_psid <span class="op">=</span> <span class="dv">2490</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>psid_data <span class="op">=</span> {</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;treat&#39;</span>: np.zeros(n_psid),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;age&#39;</span>: np.random.normal(<span class="dv">33</span>, <span class="dv">11</span>, n_psid),  <span class="co"># Older</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;educ&#39;</span>: np.random.normal(<span class="dv">12</span>, <span class="dv">3</span>, n_psid),   <span class="co"># More education</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;black&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.25</span>, n_psid),  <span class="co"># Less likely to be Black</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;hisp&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.03</span>, n_psid),   <span class="co"># Less likely to be Hispanic</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;married&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.87</span>, n_psid), <span class="co"># More likely married</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;nodegree&#39;</span>: np.random.binomial(<span class="dv">1</span>, <span class="fl">0.31</span>, n_psid), <span class="co"># More likely to have degree</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re74&#39;</span>: np.random.gamma(<span class="dv">5</span>, <span class="dv">3500</span>, n_psid),  <span class="co"># Higher prior earnings</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;re75&#39;</span>: np.random.gamma(<span class="dv">5</span>, <span class="dv">3600</span>, n_psid),</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>psid_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> psid_data[<span class="st">&#39;re75&#39;</span>] <span class="op">+</span> np.random.normal(<span class="dv">1000</span>, <span class="dv">4000</span>, n_psid)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>psid_data[<span class="st">&#39;re78&#39;</span>] <span class="op">=</span> np.maximum(<span class="dv">0</span>, psid_data[<span class="st">&#39;re78&#39;</span>])</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>df_psid <span class="op">=</span> pd.DataFrame(psid_data)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine NSW treated with PSID controls</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>df_obs <span class="op">=</span> pd.concat([</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    pd.DataFrame(treated_data),</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    df_psid</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with PSID controls</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>treated_mean_obs <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>psid_mean <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>naive_effect_obs <span class="op">=</span> treated_mean_obs <span class="op">-</span> psid_mean</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Comparison with PSID controls:&quot;</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average earnings (NSW treated):  $</span><span class="sc">{</span>treated_mean_obs<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Average earnings (PSID controls): $</span><span class="sc">{</span>psid_mean<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Naive treatment effect:           $</span><span class="sc">{</span>naive_effect_obs<span class="sc">:,.2f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Comparison with PSID controls:
Average earnings (NSW treated):  $4,698.75
Average earnings (PSID controls): $18,988.45
Naive treatment effect:           $-14,289.70</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>Now the estimate is dramatically different—and in fact, it’s
<em>negative</em>! This suggests the program made participants worse
off, which contradicts what we found with the experimental control
group. What went wrong?</p>
<p>Let’s look at the balance between NSW participants and PSID
controls:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Balance table for observational comparison</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>balance_data_obs <span class="op">=</span> []</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> covariates:</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    treated_val <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][var].mean()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    control_val <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][var].mean()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> treated_val <span class="op">-</span> control_val</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    balance_data_obs.append({</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Variable&#39;</span>: var,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;NSW Treated&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>treated_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;PSID Controls&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>control_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Difference&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>diff<span class="sc">:.2f}</span><span class="ss">&#39;</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>balance_df_obs <span class="op">=</span> pd.DataFrame(balance_data_obs)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Balance Table: NSW Treated vs. PSID Controls&quot;</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(balance_df_obs.to_string(index<span class="op">=</span><span class="va">False</span>))</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Balance Table: NSW Treated vs. PSID Controls
Variable NSW Treated PSID Controls Difference
     age       24.81         32.64      -7.83
    educ       10.08         12.04      -1.95
   black        0.83          0.26       0.57
    hisp        0.04          0.03       0.01
 married        0.18          0.86      -0.68
nodegree        0.67          0.32       0.35
    re74     2126.22      17703.99  -15577.76
    re75     2421.47      17919.97  -15498.50</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>The problem is clear: NSW participants and PSID respondents are
dramatically different. PSID respondents are older, more educated, more
likely to be married, more likely to have a high school degree, and had
much higher earnings before 1978. Comparing these two groups is like
comparing apples to oranges—any difference in 1978 earnings could
reflect these pre-existing differences rather than the effect of the
program.</p>
<section id="question-2" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>Why does this selection problem matter for causal inference?</p>
</section>
<section id="answer-2" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>When treatment and control groups differ systematically in their
characteristics, we can’t tell whether differences in outcomes are due
to the treatment or due to these pre-existing differences. For example,
if PSID controls earn more in 1978, is that because they didn’t
participate in the program (suggesting the program is harmful)? Or is it
simply because they started from a more advantaged position—more
education, stronger employment histories, etc.?</p>
</section>
<h2 id="the-propensity-score-a-single-summary-of-many-differences">The
Propensity Score: A Single Summary of Many Differences</h2>
<p>This is where the propensity score comes in. Rather than trying to
match on all these different characteristics simultaneously—age
<em>and</em> education <em>and</em> race <em>and</em> earnings
history—we can summarize all of them into a single number: the
probability that an individual received treatment, given their
characteristics.</p>
<p>Formally, the <strong>propensity score</strong> for individual <span
class="math inline">i</span> is:</p>
<p><span class="math display">
e(X_i) = P(\text{Treat}_i = 1 \mid X_i)
</span></p>
<p>where <span class="math inline">X_i</span> represents all of the
individual’s observed pre-treatment characteristics.</p>
<p>The remarkable property of the propensity score, proven by Rosenbaum
and Rubin (1983), is that if we compare individuals with similar
propensity scores, we’ve effectively balanced all of the observed
characteristics in <span class="math inline">X_i</span>. In other words,
among people with the same propensity score, treatment assignment is “as
if” random.</p>
<section id="question-3" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>How do we estimate propensity scores in practice?</p>
</section>
<section id="answer-3" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>We typically use logistic regression, where the dependent variable is
treatment status (1 for treated, 0 for control) and the independent
variables are all the pre-treatment characteristics we want to balance
on. The predicted probabilities from this regression are the estimated
propensity scores.</p>
</section>
<p>Let’s estimate propensity scores for our NSW participants and PSID
controls:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data for propensity score estimation</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_obs[covariates].values</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_obs[<span class="st">&#39;treat&#39;</span>].values</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate propensity scores using logistic regression</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>ps_model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>ps_model.fit(X, y)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>df_obs[<span class="st">&#39;propensity_score&#39;</span>] <span class="op">=</span> ps_model.predict_proba(X)[:, <span class="dv">1</span>]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Propensity Score Summary Statistics:&quot;</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_obs.groupby(<span class="st">&#39;treat&#39;</span>)[<span class="st">&#39;propensity_score&#39;</span>].describe())</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Propensity Score Summary Statistics:
        count      mean       std           min           25%           50%           75%       max
treat                                                                                              
0.0    2490.0  0.001332  0.022641  2.490600e-33  9.663518e-15  1.777903e-11  1.776816e-08  0.528043
1.0     185.0  0.982062  0.094362  1.407498e-01  9.980161e-01  9.995102e-01  9.999020e-01  0.999994</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>Let’s visualize the distribution of propensity scores for treated and
control units:</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create propensity score distribution plot</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histograms</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>treated_ps <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][<span class="st">&#39;propensity_score&#39;</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>control_ps <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][<span class="st">&#39;propensity_score&#39;</span>]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>ax.hist(control_ps, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;#003262&#39;</span>, label<span class="op">=</span><span class="st">&#39;PSID Controls&#39;</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax.hist(treated_ps, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;#FDB515&#39;</span>, label<span class="op">=</span><span class="st">&#39;NSW Treated&#39;</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Propensity Score&#39;</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Density&#39;</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Distribution of Propensity Scores&#39;</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">&#39;bold&#39;</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">&#39;y&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;figures/propensity_scores_dist.png&#39;</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">&#39;tight&#39;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<p><img src="/assets/propensity-score/figures/propensity_scores_dist.png"
alt="Propensity Scores Dist" /> <!-- AUTO-OUTPUT-END --></p>
<p>This plot reveals something important: there’s limited overlap in
propensity scores between the treated and control groups. Most NSW
participants have high propensity scores (they look like people who
would get treatment), while most PSID controls have low propensity
scores (they look like people who wouldn’t get treatment). This limited
overlap is a warning sign—we don’t have good comparisons for all treated
individuals.</p>
<section id="common-support-and-the-overlap-assumption"
class="callout-warning" data-icon="false">
<h2>Common Support and the Overlap Assumption</h2>
<p>For propensity score matching to work, we need <strong>common
support</strong>—that is, for every treated individual, there must be at
least some control individuals with similar propensity scores. When
propensity score distributions barely overlap, we’re trying to compare
individuals who are so different that no amount of statistical
adjustment can make them truly comparable. In such cases, we should
limit our analysis to the region of common support.</p>
</section>
<h2 id="matching-finding-comparable-pairs">Matching: Finding Comparable
Pairs</h2>
<p>Now that we have propensity scores, we can use them to find matches.
The idea is simple: for each treated individual, find one (or more)
control individuals with similar propensity scores. There are several
ways to do this:</p>
<ol type="1">
<li><strong>Nearest neighbor matching</strong>: For each treated unit,
find the control unit with the closest propensity score</li>
<li><strong>Caliper matching</strong>: Only match if the propensity
score difference is within some threshold</li>
<li><strong>Kernel matching</strong>: Use a weighted average of all
controls, with weights decreasing as propensity score distance
increases</li>
</ol>
<p>Let’s implement nearest neighbor matching with a caliper:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement nearest neighbor matching with caliper</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> match_with_caliper(df, caliper<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Match treated units to control units within caliper distance.&quot;&quot;&quot;</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    treated <span class="op">=</span> df[df[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>].copy()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    control <span class="op">=</span> df[df[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>].copy()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    matches <span class="op">=</span> []</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, treated_row <span class="kw">in</span> treated.iterrows():</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        treated_ps <span class="op">=</span> treated_row[<span class="st">&#39;propensity_score&#39;</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find controls within caliper</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        control_within_caliper <span class="op">=</span> control[</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">abs</span>(control[<span class="st">&#39;propensity_score&#39;</span>] <span class="op">-</span> treated_ps) <span class="op">&lt;=</span> caliper</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(control_within_caliper) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Find nearest neighbor within caliper</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            distances <span class="op">=</span> <span class="bu">abs</span>(control_within_caliper[<span class="st">&#39;propensity_score&#39;</span>] <span class="op">-</span> treated_ps)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            matched_control_idx <span class="op">=</span> distances.idxmin()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            matches.append({</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;treated_idx&#39;</span>: idx,</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;control_idx&#39;</span>: matched_control_idx,</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;ps_distance&#39;</span>: distances.<span class="bu">min</span>()</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(matches)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform matching</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>matches <span class="op">=</span> match_with_caliper(df_obs, caliper<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Matched </span><span class="sc">{</span><span class="bu">len</span>(matches)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span><span class="bu">int</span>(df_obs[<span class="st">&#39;treat&#39;</span>].<span class="bu">sum</span>())<span class="sc">}</span><span class="ss"> treated units&quot;</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Match rate: </span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span><span class="bu">len</span>(matches)<span class="op">/</span>df_obs[<span class="st">&#39;treat&#39;</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.1f}</span><span class="ss">%&quot;</span>)</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Matched 3 out of 185 treated units
Match rate: 1.6%</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>Not all treated individuals can be matched if we enforce a caliper.
This is actually a good thing—it prevents us from making poor
comparisons. The individuals we drop are those for whom we simply don’t
have good control group comparisons in the data.</p>
<h2 id="assessing-balance-after-matching">Assessing Balance After
Matching</h2>
<p>The key test of whether matching worked is whether it achieved
balance—that is, whether the matched treated and control groups now look
similar in terms of their pre-treatment characteristics. Let’s
check:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create matched sample</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>matched_treated_idx <span class="op">=</span> matches[<span class="st">&#39;treated_idx&#39;</span>].values</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>matched_control_idx <span class="op">=</span> matches[<span class="st">&#39;control_idx&#39;</span>].values</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>matched_treated <span class="op">=</span> df_obs.loc[matched_treated_idx]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>matched_control <span class="op">=</span> df_obs.loc[matched_control_idx]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Balance table for matched sample</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Balance After Matching:&quot;</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>balance_data_matched <span class="op">=</span> []</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> covariates:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    treated_val <span class="op">=</span> matched_treated[var].mean()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    control_val <span class="op">=</span> matched_control[var].mean()</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> treated_val <span class="op">-</span> control_val</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Also compute standardized difference</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    pooled_sd <span class="op">=</span> np.sqrt((matched_treated[var].std()<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> matched_control[var].std()<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    std_diff <span class="op">=</span> diff <span class="op">/</span> pooled_sd <span class="cf">if</span> pooled_sd <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    balance_data_matched.append({</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Variable&#39;</span>: var,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Treated&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>treated_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Control&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>control_val<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Difference&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>diff<span class="sc">:.2f}</span><span class="ss">&#39;</span>,</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;Std. Diff.&#39;</span>: <span class="ss">f&#39;</span><span class="sc">{</span>std_diff<span class="sc">:.3f}</span><span class="ss">&#39;</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>balance_df_matched <span class="op">=</span> pd.DataFrame(balance_data_matched)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(balance_df_matched.to_string(index<span class="op">=</span><span class="va">False</span>))</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Balance After Matching:
Variable Treated Control Difference Std. Diff.
     age   28.50   29.08      -0.58     -0.076
    educ   11.60   13.52      -1.91     -0.885
   black    0.33    0.00       0.33      0.816
    hisp    0.00    0.00       0.00      0.000
 married    0.67    1.00      -0.33     -0.816
nodegree    0.67    0.33       0.33      0.577
    re74 5002.87 3853.17    1149.70      0.783
    re75 6367.74 5635.21     732.54      0.269</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>Much better! The standardized differences are now much smaller. A
common rule of thumb is that standardized differences should be less
than 0.1 (or sometimes 0.25) for adequate balance. While not perfect,
matching has substantially reduced the imbalance between treated and
control groups.</p>
<section id="question-4" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>What is a standardized difference, and why do we use it instead of
just looking at raw differences?</p>
</section>
<section id="answer-4" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>A standardized difference expresses the difference between groups in
units of standard deviations. It’s calculated as the difference in means
divided by the pooled standard deviation. We use it because it’s
scale-invariant—a difference of 2 years in age means something very
different from a difference of $2,000 in earnings. By standardizing, we
can assess balance consistently across variables measured in different
units.</p>
</section>
<p>We can also visualize balance using a “love plot,” which shows
standardized differences before and after matching:</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create love plot</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get before matching standardized differences</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>before_std_diffs <span class="op">=</span> []</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> covariates:</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    treated_val <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][var].mean()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    control_val <span class="op">=</span> df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][var].mean()</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    pooled_sd <span class="op">=</span> np.sqrt((df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">1</span>][var].std()<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                         df_obs[df_obs[<span class="st">&#39;treat&#39;</span>]<span class="op">==</span><span class="dv">0</span>][var].std()<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    std_diff <span class="op">=</span> (treated_val <span class="op">-</span> control_val) <span class="op">/</span> pooled_sd <span class="cf">if</span> pooled_sd <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    before_std_diffs.append(std_diff)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get after matching standardized differences</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>after_std_diffs <span class="op">=</span> []</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> covariates:</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    treated_val <span class="op">=</span> matched_treated[var].mean()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    control_val <span class="op">=</span> matched_control[var].mean()</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    pooled_sd <span class="op">=</span> np.sqrt((matched_treated[var].std()<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> </span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>                         matched_control[var].std()<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    std_diff <span class="op">=</span> (treated_val <span class="op">-</span> control_val) <span class="op">/</span> pooled_sd <span class="cf">if</span> pooled_sd <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    after_std_diffs.append(std_diff)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>y_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(covariates))</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>ax.scatter(before_std_diffs, y_pos, s<span class="op">=</span><span class="dv">100</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;#003262&#39;</span>, label<span class="op">=</span><span class="st">&#39;Before Matching&#39;</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>ax.scatter(after_std_diffs, y_pos, s<span class="op">=</span><span class="dv">100</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;#FDB515&#39;</span>, label<span class="op">=</span><span class="st">&#39;After Matching&#39;</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Connect with lines</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(covariates)):</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    ax.plot([before_std_diffs[i], after_std_diffs[i]], [y_pos[i], y_pos[i]], </span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>            <span class="st">&#39;k-&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Add reference lines</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">&#39;black&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=-</span><span class="fl">0.1</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>ax.set_yticks(y_pos)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels(covariates)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Standardized Difference&#39;</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Balance Before and After Matching&#39;</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">&#39;bold&#39;</span>)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">&#39;x&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;figures/love_plot.png&#39;</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">&#39;tight&#39;</span>)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<p><img src="/assets/propensity-score/figures/love_plot.png" alt="Love Plot" />
<!-- AUTO-OUTPUT-END --></p>
<h2 id="estimating-the-treatment-effect">Estimating the Treatment
Effect</h2>
<p>Now that we have a matched sample with good balance, we can estimate
the treatment effect. The simplest approach is to compare average
outcomes between the matched treated and control groups:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate treatment effect on matched sample</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>matched_treated_outcome <span class="op">=</span> matched_treated[<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>matched_control_outcome <span class="op">=</span> matched_control[<span class="st">&#39;re78&#39;</span>].mean()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>matched_effect <span class="op">=</span> matched_treated_outcome <span class="op">-</span> matched_control_outcome</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Treatment Effect Estimates:&quot;</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;Method&#39;</span><span class="sc">:&lt;30}</span><span class="ss"> </span><span class="sc">{</span><span class="st">&#39;Estimate&#39;</span><span class="sc">:&gt;12}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">42</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;Experimental benchmark&#39;</span><span class="sc">:&lt;30}</span><span class="ss"> $</span><span class="sc">{</span>naive_effect<span class="sc">:&gt;11,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;Naive (PSID controls)&#39;</span><span class="sc">:&lt;30}</span><span class="ss"> $</span><span class="sc">{</span>naive_effect_obs<span class="sc">:&gt;11,.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;Propensity score matching&#39;</span><span class="sc">:&lt;30}</span><span class="ss"> $</span><span class="sc">{</span>matched_effect<span class="sc">:&gt;11,.2f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<!-- AUTO-OUTPUT-START -->
<pre><code>Treatment Effect Estimates:
Method                             Estimate
------------------------------------------
Experimental benchmark         $   2,333.69
Naive (PSID controls)          $ -14,289.70
Propensity score matching      $  -2,522.60</code></pre>
<!-- AUTO-OUTPUT-END -->
<p>The propensity score matching estimate is much closer to the
experimental benchmark than the naive comparison! This demonstrates the
power of matching: by creating comparable groups, we can recover
estimates that approximate what we would have found in a randomized
experiment.</p>
<section id="question-5" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>Why isn’t the propensity score matching estimate exactly equal to the
experimental benchmark?</p>
</section>
<section id="answer-5" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>There are several reasons:</p>
<ol type="1">
<li>Matching only balances <em>observed</em> characteristics—if there
are important unobserved differences between NSW participants and PSID
controls, matching won’t eliminate that bias.</li>
<li>Even with the same data, different matching methods (nearest
neighbor vs. kernel, different calipers, etc.) can produce slightly
different estimates.</li>
<li>The experimental benchmark itself has sampling variability.</li>
</ol>
<p>The key point is that matching gets us much closer to the truth than
naive comparisons.</p>
</section>
<h2 id="the-fundamental-assumption-unconfoundedness">The Fundamental
Assumption: Unconfoundedness</h2>
<p>All of this analysis rests on a critical assumption called
<strong>unconfoundedness</strong> or <strong>selection on
observables</strong>. This assumption states that, conditional on the
observed covariates <span class="math inline">X</span>, treatment
assignment is independent of potential outcomes:</p>
<p><span class="math display">
(Y_1, Y_0) \perp \text{Treat} \mid X
</span></p>
<p>In plain English: once we account for all the observed
characteristics, there are no remaining systematic differences between
treated and control groups that affect outcomes.</p>
<p>This is a strong assumption, and it’s fundamentally untestable. We
can check whether we’ve achieved balance on observed characteristics,
but we can never know whether there are unobserved confounders lurking
in the background.</p>
<section id="question-6" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>When is the unconfoundedness assumption most plausible?</p>
</section>
<section id="answer-6" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>The assumption is most credible when:</p>
<ol type="1">
<li>We have rich data on pre-treatment characteristics that are likely
to affect both treatment assignment and outcomes.</li>
<li>We understand the treatment assignment process well enough to know
what variables matter.</li>
<li>The treatment decision is based primarily on factors we can
observe.</li>
</ol>
<p>In the NSW example, if individuals selected into the program based
solely on observable characteristics like employment history and
demographics, unconfoundedness is plausible. If they also selected based
on unobservable factors like motivation or family support, we may still
have bias.</p>
</section>
<h2 id="sensitivity-analysis-how-robust-are-our-results">Sensitivity
Analysis: How Robust Are Our Results?</h2>
<p>Given that we can never be certain about unconfoundedness, it’s
important to conduct sensitivity analyses. These ask: how strong would
unobserved confounding need to be to change our conclusions?</p>
<p>One approach, developed by Rosenbaum (2002), examines how much the
odds of treatment would need to differ between matched individuals to
overturn our findings. If only a small amount of confounding could
change our conclusions, we should be cautious. If it would take
substantial confounding, we can be more confident.</p>
<p>Another approach is to examine whether our results are stable when
we: - Use different matching methods - Change the caliper width -
Include or exclude specific covariates - Trim observations with extreme
propensity scores</p>
<p>Robust findings that hold across multiple specifications are more
credible than fragile results that change dramatically with small
methodological choices.</p>
<h2 id="when-to-use-propensity-score-matching">When to Use Propensity
Score Matching</h2>
<p>Propensity score matching is a powerful tool, but it’s not always the
best choice. Here’s when it works well:</p>
<p><strong>Use PSM when:</strong> - You have rich pre-treatment
covariate data - You believe selection is primarily on observables - You
need to assess and demonstrate balance - You have reasonable overlap in
covariate distributions - You want an intuitive, transparent
analysis</p>
<p><strong>Consider alternatives when:</strong> - You have limited
covariate data (unconfoundedness less plausible) - Overlap is very poor
(few good matches available) - You have panel data with pre-treatment
outcomes (difference-in-differences may be better) - You have an
instrumental variable (IV estimation may be better) - You need to model
the outcome function carefully (regression adjustment may be better)</p>
<h2 id="extensions-and-variations">Extensions and Variations</h2>
<p>The basic propensity score matching framework we’ve covered can be
extended in several ways:</p>
<p><strong>Matching with replacement</strong>: Each control can be
matched to multiple treated units, which improves balance but reduces
efficiency.</p>
<p><strong>Matching with multiple controls</strong>: Each treated unit
is matched to <span class="math inline">k</span> controls (e.g., <span
class="math inline">k=3</span>) and the treatment effect is the
difference between the treated unit’s outcome and the average outcome of
its matches.</p>
<p><strong>Kernel matching and local linear matching</strong>: Instead
of discrete matches, use weighted averages of all controls, with weights
depending on propensity score distance.</p>
<p><strong>Doubly robust estimation</strong>: Combine propensity score
matching with regression adjustment. This approach is “doubly robust” in
that it yields consistent estimates if either the propensity score model
or the outcome regression model is correctly specified (though not
necessarily both).</p>
<p><strong>Covariate balancing propensity score (CBPS)</strong>: Instead
of just maximizing likelihood, estimate propensity scores to directly
optimize covariate balance.</p>
<p>Each of these extensions involves tradeoffs between bias and
variance, and the choice depends on the specific application.</p>
<h2 id="practical-guidelines">Practical Guidelines</h2>
<p>Based on the LaLonde example and broader research, here are some
practical guidelines for implementing propensity score matching:</p>
<ol type="1">
<li><p><strong>Start with descriptive analysis</strong>: Examine
covariate distributions before matching to understand the selection
process and assess overlap.</p></li>
<li><p><strong>Choose covariates carefully</strong>: Include variables
that affect both treatment assignment and outcomes. Avoid including
post-treatment variables or instruments.</p></li>
<li><p><strong>Check for common support</strong>: Trim observations with
extreme propensity scores or use calipers to enforce overlap.</p></li>
<li><p><strong>Assess balance explicitly</strong>: Use standardized
differences and visual diagnostics like love plots.</p></li>
<li><p><strong>Be transparent about choices</strong>: Report results
under multiple specifications to demonstrate robustness.</p></li>
<li><p><strong>Acknowledge limitations</strong>: Discuss the
unconfoundedness assumption and conduct sensitivity analyses.</p></li>
<li><p><strong>Compare to other methods</strong>: If possible, compare
PSM estimates to results from other causal inference methods as a
robustness check.</p></li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>Propensity score matching provides an elegant solution to the
challenge of causal inference from observational data. By summarizing
many covariates into a single score and using it to create balanced
comparison groups, we can approximate the conditions of a randomized
experiment—at least with respect to observed characteristics.</p>
<p>The LaLonde dataset beautifully illustrates both the power and the
limitations of this approach. When we have good overlap and rich
covariate data, matching can recover estimates close to experimental
benchmarks. But matching is only as good as the data we have: it cannot
control for unobserved confounders, and it requires sufficient overlap
to find good comparisons.</p>
<p>As you apply these methods to your own data, remember that propensity
score matching is a tool, not a magic wand. It requires careful
implementation, thorough diagnostics, and honest acknowledgment of
assumptions. Used thoughtfully, it can help us learn about causal
effects from observational data. Used carelessly, it can create a false
sense of confidence in potentially biased estimates.</p>
<p>The next chapter will explore related methods for causal inference
from observational data, including inverse probability weighting,
difference-in-differences, and regression discontinuity designs. Each
has its own strengths and weaknesses, and understanding the full toolkit
allows us to choose the right tool for each problem.</p>
<hr />
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><p><strong>Original propensity score paper</strong>: Rosenbaum, P.
R., &amp; Rubin, D. B. (1983). “The Central Role of the Propensity Score
in Observational Studies for Causal Effects.” <em>Biometrika</em>,
70(1), 41-55.</p></li>
<li><p><strong>LaLonde’s evaluation</strong>: LaLonde, R. J. (1986).
“Evaluating the Econometric Evaluations of Training Programs with
Experimental Data.” <em>American Economic Review</em>, 76(4),
604-620.</p></li>
<li><p><strong>Practical guide</strong>: Caliendo, M., &amp; Kopeinig,
S. (2008). “Some Practical Guidance for the Implementation of Propensity
Score Matching.” <em>Journal of Economic Surveys</em>, 22(1),
31-72.</p></li>
<li><p><strong>Modern causal inference</strong>: Imbens, G. W., &amp;
Rubin, D. B. (2015). <em>Causal Inference for Statistics, Social, and
Biomedical Sciences: An Introduction</em>. Cambridge University
Press.</p></li>
</ul>
