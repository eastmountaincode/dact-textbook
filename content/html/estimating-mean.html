<h1 id="sec-estimator-properties">Estimating the population mean</h1>
<p>In this chapter, we’ll explore three fundamental properties of
statistical estimators that form the backbone of statistical inference.
We’ll prove that the sample mean is an unbiased estimator of the
population mean, demonstrate that it’s the most efficient among all
unbiased estimators, and examine why the sample variance requires a
correction factor. These proofs are not merely mathematical
exercises—they reveal deep truths about how we can reliably learn about
populations from samples.</p>
<p>By the end of this chapter, you will understand:</p>
<ul>
<li>What makes an estimator “unbiased” and why this matters</li>
<li>How to compare estimators using the criterion of efficiency</li>
<li>Why the sample variance formula uses <span
class="math inline">n-1</span> instead of <span
class="math inline">n</span></li>
<li>The relationship between sample statistics and population
parameters</li>
</ul>
<h2 id="the-unbiasedness-of-the-sample-mean">The Unbiasedness of the
Sample Mean</h2>
<p>Let’s begin with a fundamental question that underlies all of
statistical inference.</p>
<section id="question" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>How do we know that the sample mean is a reliable estimator of the
population mean? Could there be systematic error in our estimates?</p>
</section>
<section id="answer" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>The sample mean is <strong>unbiased</strong>, meaning that if we
could take infinitely many samples and calculate the mean for each one,
the average of all those sample means would exactly equal the population
mean. This property holds regardless of sample size or the shape of the
population distribution—it’s assumption-free except for requiring that
the population mean is finite.</p>
</section>
<h3 id="understanding-unbiasedness">Understanding Unbiasedness</h3>
<p>An estimator is <strong>unbiased</strong> if its expected value
equals the parameter it’s trying to estimate. For the sample mean <span
class="math inline">\bar{Y}</span> estimating the population mean <span
class="math inline">\mu</span>, we want to show:</p>
<p><span class="math display">
E[\bar{Y}] = \mu
</span></p>
<p>This is a powerful property because it guarantees that our estimator
has no systematic tendency to overestimate or underestimate the true
parameter. Some samples will give us values above <span
class="math inline">\mu</span>, others below, but on average—across
infinitely many samples—we hit the target exactly.</p>
<h3 id="the-proof">The Proof</h3>
<p>The proof is remarkably elegant. Let’s work through it step by
step.</p>
<p>We start with the definition of the sample mean:</p>
<p><span class="math display">
\bar{Y} = \frac{Y_1 + Y_2 + \cdots + Y_n}{n}
</span></p>
<p>Taking the expected value of both sides:</p>
<p><span class="math display">
E[\bar{Y}] = E\left[\frac{Y_1 + Y_2 + \cdots + Y_n}{n}\right]
</span></p>
<p>Since <span class="math inline">n</span> is a constant (our fixed
sample size), we can factor it out using the linearity of
expectation:</p>
<p><span class="math display">
E[\bar{Y}] = \frac{1}{n} E[Y_1 + Y_2 + \cdots + Y_n]
</span></p>
<p>Using the linearity property again, the expectation of a sum equals
the sum of expectations:</p>
<p><span class="math display">
E[\bar{Y}] = \frac{1}{n} \left(E[Y_1] + E[Y_2] + \cdots + E[Y_n]\right)
</span></p>
<p>Now comes the key insight. Each observation <span
class="math inline">Y_i</span> is drawn from the same population, so
each has the same expected value—the population mean <span
class="math inline">\mu</span>. Think about it this way: if you could
observe infinitely many “first observations” from different samples,
their distribution would look exactly like the population distribution,
and their mean would be <span class="math inline">\mu</span>. The same
holds for the second observation, the third, and all others.</p>
<p>Therefore:</p>
<p><span class="math display">
E[\bar{Y}] = \frac{1}{n}(\mu + \mu + \cdots + \mu) = \frac{1}{n}(n\mu) =
\mu
</span></p>
<p>The proof is complete. ∎</p>
<section id="important-assumption-free-result" class="callout-important"
data-icon="false">
<h2>Important: Assumption-Free Result</h2>
<p>This proof makes <strong>no assumptions</strong> about:</p>
<ul>
<li>The sample size <span class="math inline">n</span> (it can be as
small as 2)</li>
<li>The distribution of the population (it can be skewed, multimodal, or
anything)</li>
<li>The variance of the population (it doesn’t even need to exist)</li>
</ul>
<p>The only requirement is that <span class="math inline">\mu</span> is
finite. There are some exotic distributions (like the Cauchy
distribution) whose mean is undefined, but for all practical purposes,
if the population has a finite mean, the sample mean is an unbiased
estimator of it.</p>
</section>
<h2 id="the-efficiency-of-the-sample-mean">The Efficiency of the Sample
Mean</h2>
<p>Proving unbiasedness is just the first step. After all, there are
<strong>infinitely many</strong> unbiased estimators of the population
mean. We need another criterion to choose among them.</p>
<section id="question-1" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>If there are infinitely many unbiased estimators, how do we decide
which one to use? What makes the sample mean special?</p>
</section>
<section id="answer-1" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>Among all unbiased estimators, the sample mean has the
<strong>smallest variance</strong>—a property called efficiency. This
means it gives us the most precise estimates on average. While other
unbiased estimators might occasionally give better results in particular
samples, the sample mean is most reliable in the long run.</p>
</section>
<h3 id="the-problem-of-too-many-estimators">The Problem of Too Many
Estimators</h3>
<p>Consider this: the first observation <span
class="math inline">Y_1</span> by itself is an unbiased estimator of
<span class="math inline">\mu</span> since <span
class="math inline">E[Y_1] = \mu</span>. So is the second observation.
So is any weighted average of your observations, as long as the weights
sum to one. For example:</p>
<p><span class="math display">
T = \frac{1}{4}Y_1 + \frac{3}{4}Y_2
</span></p>
<p>This is unbiased (you can verify that <span class="math inline">E[T]
= \mu</span>). But it completely ignores observations 3 through <span
class="math inline">n</span> if you have more data! Intuitively, this
seems wasteful. We need a way to filter these infinitely many unbiased
estimators.</p>
<h3 id="the-efficiency-criterion">The Efficiency Criterion</h3>
<p>We use <strong>efficiency</strong> as our second filter. An efficient
estimator is one that has the minimum variance among all unbiased
estimators. Why variance? Because variance measures precision—how much
our estimates vary from sample to sample. Lower variance means more
consistent, reliable estimates.</p>
<h3 id="setting-up-the-proof">Setting Up the Proof</h3>
<p>Let’s define a general unbiased estimator as a weighted combination
of our observations:</p>
<p><span class="math display">
T = \sum_{i=1}^{n} A_i Y_i
</span></p>
<p>where the <span class="math inline">A_i</span> are arbitrary weights
(constants). Since we’re restricting attention to unbiased estimators,
we require:</p>
<p><span class="math display">
E[T] = \mu
</span></p>
<p>Let’s see what this constraint implies. Taking expectations:</p>
<p><span class="math display">
E[T] = E\left[\sum_{i=1}^{n} A_i Y_i\right] = \sum_{i=1}^{n} A_i E[Y_i]
= \sum_{i=1}^{n} A_i \mu = \mu \sum_{i=1}^{n} A_i
</span></p>
<p>For this to equal <span class="math inline">\mu</span>, we need:</p>
<p><span class="math display">
\sum_{i=1}^{n} A_i = 1
</span></p>
<p>This is our first constraint: the weights must sum to one. The sample
mean satisfies this with <span class="math inline">A_i = 1/n</span> for
all <span class="math inline">i</span>.</p>
<h3 id="finding-the-variance">Finding the Variance</h3>
<p>Now let’s calculate the variance of our general estimator <span
class="math inline">T</span>:</p>
<p><span class="math display">
\text{Var}(T) = \text{Var}\left(\sum_{i=1}^{n} A_i Y_i\right)
</span></p>
<p>Since the <span class="math inline">A_i</span> are constants and the
observations are independent:</p>
<p><span class="math display">
\text{Var}(T) = \sum_{i=1}^{n} A_i^2 \text{Var}(Y_i) = \sum_{i=1}^{n}
A_i^2 \sigma^2 = \sigma^2 \sum_{i=1}^{n} A_i^2
</span></p>
<p>where <span class="math inline">\sigma^2</span> is the population
variance (assumed to be the same for all observations, since they’re all
drawn from the same population).</p>
<h3 id="the-key-inequality">The Key Inequality</h3>
<p>Now we employ a clever algebraic trick. Notice that:</p>
<p><span class="math display">
\sum_{i=1}^{n} A_i^2 = \sum_{i=1}^{n} \left(A_i - \frac{1}{n}\right)^2 +
\frac{1}{n}
</span></p>
<p>This might seem to come out of nowhere, but let’s verify it by
expanding the squared term:</p>
<p><span class="math display">\begin{align}
\sum_{i=1}^{n} \left(A_i - \frac{1}{n}\right)^2 &amp;= \sum_{i=1}^{n}
\left(A_i^2 - 2A_i \cdot \frac{1}{n} + \frac{1}{n^2}\right)\\
&amp;= \sum_{i=1}^{n} A_i^2 - \frac{2}{n}\sum_{i=1}^{n} A_i +
\sum_{i=1}^{n}\frac{1}{n^2}\\
&amp;= \sum_{i=1}^{n} A_i^2 - \frac{2}{n}(1) + \frac{n}{n^2}\\
&amp;= \sum_{i=1}^{n} A_i^2 - \frac{2}{n} + \frac{1}{n}\\
&amp;= \sum_{i=1}^{n} A_i^2 - \frac{1}{n}
\end{align}</span></p>
<p>where we used the constraint that <span class="math inline">\sum A_i
= 1</span>. Rearranging gives us the identity we claimed.</p>
<h3 id="completing-the-proof">Completing the Proof</h3>
<p>Since squares are always non-negative, we have:</p>
<p><span class="math display">
\sum_{i=1}^{n} \left(A_i - \frac{1}{n}\right)^2 \geq 0
</span></p>
<p>with equality if and only if <span class="math inline">A_i =
1/n</span> for all <span class="math inline">i</span>. Therefore:</p>
<p><span class="math display">
\sum_{i=1}^{n} A_i^2 \geq \frac{1}{n}
</span></p>
<p>Multiplying both sides by <span
class="math inline">\sigma^2</span>:</p>
<p><span class="math display">
\text{Var}(T) = \sigma^2 \sum_{i=1}^{n} A_i^2 \geq \frac{\sigma^2}{n} =
\text{Var}(\bar{Y})
</span></p>
<p>The minimum variance is achieved when <span class="math inline">A_i =
1/n</span> for all <span class="math inline">i</span>—which is precisely
the sample mean! ∎</p>
<section id="the-meaning-of-efficiency" class="callout-important"
data-icon="false">
<h2>The Meaning of Efficiency</h2>
<p>The sample mean is <strong>efficient</strong> because it makes
optimal use of all available information. Giving equal weight to each
observation minimizes the variance of our estimate. Any other weighting
scheme—whether it’s emphasizing early observations, ignoring some data,
or using unequal weights—will produce a less precise estimator.</p>
<p>This result has a beautiful interpretation: democracy in data is
optimal. No observation deserves more weight than any other when they’re
all drawn from the same population.</p>
</section>
<h2 id="the-bias-of-the-sample-variance">The Bias of the Sample
Variance</h2>
<p>Having established the virtues of the sample mean, we now turn to a
more subtle problem: estimating the population variance <span
class="math inline">\sigma^2</span>.</p>
<section id="question-2" class="callout-note" data-icon="false">
<h2>Question</h2>
<p>The natural estimator of population variance would seem to be the
average squared deviation from the sample mean: <span
class="math inline">\frac{1}{n}\sum_{i=1}^{n}(Y_i - \bar{Y})^2</span>.
Why do we use <span class="math inline">n-1</span> instead of <span
class="math inline">n</span> in the denominator? Is this just a
convention, or is there a deeper reason?</p>
</section>
<section id="answer-2" class="callout-tip" data-icon="false"
data-collapse="true">
<h2>Answer</h2>
<p>The formula with <span class="math inline">n</span> in the
denominator is actually <strong>biased</strong>—it systematically
underestimates the true population variance. Using <span
class="math inline">n-1</span> corrects this bias, giving us an unbiased
estimator. This isn’t arbitrary; it follows from a careful mathematical
analysis of how sample statistics relate to population parameters.</p>
</section>
<h3 id="defining-the-sample-variance">Defining the Sample Variance</h3>
<p>Let’s define what we’ll call the “uncorrected” sample variance:</p>
<p><span class="math display">
S^2 = \frac{1}{n}\sum_{i=1}^{n}(Y_i - \bar{Y})^2
</span></p>
<p>This is the natural definition—it’s the average squared deviation
from the sample mean. But is it unbiased? To answer this, we need to
calculate <span class="math inline">E[S^2]</span> and see if it equals
<span class="math inline">\sigma^2</span>.</p>
<h3 id="a-clever-algebraic-manipulation">A Clever Algebraic
Manipulation</h3>
<p>The key to this proof is recognizing that we can rewrite each
deviation <span class="math inline">(Y_i - \bar{Y})</span> in terms of
deviations from the true population mean <span
class="math inline">\mu</span>:</p>
<p><span class="math display">
Y_i - \bar{Y} = (Y_i - \mu) - (\bar{Y} - \mu)
</span></p>
<p>This is just adding and subtracting <span
class="math inline">\mu</span>. Now let’s square both sides:</p>
<p><span class="math display">
(Y_i - \bar{Y})^2 = [(Y_i - \mu) - (\bar{Y} - \mu)]^2
</span></p>
<p>Expanding the square:</p>
<p><span class="math display">
(Y_i - \bar{Y})^2 = (Y_i - \mu)^2 - 2(Y_i - \mu)(\bar{Y} - \mu) +
(\bar{Y} - \mu)^2
</span></p>
<h3 id="summing-over-all-observations">Summing Over All
Observations</h3>
<p>Now sum both sides from <span class="math inline">i=1</span> to <span
class="math inline">n</span>:</p>
<p><span class="math display">
\sum_{i=1}^{n}(Y_i - \bar{Y})^2 = \sum_{i=1}^{n}(Y_i - \mu)^2 -
2(\bar{Y} - \mu)\sum_{i=1}^{n}(Y_i - \mu) + \sum_{i=1}^{n}(\bar{Y} -
\mu)^2
</span></p>
<p>Let’s examine each term carefully. For the middle term, notice
that:</p>
<p><span class="math display">
\sum_{i=1}^{n}(Y_i - \mu) = \sum_{i=1}^{n}Y_i - n\mu = n\bar{Y} - n\mu =
n(\bar{Y} - \mu)
</span></p>
<p>So the middle term becomes:</p>
<p><span class="math display">
-2(\bar{Y} - \mu) \cdot n(\bar{Y} - \mu) = -2n(\bar{Y} - \mu)^2
</span></p>
<p>For the last term, <span class="math inline">(\bar{Y} - \mu)^2</span>
doesn’t depend on <span class="math inline">i</span>, so:</p>
<p><span class="math display">
\sum_{i=1}^{n}(\bar{Y} - \mu)^2 = n(\bar{Y} - \mu)^2
</span></p>
<p>Putting it all together:</p>
<p><span class="math display">
\sum_{i=1}^{n}(Y_i - \bar{Y})^2 = \sum_{i=1}^{n}(Y_i - \mu)^2 -
2n(\bar{Y} - \mu)^2 + n(\bar{Y} - \mu)^2
</span></p>
<p><span class="math display">
= \sum_{i=1}^{n}(Y_i - \mu)^2 - n(\bar{Y} - \mu)^2
</span></p>
<p>Therefore, dividing both sides by <span
class="math inline">n</span>:</p>
<p><span class="math display">
S^2 = \frac{1}{n}\sum_{i=1}^{n}(Y_i - \mu)^2 - (\bar{Y} - \mu)^2
</span></p>
<h3 id="taking-expectations">Taking Expectations</h3>
<p>Now we take the expected value of both sides:</p>
<p><span class="math display">
E[S^2] = E\left[\frac{1}{n}\sum_{i=1}^{n}(Y_i - \mu)^2\right] -
E[(\bar{Y} - \mu)^2]
</span></p>
<p>Let’s evaluate each term. For the first term:</p>
<p><span class="math display">
E\left[\frac{1}{n}\sum_{i=1}^{n}(Y_i - \mu)^2\right] =
\frac{1}{n}\sum_{i=1}^{n}E[(Y_i - \mu)^2]
</span></p>
<p>But <span class="math inline">E[(Y_i - \mu)^2]</span> is precisely
the definition of the population variance <span
class="math inline">\sigma^2</span> (the expected squared deviation from
the population mean). So:</p>
<p><span class="math display">
\frac{1}{n}\sum_{i=1}^{n}E[(Y_i - \mu)^2] = \frac{1}{n} \cdot n\sigma^2
= \sigma^2
</span></p>
<p>For the second term, <span class="math inline">E[(\bar{Y} -
\mu)^2]</span> is the expected squared deviation of the sample mean from
the population mean—which is exactly the variance of the sample
mean:</p>
<p><span class="math display">
E[(\bar{Y} - \mu)^2] = \text{Var}(\bar{Y}) = \frac{\sigma^2}{n}
</span></p>
<p>We proved this earlier when showing that the sample mean is
efficient.</p>
<h3 id="the-final-result">The Final Result</h3>
<p>Combining these results:</p>
<p><span class="math display">
E[S^2] = \sigma^2 - \frac{\sigma^2}{n} = \sigma^2\left(1 -
\frac{1}{n}\right) = \sigma^2 \cdot \frac{n-1}{n}
</span></p>
<p>This is the crucial finding: the expected value of <span
class="math inline">S^2</span> is not <span
class="math inline">\sigma^2</span>, but rather <span
class="math inline">\sigma^2 \cdot \frac{n-1}{n}</span>. Since <span
class="math inline">(n-1)/n &lt; 1</span> for all <span
class="math inline">n &gt; 1</span>, the uncorrected sample variance
systematically underestimates the true population variance. It is
<strong>biased downward</strong>.</p>
<section id="why-the-bias-occurs" class="callout-important"
data-icon="false">
<h2>Why the Bias Occurs</h2>
<p>The bias arises because we use <span
class="math inline">\bar{Y}</span> instead of <span
class="math inline">\mu</span> in our formula. The sample mean is
calculated from the same data we’re using to measure variation, so it’s
“closer” to the data points than the true mean would be. The deviations
<span class="math inline">(Y_i - \bar{Y})</span> are systematically
smaller than the deviations <span class="math inline">(Y_i - \mu)</span>
would be, leading to underestimation.</p>
<p>Using <span class="math inline">n-1</span> in the denominator exactly
corrects for this bias. The unbiased sample variance is:</p>
<p><span class="math display">
s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(Y_i - \bar{Y})^2
</span></p>
<p>since <span class="math inline">E[s^2] = \frac{n}{n-1} \cdot E[S^2] =
\frac{n}{n-1} \cdot \sigma^2 \cdot \frac{n-1}{n} = \sigma^2</span>.</p>
</section>
<h2 id="a-challenge-for-you">A Challenge for You</h2>
<p>Now that we’ve proven the sample variance with <span
class="math inline">n</span> in the denominator is biased, here’s a
thought question:</p>
<section id="challenge-question" class="callout-note" data-icon="false">
<h2>Challenge Question</h2>
<p>We know that <span class="math inline">S^2 =
\frac{1}{n}\sum_{i=1}^{n}(Y_i - \bar{Y})^2</span> is a
<strong>biased</strong> estimator of <span
class="math inline">\sigma^2</span>, with <span
class="math inline">E[S^2] = \frac{n-1}{n}\sigma^2</span>.</p>
<p>Can you propose an <strong>unbiased</strong> estimator of the
population variance? Don’t look it up—use what we’ve learned in this
chapter to construct one yourself.</p>
<p><strong>Hint</strong>: If you know the expected value of <span
class="math inline">S^2</span>, what simple transformation would make it
unbiased?</p>
</section>
<h2 id="synthesis-and-reflection">Synthesis and Reflection</h2>
<p>Let’s step back and consider what these three proofs reveal about the
nature of statistical estimation.</p>
<p>The sample mean emerged as the gold standard estimator not by
accident, but because it possesses two fundamental virtues: it’s
unbiased (correct on average) and efficient (most precise). These aren’t
just mathematical curiosities—they have practical implications. When you
calculate a sample mean, you can trust that you’re using the best
possible estimator given your data.</p>
<p>The sample variance case is more subtle. The natural estimator <span
class="math inline">S^2</span> turns out to be biased, but the bias is
systematic and predictable, allowing us to correct it. This illustrates
an important principle: not all biases are equal. A systematic, known
bias that we can correct is far less problematic than an unknown or
random error.</p>
<p>Moreover, these proofs showcase the power of mathematical statistics.
We’re not guessing or using intuition—we’re proving with logical
certainty that our estimators have desirable properties. This rigor is
what allows statistics to be a reliable tool for scientific
inference.</p>
<section id="key-takeaways" class="callout-important" data-icon="false">
<h2>Key Takeaways</h2>
<ol type="1">
<li><p><strong>The sample mean is unbiased</strong>: <span
class="math inline">E[\bar{Y}] = \mu</span>, regardless of sample size
or population distribution (as long as <span
class="math inline">\mu</span> is finite).</p></li>
<li><p><strong>The sample mean is efficient</strong>: Among all unbiased
estimators, <span class="math inline">\bar{Y}</span> has the minimum
variance. Equal weighting is optimal.</p></li>
<li><p><strong>The sample variance needs correction</strong>: The
natural estimator <span class="math inline">\frac{1}{n}\sum(Y_i -
\bar{Y})^2</span> underestimates <span
class="math inline">\sigma^2</span> by a factor of <span
class="math inline">(n-1)/n</span>. Dividing by <span
class="math inline">n-1</span> instead of <span
class="math inline">n</span> corrects this bias.</p></li>
<li><p><strong>Degrees of freedom matter</strong>: The <span
class="math inline">n-1</span> denominator reflects that we “lose” one
degree of freedom by using <span class="math inline">\bar{Y}</span>
instead of <span class="math inline">\mu</span>.</p></li>
</ol>
</section>
<p>These results form the foundation for much of what follows in
statistical inference. Understanding why they’re true—not just
memorizing the formulas—will serve you well as we build toward more
sophisticated techniques.</p>
