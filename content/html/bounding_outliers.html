<h1 id="bounding-outliers">Bounding Outliers</h1>
<p>Understanding the behavior of extreme values, or outliers, is crucial
    in statistical analysis. In real-world data, we often encounter
    observations that lie far from the center of a distribution. While we
    may not know the exact probability of such extreme events, probability
    inequalities allow us to establish upper bounds on how likely they are
    to occur. This chapter introduces two fundamental inequalities—Markov’s
    inequality and Chebyshev’s inequality—that help us bound the probability
    of outliers using only basic distributional properties.</p>
<h2 id="why-bounding-outliers-matters">Why Bounding Outliers
    Matters</h2>
<div class="question">
    <p>Why do we need mathematical tools to bound the probability of
        outliers?</p>
</div>
<div class="answer">
    <p>In many practical situations, we don’t know the complete probability
        distribution of a random variable. However, we often know simpler
        properties like the mean or variance. Probability inequalities allow us
        to make rigorous statements about tail probabilities (the likelihood of
        extreme values) using only this limited information. This is invaluable
        for risk assessment, quality control, and understanding the reliability
        of statistical estimates.</p>
</div>
<p>Consider a manufacturing process where you’re monitoring the weight
    of products. You know the average weight is 500 grams, but you don’t
    know the full distribution of weights. If a product weighs 1000 grams or
    more, it might indicate a defect. How can you bound the probability of
    such an outlier? This is precisely the type of question that Markov’s
    inequality addresses.</p>
<h2 id="markovs-inequality">Markov’s Inequality</h2>
<p>Markov’s inequality provides a remarkably simple bound on tail
    probabilities for non-negative random variables, requiring only
    knowledge of the mean.</p>
<div class="importantbox">
    <p><strong>Markov’s Inequality</strong></p>
    <p>Let <span class="math inline">Y</span> be a random variable defined
        over the positive subspace of <span class="math inline">\mathbb{R}^1</span>. Then for any positive constant
        <span class="math inline">a &gt; 0</span>, <span class="math display">\mathrm{P}(Y \geq a) \leq
            \frac{\mathbb{E}(Y)}{a}</span>
    </p>
</div>
<p>This inequality tells us that the probability of a non-negative
    random variable exceeding some value <span class="math inline">a</span>
    is at most the mean divided by <span class="math inline">a</span>. The
    larger the value of <span class="math inline">a</span> relative to the
    mean, the smaller this upper bound becomes.</p>
<div class="question">
    <p>What does Markov’s inequality tell us intuitively?</p>
</div>
<div class="answer">
    <p>Markov’s inequality formalizes the intuition that if a non-negative
        random variable has a small mean, it’s unlikely to take on very large
        values. For instance, if the average value is 10, the probability of
        seeing a value of 100 or more cannot exceed 10/100 = 0.1, or 10%.</p>
</div>
<h3 id="example-manufacturing-quality-control">Example: Manufacturing
    Quality Control</h3>
<p>Let’s return to our manufacturing example. Suppose the average
    product weight is <span class="math inline">\mathbb{E}(Y) = 500</span>
    grams, and all products have non-negative weight. We want to know:
    what’s the maximum probability that a randomly selected product weighs
    1000 grams or more?</p>
<p>Using Markov’s inequality with <span class="math inline">a =
        1000</span>: <span class="math display">\mathrm{P}(Y \geq 1000) \leq
        \frac{500}{1000} = 0.5</span></p>
<p>This tells us that at most 50% of products can weigh 1000 grams or
    more. While this bound might seem loose, remember that we derived it
    using only the mean—no other information about the distribution!</p>
<p>We can also ask: what’s the probability of a product weighing at
    least twice the average? <span class="math display">\mathrm{P}(Y \geq
        1000) = \mathrm{P}(Y \geq 2 \cdot 500) \leq \frac{1}{2}</span></p>
<p>More generally, the probability of exceeding <span class="math inline">k</span> times the mean is bounded by <span
        class="math inline">1/k</span>: <span class="math display">\mathrm{P}(Y
        \geq k \cdot \mathbb{E}(Y)) \leq \frac{1}{k}</span></p>
<h2 id="chebyshevs-inequality">Chebyshev’s Inequality</h2>
<p>While Markov’s inequality is powerful in its simplicity, we can
    obtain tighter bounds if we have more information. Chebyshev’s
    inequality leverages both the mean and variance to bound the probability
    of deviations from the mean in either direction.</p>
<div class="importantbox">
    <p><strong>Chebyshev’s Inequality</strong></p>
    <p>Let <span class="math inline">Y</span> be a random variable with
        finite mean <span class="math inline">\mu</span> and finite, non-zero
        variance <span class="math inline">\sigma^2</span>. Then for any real
        number <span class="math inline">k &gt; 0</span>, <span class="math display">\mathrm{P}(|Y - \mu| \geq k\sigma)
            \leq
            \frac{1}{k^2}</span></p>
</div>
<p>This inequality bounds the probability that <span class="math inline">Y</span> deviates from its mean <span
        class="math inline">\mu</span> by at least <span class="math inline">k</span> standard deviations. Notice that
    the bound
    depends on <span class="math inline">k^2</span> rather than <span class="math inline">k</span>, making it much
    tighter than Markov’s
    inequality for large deviations.</p>
<div class="question">
    <p>How does Chebyshev’s inequality improve upon Markov’s inequality?</p>
</div>
<div class="answer">
    <p>Chebyshev’s inequality provides three key advantages: (1) it applies
        to any random variable, not just non-negative ones; (2) it bounds
        deviations in both directions from the mean; and (3) it typically
        provides tighter bounds because it incorporates information about
        variability through the variance. The <span class="math inline">1/k^2</span> decay is much faster than the <span
            class="math inline">1/k</span> decay in Markov’s inequality.</p>
</div>
<h3 id="example-manufacturing-quality-control-revisited">Example:
    Manufacturing Quality Control Revisited</h3>
<p>Let’s enhance our manufacturing example with variance information.
    Suppose products have mean weight <span class="math inline">\mu =
        500</span> grams and standard deviation <span class="math inline">\sigma
        = 50</span> grams. We want to bound the probability that a product’s
    weight deviates from the mean by 100 grams or more (either heavier or
    lighter).</p>
<p>Here, we’re asking about <span class="math inline">\mathrm{P}(|Y -
        500| \geq 100)</span>. Since <span class="math inline">100 = 2 \times 50
        = 2\sigma</span>, we have <span class="math inline">k = 2</span>.
    Applying Chebyshev’s inequality: <span class="math display">\mathrm{P}(|Y - 500| \geq 100) = \mathrm{P}(|Y -
        \mu| \geq 2\sigma) \leq \frac{1}{2^2} = \frac{1}{4} = 0.25</span></p>
<p>So at most 25% of products have weights outside the range [400, 600]
    grams.</p>
<p>Let’s compare this to what Markov’s inequality would tell us. For the
    upper tail only, Markov’s inequality gives: <span class="math display">\mathrm{P}(Y \geq 600) \leq \frac{500}{600}
        \approx
        0.833</span></p>
<p>Chebyshev’s inequality provides a much tighter bound! Even though
    Chebyshev bounds both tails (products lighter than 400g and heavier than
    600g), it gives us 0.25, compared to Markov’s 0.833 for just the upper
    tail.</p>
<h3 id="the-68-95-99.7-rule-perspective">The 68-95-99.7 Rule
    Perspective</h3>
<p>If you’re familiar with the 68-95-99.7 rule for normal distributions,
    you might notice that Chebyshev’s bounds are quite conservative. For a
    normal distribution, approximately 95% of observations fall within 2
    standard deviations of the mean—meaning only about 5% fall outside this
    range. Chebyshev’s inequality bounds this at 25%.</p>
<p>This conservatism is the price we pay for generality: Chebyshev’s
    inequality holds for <em>any</em> distribution with finite mean and
    variance, not just the normal distribution. For specific distributions,
    we can often do better, but Chebyshev gives us a universal
    guarantee.</p>
<div class="question">
    <p>What’s the probability of being more than 3 standard deviations from
        the mean?</p>
</div>
<div class="answer">
    <p>Using Chebyshev’s inequality with <span class="math inline">k =
            3</span>: <span class="math display">\mathrm{P}(|Y - \mu| \geq 3\sigma)
            \leq \frac{1}{9} \approx 0.111</span> So at most about 11% of
        observations can lie more than 3 standard deviations from the mean. For
        a normal distribution, this probability is only about 0.3%, but
        Chebyshev’s bound holds for any distribution.</p>
</div>
<h2 id="practical-implications">Practical Implications</h2>
<p>These inequalities have far-reaching applications:</p>
<ul>
    <li>
        <p><strong>Quality control:</strong> Set tolerance limits based on
            guaranteed maximum defect rates</p>
    </li>
    <li>
        <p><strong>Risk management:</strong> Bound the probability of
            extreme losses without assuming specific distributions</p>
    </li>
    <li>
        <p><strong>Algorithm analysis:</strong> Bound the probability that a
            randomized algorithm performs poorly</p>
    </li>
    <li>
        <p><strong>Sample size determination:</strong> Ensure that sample
            means are close to population means with high probability</p>
    </li>
</ul>
<p>The key insight is that even with minimal information (just the mean,
    or the mean and variance), we can make rigorous probabilistic statements
    about outliers. While these bounds may not be tight for specific
    distributions, their generality and simplicity make them indispensable
    tools in statistical reasoning.</p>
<h1 id="proofs-of-probability-inequalities">Proofs of Probability
    Inequalities</h1>
<h2 id="proof-of-markovs-inequality">Proof of Markov’s Inequality</h2>
<div class="theorem">
    <p>Let <span class="math inline">Y</span> be a random variable defined
        over the positive subspace of <span class="math inline">\mathbb{R}^1</span>. Then for any positive constant
        <span class="math inline">a &gt; 0</span>, <span class="math display">\mathrm{P}(Y \geq a) \leq
            \frac{\mathbb{E}(Y)}{a}</span>
    </p>
</div>
<div class="proof">
    <p><em>Proof.</em> Since <span class="math inline">Y</span> is defined
        over the positive subspace of <span class="math inline">\mathbb{R}^1</span>, its expectation is <span
            class="math display">\mathbb{E}(Y) = \int_0^{\infty} Y \cdot
            \mathrm{P}(Y) \, dY</span></p>
    <p>Given some arbitrary positive constant <span class="math inline">a</span>, the right-hand side of this equation
        can
        be partitioned as <span class="math display">\begin{aligned}
            \mathbb{E}(Y) &amp;= \int_0^a Y \cdot \mathrm{P}(Y) \, dY +
            \int_a^{\infty} Y \cdot \mathrm{P}(Y) \, dY \\
            &amp;\geq \int_a^{\infty} Y \cdot \mathrm{P}(Y) \, dY \\
            &amp;\geq \int_a^{\infty} a \cdot \mathrm{P}(Y) \, dY \\
            &amp;= a \int_a^{\infty} \mathrm{P}(Y) \, dY \\
            &amp;= a \cdot \mathrm{P}(Y \geq a)

            \end{aligned}</span></p>
    <p>The first inequality holds because we drop a non-negative term (the
        integral from 0 to <span class="math inline">a</span>). The second
        inequality holds because <span class="math inline">Y \geq a</span> in
        the region of integration, so <span class="math inline">Y \cdot
            \mathrm{P}(Y) \geq a \cdot \mathrm{P}(Y)</span>.</p>
    <p>Dividing both sides by <span class="math inline">a</span> and
        rearranging terms, we obtain <span class="math display">\mathrm{P}(Y
            \geq a) \leq \frac{\mathbb{E}(Y)}{a}</span> ◻</p>
</div>
<h2 id="proof-of-chebyshevs-inequality">Proof of Chebyshev’s
    Inequality</h2>
<div class="theorem">
    <p>Let <span class="math inline">Y</span> be a random variable with
        finite mean <span class="math inline">\mu</span> and finite, non-zero
        variance <span class="math inline">\sigma^2</span>. Then for any real
        number <span class="math inline">k &gt; 0</span>, <span class="math display">\mathrm{P}(|Y - \mu| \geq k\sigma)
            \leq
            \frac{1}{k^2}</span></p>
</div>
<div class="proof">
    <p><em>Proof.</em> Since <span class="math inline">Y</span> has finite
        mean <span class="math inline">\mu</span>, the random variable <span class="math inline">(Y - \mu)^2</span> is
        defined over the positive
        subspace of <span class="math inline">\mathbb{R}^1</span>. By Markov’s
        inequality, <span class="math display">\mathrm{P}\left((Y - \mu)^2 \geq
            a\right) \leq \frac{\mathbb{E}\left((Y - \mu)^2\right)}{a}</span></p>
    <p>By definition, <span class="math inline">\mathbb{E}((Y - \mu)^2) =
            \sigma^2</span>. Therefore, <span class="math display">\mathrm{P}\left((Y - \mu)^2 \geq a\right) \leq
            \frac{\sigma^2}{a}</span></p>
    <p>For any real <span class="math inline">k &gt; 0</span>, define <span class="math inline">a \equiv
            k^2\sigma^2</span>. Substituting for <span class="math inline">a</span>, <span
            class="math display">\mathrm{P}\left((Y - \mu)^2 \geq k^2\sigma^2\right)
            \leq \frac{\sigma^2}{k^2\sigma^2} = \frac{1}{k^2}</span></p>
    <p>Since <span class="math inline">(Y - \mu)^2 \geq k^2\sigma^2</span>
        is equivalent to <span class="math inline">|Y - \mu| \geq
            k\sigma</span>, we have <span class="math display">\mathrm{P}(|Y - \mu|
            \geq k\sigma) \leq \frac{1}{k^2}</span> ◻</p>
</div>