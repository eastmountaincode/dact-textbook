<h1 id="foundations-of-frequentist-statistics">Foundations of
Frequentist Statistics</h1>
<p>In this chapter, we embark on a journey into the heart of frequentist
statistical inference‚Äîa framework that dominates modern empirical
research. At its core, frequentist statistics is about making
observations from a sample and then drawing inferences about the broader
population from which that sample was drawn. The fundamental question we
seek to answer is: <em>How confident can we be that the patterns we
observe in our limited sample reflect true patterns in the
population?</em></p>
<p>By the end of this chapter, you will understand the foundational
concepts that underpin frequentist inference, including the philosophy
of repeated sampling, the nature of estimators, and the mathematical
criteria we use to distinguish good estimators from poor ones.</p>
<h2 id="the-nature-of-inferential-statistics">The Nature of Inferential
Statistics</h2>
<section id="what-are-we-really-doing" class="callout-note"
data-icon="false">
<h2>What Are We Really Doing?</h2>
<p>Inferential statistics is fundamentally about making observations in
<strong>sample data</strong> and then attempting to extrapolate causal
connections or patterns to the <strong>population data</strong>. When we
successfully extrapolate these connections, we say our results are
<strong>statistically significant</strong>. When we cannot extrapolate
with confidence, we say our results are <strong>not statistically
significant</strong>.</p>
</section>
<p>This distinction‚Äîbetween what we observe in our sample and what we
can confidently claim about the population‚Äîlies at the heart of all
inferential statistics. But what exactly are we making claims about when
we talk about populations?</p>
<h3 id="population-parameters-vs.-sample-statistics">Population
Parameters vs.¬†Sample Statistics</h3>
<p>When we make claims about a population, we are not making claims
about individual observations. After all, populations are conceptually
infinite in size. Instead, we make claims about specific
<strong>parameters</strong> of the population‚Äôs distribution. The two
parameters we encounter most frequently are:</p>
<ol type="1">
<li><p><strong>The population mean</strong> (<span
class="math inline">\mu</span>): This is by far the most common
parameter we test hypotheses about in applied statistics.</p></li>
<li><p><strong>The population variance</strong> (<span
class="math inline">\sigma^2</span>): This parameter is crucial because
tests for the population mean often depend on our ability to estimate
the population variance.</p></li>
</ol>
<p>Because we never truly know the values of <span
class="math inline">\mu</span> or <span
class="math inline">\sigma^2</span>, we must estimate them using sample
data. The corresponding quantities we calculate from our sample are:</p>
<ul>
<li><strong>Sample mean</strong> (<span
class="math inline">ar{y}</span>): The analog to the population
mean</li>
<li><strong>Sample variance</strong> (<span
class="math inline">s^2</span>): The analog to the population
variance</li>
</ul>
<section id="a-critical-distinction" class="callout-important"
data-icon="false">
<h2>A Critical Distinction</h2>
<p>When we call the sample mean and sample variance ‚Äúanalogs‚Äù or
‚Äúcounterparts‚Äù to their population equivalents, we mean only that they
correspond conceptually. We are <em>not</em> claiming they are equal or
even necessarily good estimates. Establishing which sample statistics
make good estimators of population parameters is precisely what this
chapter is about.</p>
</section>
<h2 id="transformations-of-random-variables">Transformations of Random
Variables</h2>
<p>Before we dive into the philosophy of estimation, we need to develop
some mathematical machinery. In statistics, we routinely transform
data‚Äîwe take numbers, apply formulas to them, and generate new numbers.
Understanding how these transformations affect the mean and variance of
our data is essential.</p>
<h3 id="affine-transformations">Affine Transformations</h3>
<p>Consider a simple but powerful type of transformation called an
<strong>affine transformation</strong>. If we have a random variable
<span class="math inline">X</span> with mean <span
class="math inline">ar{x}</span> and variance <span
class="math inline">s^2</span>, we might create a new variable:</p>
<p><span class="math display">Y = mX + c</span></p>
<p>where <span class="math inline">m</span> is a multiplicative constant
(the slope) and <span class="math inline">c</span> is an additive
constant (the intercept). This is exactly the form of a linear equation
you‚Äôve seen since high school algebra.</p>
<p>The question is: if we know the mean and variance of <span
class="math inline">X</span>, what are the mean and variance of <span
class="math inline">Y</span>?</p>
<p>We can decompose this affine transformation into two simpler
operations:</p>
<ol type="1">
<li><strong>Translation</strong>: <span class="math inline">X ightarrow
X + c</span> (adding a constant)</li>
<li><strong>Linear transformation</strong>: <span class="math inline">X
ightarrow mX</span> (multiplying by a constant)</li>
</ol>
<h4 id="properties-of-translation">Properties of Translation</h4>
<p>When you add a constant <span class="math inline">c</span> to every
value in your dataset, creating <span class="math inline">Y = X +
c</span>:</p>
<p><span class="math display">
egin{aligned}
    ext{Mean of } Y &amp;= ar{x} + c \
    ext{Variance of } Y &amp;= s^2
nd{aligned}
</span></p>
<p>The mean shifts by exactly <span class="math inline">c</span>, but
the variance remains unchanged. Why? Because variance measures the
spread of data around the mean, and when you shift all values by the
same amount, their relative positions don‚Äôt change.</p>
<section id="connecting-to-earlier-concepts" class="callout-tip"
data-icon="false">
<h2>Connecting to Earlier Concepts</h2>
<p>You‚Äôve already encountered this idea when we discussed the <span
class="math inline">z</span>-transformation. When we subtract the mean
from a variable, we‚Äôre performing a translation that shifts the entire
distribution to have mean zero. The shape and spread of the distribution
remain the same.</p>
</section>
<h4 id="properties-of-linear-transformation">Properties of Linear
Transformation</h4>
<p>When you multiply every value by a constant <span
class="math inline">m</span>, creating <span class="math inline">Y =
mX</span>:</p>
<p><span class="math display">
egin{aligned}
    ext{Mean of } Y &amp;= mar{x} \
    ext{Variance of } Y &amp;= m^2 s^2 \
    ext{Standard deviation of } Y &amp;= |m| s
nd{aligned}
</span></p>
<p>Notice that the variance is multiplied by <span
class="math inline">m^2</span>, not <span class="math inline">m</span>.
This occurs because variance involves squared deviations, so a
multiplicative constant gets squared in the process.</p>
<h4 id="combining-both-transformations">Combining Both
Transformations</h4>
<p>For the full affine transformation <span class="math inline">Y = mX +
c</span>:</p>
<p><span class="math display">
egin{aligned}
    ext{Mean of } Y &amp;= mar{x} + c \
    ext{Variance of } Y &amp;= m^2 s^2 \
    ext{Standard deviation of } Y &amp;= |m| s
nd{aligned}
</span></p>
<p>These formulas will prove invaluable as we develop more sophisticated
statistical techniques.</p>
<h2 id="the-frequentist-philosophy-repeated-sampling">The Frequentist
Philosophy: Repeated Sampling</h2>
<p>We now arrive at the conceptual heart of frequentist statistics. The
entire edifice of frequentist inference rests on an imaginary exercise:
<strong>repeated sampling</strong>.</p>
<section id="the-thought-experiment" class="callout-note"
data-icon="false">
<h2>The Thought Experiment</h2>
<p>Imagine that we could:</p>
<ol type="1">
<li>Draw a random sample from the population</li>
<li>Calculate some statistic from that sample</li>
<li>Return the sample to the population</li>
<li>Draw another random sample</li>
<li>Calculate the statistic again</li>
<li>Repeat this process infinitely many times</li>
</ol>
<p>This thought experiment‚Äîsampling repeatedly from the same
population‚Äîforms the foundation for how we evaluate estimators in
frequentist statistics.</p>
</section>
<p>Here‚Äôs the crucial point: <em>in practice, we only sample once</em>.
But theoretically, we imagine what would happen if we could sample
infinitely many times. The behavior of our estimator across these
hypothetical repeated samples tells us whether it‚Äôs a good estimator or
not.</p>
<h3 id="the-concept-of-an-estimator">The Concept of an Estimator</h3>
<p>An <strong>estimator</strong> is simply a formula that we apply to
sample data to estimate a population parameter. Importantly, there are
infinitely many possible estimators for any given parameter.</p>
<p>For example, suppose we want to estimate the population mean <span
class="math inline">\mu</span>. Here are just a few of the infinitely
many estimators we could choose:</p>
<ul>
<li>The first observation: <span class="math inline">\hat{\mu}_1 =
y_1</span></li>
<li>The sum of the first two observations: <span
class="math inline">\hat{\mu}_2 = y_1 + y_2</span></li>
<li>The cube of the first observation: <span
class="math inline">\hat{\mu}_3 = y_1^3</span></li>
<li>The fourth power of the seventh observation times the sine of the
second: <span class="math inline">\hat{\mu}_4 = y_7^4    imes
\sin(y_2)</span></li>
<li>The sample mean: <span class="math inline">\hat{\mu}_5 = ar{y} =
rac{1}{n}\sum_{i=1}^{n} y_i</span></li>
</ul>
<p>Most of these are obviously terrible estimators. But the point is
that we can construct any formula we want, and each formula defines a
different estimator. The set of all possible estimators is infinite.</p>
<p>So how do we choose among them? How do we determine which estimators
are ‚Äúgood‚Äù and which are ‚Äúbad‚Äù?</p>
<p>The answer lies in examining the <strong>sampling
distribution</strong> of each estimator.</p>
<h3 id="sampling-distributions">Sampling Distributions</h3>
<p>For any estimator, we can imagine the repeated sampling process:</p>
<ol type="1">
<li>Draw a sample of size <span class="math inline">n</span></li>
<li>Apply the estimator to get an estimate</li>
<li>Record that estimate</li>
<li>Repeat infinitely many times</li>
</ol>
<p>The distribution of all these estimates is called the
<strong>sampling distribution</strong> of the estimator. Each different
estimator has its own sampling distribution.</p>
<section id="key-insight" class="callout-important" data-icon="false">
<h2>Key Insight</h2>
<p>The sampling distribution is a theoretical construct. We never
actually observe it because we only sample once in practice. But by
imagining what it would look like, we can develop mathematical criteria
for judging the quality of different estimators.</p>
</section>
<h2 id="a-concrete-example-estimating-from-a-simple-population">A
Concrete Example: Estimating from a Simple Population</h2>
<p>To make these abstract ideas concrete, let‚Äôs work through a simple
example. Consider a population with only three values: <span
class="math inline">\{1, 2, 3\}</span>. Since there‚Äôs one of each value,
each has probability <span class="math inline">1/3</span> of being
selected if we draw randomly from this population.</p>
<h3 id="the-true-population-parameters">The True Population
Parameters</h3>
<p>This is a discrete uniform distribution, and we can easily calculate
the true population mean and variance:</p>
<p>$$ = E[Y] = 1</p>
