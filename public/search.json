[
  {
    "objectID": "bayesian-inference",
    "href": "/chapter/bayesian-inference",
    "title": "Bayesian inference",
    "section": "",
    "text": "Bayesian inference This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "bivariate-regression",
    "href": "/chapter/bivariate-regression",
    "title": "Bivariate regression",
    "section": "",
    "text": "Bivariate regression This chapter is coming soon. Add your content here. You can add question/answer pairs, important boxes, videos, and other content following the same format as Chapter 1."
  },
  {
    "objectID": "bounding_outliers",
    "href": "/chapter/bounding_outliers",
    "title": "Bounding Outliers",
    "section": "",
    "text": "Bounding Outliers Understanding the behavior of extreme values, or outliers, is crucial in statistical analysis. In real-world data, we often encounter observations that lie far from the center of a distribution. While we may not know the exact probability of such extreme events, probability inequalities allow us to establish upper bounds on how likely they are to occur. This chapter introduces two fundamental inequalities—Markov’s inequality and Chebyshev’s inequality—that help us bound the probability of outliers using only basic distributional properties."
  },
  {
    "objectID": "bounding_outliers#why-bounding-outliers-matters",
    "href": "/chapter/bounding_outliers#why-bounding-outliers-matters",
    "title": "Bounding Outliers",
    "section": "Why Bounding Outliers Matters",
    "text": "Why do we need mathematical tools to bound the probability of outliers? In many practical situations, we don’t know the complete probability distribution of a random variable. However, we often know simpler properties like the mean or variance. Probability inequalities allow us to make rigorous statements about tail probabilities (the likelihood of extreme values) using only this limited information. This is invaluable for risk assessment, quality control, and understanding the reliability of statistical estimates. Consider a manufacturing process where you’re monitoring the weight of products. You know the average weight is 500 grams, but you don’t know the full distribution of weights. If a product weighs 1000 grams or more, it might indicate a defect. How can you bound the probability of such an outlier? This is precisely the type of question that Markov’s inequality addresses."
  },
  {
    "objectID": "bounding_outliers#markovs-inequality",
    "href": "/chapter/bounding_outliers#markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Markov’s Inequality",
    "text": "Markov’s inequality provides a remarkably simple bound on tail probabilities for non-negative random variables, requiring only knowledge of the mean. Markov’s Inequality Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} This inequality tells us that the probability of a non-negative random variable exceeding some value a is at most the mean divided by a . The larger the value of a relative to the mean, the smaller this upper bound becomes. What does Markov’s inequality tell us intuitively? Markov’s inequality formalizes the intuition that if a non-negative random variable has a small mean, it’s unlikely to take on very large values. For instance, if the average value is 10, the probability of seeing a value of 100 or more cannot exceed 10/100 = 0.1, or 10%. Example: Manufacturing Quality Control Let’s return to our manufacturing example. Suppose the average product weight is \\mathbb{E}(Y) = 500 grams, and all products have non-negative weight. We want to know: what’s the maximum probability that a randomly selected product weighs 1000 grams or more? Using Markov’s inequality with a = 1000 : \\mathrm{P}(Y \\geq 1000) \\leq \\frac{500}{1000} = 0.5 This tells us that at most 50% of products can weigh 1000 grams or more. While this bound might seem loose, remember that we derived it using only the mean—no other information about the distribution! We can also ask: what’s the probability of a product weighing at least twice the average? \\mathrm{P}(Y \\geq 1000) = \\mathrm{P}(Y \\geq 2 \\cdot 500) \\leq \\frac{1}{2} More generally, the probability of exceeding k times the mean is bounded by 1/k : \\mathrm{P}(Y \\geq k \\cdot \\mathbb{E}(Y)) \\leq \\frac{1}{k}"
  },
  {
    "objectID": "bounding_outliers#chebyshevs-inequality",
    "href": "/chapter/bounding_outliers#chebyshevs-inequality",
    "title": "Bounding Outliers",
    "section": "Chebyshev’s Inequality",
    "text": "While Markov’s inequality is powerful in its simplicity, we can obtain tighter bounds if we have more information. Chebyshev’s inequality leverages both the mean and variance to bound the probability of deviations from the mean in either direction. Chebyshev’s Inequality Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} This inequality bounds the probability that Y deviates from its mean \\mu by at least k standard deviations. Notice that the bound depends on k^2 rather than k , making it much tighter than Markov’s inequality for large deviations. How does Chebyshev’s inequality improve upon Markov’s inequality? Chebyshev’s inequality provides three key advantages: (1) it applies to any random variable, not just non-negative ones; (2) it bounds deviations in both directions from the mean; and (3) it typically provides tighter bounds because it incorporates information about variability through the variance. The 1/k^2 decay is much faster than the 1/k decay in Markov’s inequality. Example: Manufacturing Quality Control Revisited Let’s enhance our manufacturing example with variance information. Suppose products have mean weight \\mu = 500 grams and standard deviation \\sigma = 50 grams. We want to bound the probability that a product’s weight deviates from the mean by 100 grams or more (either heavier or lighter). Here, we’re asking about \\mathrm{P}(|Y - 500| \\geq 100) . Since 100 = 2 \\times 50 = 2\\sigma , we have k = 2 . Applying Chebyshev’s inequality: \\mathrm{P}(|Y - 500| \\geq 100) = \\mathrm{P}(|Y - \\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^2} = \\frac{1}{4} = 0.25 So at most 25% of products have weights outside the range [400, 600] grams. Let’s compare this to what Markov’s inequality would tell us. For the upper tail only, Markov’s inequality gives: \\mathrm{P}(Y \\geq 600) \\leq \\frac{500}{600} \\approx 0.833 Chebyshev’s inequality provides a much tigh"
  },
  {
    "objectID": "bounding_outliers#practical-implications",
    "href": "/chapter/bounding_outliers#practical-implications",
    "title": "Bounding Outliers",
    "section": "Practical Implications",
    "text": "These inequalities have far-reaching applications: Quality control: Set tolerance limits based on guaranteed maximum defect rates Risk management: Bound the probability of extreme losses without assuming specific distributions Algorithm analysis: Bound the probability that a randomized algorithm performs poorly Sample size determination: Ensure that sample means are close to population means with high probability The key insight is that even with minimal information (just the mean, or the mean and variance), we can make rigorous probabilistic statements about outliers. While these bounds may not be tight for specific distributions, their generality and simplicity make them indispensable tools in statistical reasoning. Proofs of Probability Inequalities"
  },
  {
    "objectID": "bounding_outliers#proof-of-markovs-inequality",
    "href": "/chapter/bounding_outliers#proof-of-markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Proof of Markov’s Inequality",
    "text": "Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} Proof. Since Y is defined over the positive subspace of \\mathbb{R}^1 , its expectation is \\mathbb{E}(Y) = \\int_0^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY Given some arbitrary positive constant a , the right-hand side of this equation can be partitioned as \\begin{aligned} \\mathbb{E}(Y) &= \\int_0^a Y \\cdot \\mathrm{P}(Y) \\, dY + \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} a \\cdot \\mathrm{P}(Y) \\, dY \\\\ &= a \\int_a^{\\infty} \\mathrm{P}(Y) \\, dY \\\\ &= a \\cdot \\mathrm{P}(Y \\geq a) \\end{aligned} The first inequality holds because we drop a non-negative term (the integral from 0 to a ). The second inequality holds because Y \\geq a in the region of integration, so Y \\cdot \\mathrm{P}(Y) \\geq a \\cdot \\mathrm{P}(Y) . Dividing both sides by a and rearranging terms, we obtain \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} ◻"
  },
  {
    "objectID": "bounding_outliers#proof-of-chebyshevs-inequality",
    "href": "/chapter/bounding_outliers#proof-of-chebyshevs-inequality",
    "title": "Bounding Outliers",
    "section": "Proof of Chebyshev’s Inequality",
    "text": "Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} Proof. Since Y has finite mean \\mu , the random variable (Y - \\mu)^2 is defined over the positive subspace of \\mathbb{R}^1 . By Markov’s inequality, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\mathbb{E}\\left((Y - \\mu)^2\\right)}{a} By definition, \\mathbb{E}((Y - \\mu)^2) = \\sigma^2 . Therefore, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\sigma^2}{a} For any real k > 0 , define a \\equiv k^2\\sigma^2 . Substituting for a , \\mathrm{P}\\left((Y - \\mu)^2 \\geq k^2\\sigma^2\\right) \\leq \\frac{\\sigma^2}{k^2\\sigma^2} = \\frac{1}{k^2} Since (Y - \\mu)^2 \\geq k^2\\sigma^2 is equivalent to |Y - \\mu| \\geq k\\sigma , we have \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} ◻"
  },
  {
    "objectID": "bounds-outliers",
    "href": "/chapter/bounds-outliers",
    "title": "Bounding Outliers",
    "section": "",
    "text": "Bounding Outliers Understanding the behavior of extreme values, or outliers, is crucial in statistical analysis. In real-world data, we often encounter observations that lie far from the center of a distribution. While we may not know the exact probability of such extreme events, probability inequalities allow us to establish upper bounds on how likely they are to occur. This chapter introduces two fundamental inequalities—Markov’s inequality and Chebyshev’s inequality—that help us bound the probability of outliers using only basic distributional properties."
  },
  {
    "objectID": "bounds-outliers#why-bounding-outliers-matters",
    "href": "/chapter/bounds-outliers#why-bounding-outliers-matters",
    "title": "Bounding Outliers",
    "section": "Why Bounding Outliers Matters",
    "text": "Why do we need mathematical tools to bound the probability of outliers? In many practical situations, we don’t know the complete probability distribution of a random variable. However, we often know simpler properties like the mean or variance. Probability inequalities allow us to make rigorous statements about tail probabilities (the likelihood of extreme values) using only this limited information. This is invaluable for risk assessment, quality control, and understanding the reliability of statistical estimates. Consider a manufacturing process where you’re monitoring the weight of products. You know the average weight is 500 grams, but you don’t know the full distribution of weights. If a product weighs 1000 grams or more, it might indicate a defect. How can you bound the probability of such an outlier? This is precisely the type of question that Markov’s inequality addresses."
  },
  {
    "objectID": "bounds-outliers#markovs-inequality",
    "href": "/chapter/bounds-outliers#markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Markov’s Inequality",
    "text": "Markov’s inequality provides a remarkably simple bound on tail probabilities for non-negative random variables, requiring only knowledge of the mean. Markov’s Inequality Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq rac{\\mathbb{E}(Y)}{a} {#eq-markov} This inequality tells us that the probability of a non-negative random variable exceeding some value a is at most the mean divided by a . The larger the value of a relative to the mean, the smaller this upper bound becomes. What does Markov’s inequality tell us intuitively? Markov’s inequality formalizes the intuition that if a non-negative random variable has a small mean, it’s unlikely to take on very large values. For instance, if the average value is 10, the probability of seeing a value of 100 or more cannot exceed 10/100 = 0.1, or 10%. Example: Manufacturing Quality Control Let’s return to our manufacturing example. Suppose the average product weight is \\mathbb{E}(Y) = 500 grams, and all products have non-negative weight. We want to know: what’s the maximum probability that a randomly selected product weighs 1000 grams or more? Using Markov’s inequality with a = 1000 : \\mathrm{P}(Y \\geq 1000) \\leq rac{500}{1000} = 0.5 This tells us that at most 50% of products can weigh 1000 grams or more. While this bound might seem loose, remember that we derived it using only the mean—no other information about the distribution! We can also ask: what’s the probability of a product weighing at least twice the average? $$ (Y ) = (Y"
  },
  {
    "objectID": "correlation",
    "href": "/chapter/correlation",
    "title": "Correlation",
    "section": "",
    "text": "Correlation This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "data",
    "href": "/chapter/data",
    "title": "Data",
    "section": "",
    "text": "Data This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "dichotomous-choice#introduction",
    "href": "/chapter/dichotomous-choice#introduction",
    "title": "Dichotomous Choice Modeling",
    "section": "Introduction",
    "text": "In the 1980s, researchers Ben-Akiva and Lerman interviewed commuters in Boston about their transportation choices. Their specific research question was simple but important: Does the difference in commute time between car and bus affect people’s mode choice? To answer this, they surveyed huundreds of commuters, collecting data on: - Their actual commute times by car and by bus - Their actual commuting choice (0 = drove to work, 1 = took the bus) Here’s a table showing these data for 21 of the commuters surveyed. While the authors collected data on a whole range of variables, we will just ignore them for the purpose of this chapter. In our model, we will not include any controls to keep things simple. In real life, of course, a person’s commuting choice will depend of many, many factors. ID Commute Time (minutes) Choice Auto Bus :–: :—-: :—: :——: 1 51.0 85.0 0 2 95.0 43.5 1 3 18.5 84.0 0 4 62.0 4.4 1 5 41.5 24.5 1 6 2.0 91.2 0 7 82.0 38.0 1 8 27.6 79.7 0 9 99.1 2.2 1 10 51.4 83.8 0 11 8.6 1.6 1 12 22.5 74.1 0 13 51.8 20.2 1 14 4.1 86.9 0 15 62.2 90.1 0 16 89.9 2.2 1 17 41.6 91.5 0 18 56.2 31.6 1 19 4.1 28.5 1 20 95.1 22.5 1 21 81.0 19.2 1 Some of the values in the table are very odd. And no, I double-checked, I transcribed them here correctly. The variable choice is coded as either 0 or 1, where 0 is the code for commuters who drove to work and 1 for those that took the bus. We can now sort these by this variable and calculate the difference in commute times. We will further assume that a person’s commuting choice depends only on the difference in commute time between the two options they have. Here’s the modified table. ID Commute Time (minutes) Choice Auto Bus 1 51.0 85.0 0 2 95.0 43.5 1 3 18.5 84.0 0 4 62.0 4.4 1 5 41.5 24.5 1 6 2.0 91.2 0 7 82.0 38.0 1 8 27.6 79.7 0 9 99.1 2.2 1 10 51.4 83.8 0 11 8.6 1.6 1 12 22.5 74.1 0 13 51.8 20.2 1 14 4.1 86.9 0 15 62.2 90.1 0 16 89.9 2.2 1 17 41.6 91.5 0 18 56.2 31.6 1 19 4.1 28.5 1 20 95.1 22.5 1 21 81.0 19.2 1 We can now m"
  },
  {
    "objectID": "dichotomous-choice#a-problem-of-transportation-planning",
    "href": "/chapter/dichotomous-choice#a-problem-of-transportation-planning",
    "title": "Dichotomous Choice Modeling",
    "section": "A Problem of Transportation Planning",
    "text": "In the mid-1960s, traffic congestion in the Bay Area had reached a critical juncture. The California Highway Commission faced a fundamental decision: should they continue investing in freeway expansion, or could a new mass transit system offer a better path forward? They proposed an ambitious solution—a network of buses and rail that would connect the region, fundamentally reshaping how people commuted. But there was a problem. Before committing billions in public resources, the Commission needed to answer a deceptively simple question: How many people would actually use this system? In 1969, with the first BART station under construction, the Commission faced a pilot phase evaluation. They needed to estimate ridership—not based on hunches or optimistic projections, but on actual data about people’s choices. So they conducted an extensive survey of Bay Area residents, asking a seemingly straightforward question: Would you take the bus instead of driving? Yes or no. This binary question—a dichotomous choice—would unlock something far more significant than transit planning. It would lead to the discovery of a new statistical framework that would transform how economists, marketers, and policymakers understand decision-making itself."
  },
  {
    "objectID": "dichotomous-choice#the-birth-of-a-framework-dan-mcfaddens-insight",
    "href": "/chapter/dichotomous-choice#the-birth-of-a-framework-dan-mcfaddens-insight",
    "title": "Dichotomous Choice Modeling",
    "section": "The Birth of a Framework: Dan McFadden’s Insight",
    "text": "The Commission’s first instinct was to use standard regression—treating the yes/no responses as if they were continuous measurements. Using this linear probability model, they estimated that about 15% of Bay Area residents would use the new transit system. But then they hired a young economist named Dan McFadden, recently arrived at UC Berkeley. McFadden looked at the problem differently. He recognized something fundamental: when people make discrete choices—yes or no, use transit or drive, buy or don’t buy—the standard tools of regression analysis were fundamentally mismatched to the problem. McFadden developed a new approach using what he called latent variable models . The insight was elegant: behind every observed choice lies an unobserved psychological disposition. When someone decides whether to take the bus, they’re processing information about commute time, cost, convenience, and comfort—all of which feed into a latent evaluation of the option. When that latent evaluation exceeds some threshold, they choose to use transit. Using this framework, McFadden predicted that only about 6.3% of residents would use BART. His colleagues dismissed this as too pessimistic. Yet when BART opened and ridership was measured, it came in at 6.2%—remarkably close to McFadden’s prediction. This work on discrete choice modeling was so significant that in 2000—more than three decades later—McFadden was awarded the Nobel Prize in Economics. The Nobel citation recognized his contribution: “he showed how to statistically handle fundamental aspects of microdata, namely data on the most important decisions we make in life: the choice of education, occupation, place of residence, marital status, number of children, so called discrete choices.” Today, the methods McFadden pioneered are used everywhere: predicting consumer behavior, understanding labor market decisions, analyzing election outcomes, and evaluating policy interventions."
  },
  {
    "objectID": "dichotomous-choice#back-to-boston-understanding-commuting-choices",
    "href": "/chapter/dichotomous-choice#back-to-boston-understanding-commuting-choices",
    "title": "Dichotomous Choice Modeling",
    "section": "Back to Boston: Understanding Commuting Choices",
    "text": "The data we plotted above tell a clear visual story. When the difference in commute time favors driving (negative values), people drive. When the difference favors the bus (positive values), people take transit. Yet there’s variation even within these patterns—some people take the bus despite longer commute times, and others drive even when the bus would be faster. So let’s get on with our task. We will build a model that answers the following question: If the commute time by transit could be reduce, by how much would the probability that someone will choose transit increase?"
  },
  {
    "objectID": "dichotomous-choice#the-binary-probability-function",
    "href": "/chapter/dichotomous-choice#the-binary-probability-function",
    "title": "Dichotomous Choice Modeling",
    "section": "The Binary Probability Function",
    "text": "To begin, let’s establish some basic foundations. When people make dichotomous (two-choice) decisions, we can describe the outcome using the Bernoulli distribution ."
  },
  {
    "objectID": "dichotomous-choice#section-6",
    "href": "/chapter/dichotomous-choice",
    "title": "Dichotomous Choice Modeling",
    "section": "The Bernoulli Distribution",
    "text": "For a dichotomous outcome Y that takes the value 1 with probability p and the value 0 with probability (1-p) , the probability function is: f(y) = p^y(1-p)^{1-y} The expected value of Y is simply: E(Y) = (1-p) imes 0 + p imes 1 = p In our commuting example, Y = 1 represents choosing transit and Y = 0 represents choosing a car. The probability p represents the probability that an individual will choose transit, given their specific circumstances. Following standard econometric practice, we decompose the observed outcome into a deterministic part (what we can predict) and a stochastic part (random variation): Y_i = p_i + \u001bpsilon_i where p_i is the predicted probability for individual i and \u001bpsilon_i is the error term. The key question becomes: How does the difference in commute times relate to p ?"
  },
  {
    "objectID": "dichotomous-choice#the-linear-probability-model-a-first-attempt",
    "href": "/chapter/dichotomous-choice#the-linear-probability-model-a-first-attempt",
    "title": "Dichotomous Choice Modeling",
    "section": "The Linear Probability Model: A First Attempt",
    "text": "The most straightforward approach is the Linear Probability Model (LPM) , which assumes a linear relationship between the commute time difference and the probability of choosing transit: $$p_i = \beta_0 + \beta_1"
  },
  {
    "objectID": "estimating-mean",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "",
    "text": "Estimating the population mean In this chapter, we’ll explore three fundamental properties of statistical estimators that form the backbone of statistical inference. We’ll prove that the sample mean is an unbiased estimator of the population mean, demonstrate that it’s the most efficient among all unbiased estimators, and examine why the sample variance requires a correction factor. These proofs are not merely mathematical exercises—they reveal deep truths about how we can reliably learn about populations from samples. By the end of this chapter, you will understand: What makes an estimator “unbiased” and why this matters How to compare estimators using the criterion of efficiency Why the sample variance formula uses n-1 instead of n The relationship between sample statistics and population parameters"
  },
  {
    "objectID": "estimating-mean#the-unbiasedness-of-the-sample-mean",
    "href": "/chapter/estimating-mean#the-unbiasedness-of-the-sample-mean",
    "title": "Estimating the population mean",
    "section": "The Unbiasedness of the Sample Mean",
    "text": "Let’s begin with a fundamental question that underlies all of statistical inference."
  },
  {
    "objectID": "estimating-mean#section-2",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Question",
    "text": "How do we know that the sample mean is a reliable estimator of the population mean? Could there be systematic error in our estimates?"
  },
  {
    "objectID": "estimating-mean#section-3",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Answer",
    "text": "The sample mean is unbiased , meaning that if we could take infinitely many samples and calculate the mean for each one, the average of all those sample means would exactly equal the population mean. This property holds regardless of sample size or the shape of the population distribution—it’s assumption-free except for requiring that the population mean is finite. Understanding Unbiasedness An estimator is unbiased if its expected value equals the parameter it’s trying to estimate. For the sample mean \bar{Y} estimating the population mean \\mu , we want to show: E[\bar{Y}] = \\mu This is a powerful property because it guarantees that our estimator has no systematic tendency to overestimate or underestimate the true parameter. Some samples will give us values above \\mu , others below, but on average—across infinitely many samples—we hit the target exactly. The Proof The proof is remarkably elegant. Let’s work through it step by step. We start with the definition of the sample mean: $$ \bar{Y} = rac{Y_1 + Y_2 +"
  },
  {
    "objectID": "estimating-variance",
    "href": "/chapter/estimating-variance",
    "title": "Estimating the population variance",
    "section": "",
    "text": "Estimating the population variance This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "foundations-frequentist",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "",
    "text": "Foundations of Frequentist Statistics In this chapter, we embark on a journey into the heart of frequentist statistical inference—a framework that dominates modern empirical research. At its core, frequentist statistics is about making observations from a sample and then drawing inferences about the broader population from which that sample was drawn. The fundamental question we seek to answer is: How confident can we be that the patterns we observe in our limited sample reflect true patterns in the population? By the end of this chapter, you will understand the foundational concepts that underpin frequentist inference, including the philosophy of repeated sampling, the nature of estimators, and the mathematical criteria we use to distinguish good estimators from poor ones."
  },
  {
    "objectID": "foundations-frequentist#section-2",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "What Are We Really Doing?",
    "text": "Inferential statistics is fundamentally about making observations in sample data and then attempting to extrapolate causal connections or patterns to the population data . When we successfully extrapolate these connections, we say our results are statistically significant . When we cannot extrapolate with confidence, we say our results are not statistically significant . This distinction—between what we observe in our sample and what we can confidently claim about the population—lies at the heart of all inferential statistics. But what exactly are we making claims about when we talk about populations? Population Parameters vs. Sample Statistics When we make claims about a population, we are not making claims about individual observations. After all, populations are conceptually infinite in size. Instead, we make claims about specific parameters of the population’s distribution. The two parameters we encounter most frequently are: The population mean ( \\mu ): This is by far the most common parameter we test hypotheses about in applied statistics. The population variance ( \\sigma^2 ): This parameter is crucial because tests for the population mean often depend on our ability to estimate the population variance. Because we never truly know the values of \\mu or \\sigma^2 , we must estimate them using sample data. The corresponding quantities we calculate from our sample are: Sample mean ( \bar{y} ): The analog to the population mean Sample variance ( s^2 ): The analog to the population variance"
  },
  {
    "objectID": "foundations-frequentist#section-3",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "A Critical Distinction",
    "text": "When we call the sample mean and sample variance “analogs” or “counterparts” to their population equivalents, we mean only that they correspond conceptually. We are not claiming they are equal or even necessarily good estimates. Establishing which sample statistics make good estimators of population parameters is precisely what this chapter is about."
  },
  {
    "objectID": "foundations-frequentist#transformations-of-random-variables",
    "href": "/chapter/foundations-frequentist#transformations-of-random-variables",
    "title": "Foundations of Frequentist Statistics",
    "section": "Transformations of Random Variables",
    "text": "Before we dive into the philosophy of estimation, we need to develop some mathematical machinery. In statistics, we routinely transform data—we take numbers, apply formulas to them, and generate new numbers. Understanding how these transformations affect the mean and variance of our data is essential. Affine Transformations Consider a simple but powerful type of transformation called an affine transformation . If we have a random variable X with mean \bar{x} and variance s^2 , we might create a new variable: Y = mX + c where m is a multiplicative constant (the slope) and c is an additive constant (the intercept). This is exactly the form of a linear equation you’ve seen since high school algebra. The question is: if we know the mean and variance of X , what are the mean and variance of Y ? We can decompose this affine transformation into two simpler operations: Translation : X ightarrow X + c (adding a constant) Linear transformation : X ightarrow mX (multiplying by a constant) Properties of Translation When you add a constant c to every value in your dataset, creating Y = X + c : \begin{aligned} ext{Mean of } Y &= \bar{x} + c \\ ext{Variance of } Y &= s^2 \u001bnd{aligned} The mean shifts by exactly c , but the variance remains unchanged. Why? Because variance measures the spread of data around the mean, and when you shift all values by the same amount, their relative positions don’t change."
  },
  {
    "objectID": "foundations-frequentist#section-5",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Connecting to Earlier Concepts",
    "text": "You’ve already encountered this idea when we discussed the z -transformation. When we subtract the mean from a variable, we’re performing a translation that shifts the entire distribution to have mean zero. The shape and spread of the distribution remain the same. Properties of Linear Transformation When you multiply every value by a constant m , creating Y = mX : \begin{aligned} ext{Mean of } Y &= m\bar{x} \\ ext{Variance of } Y &= m^2 s^2 \\ ext{Standard deviation of } Y &= |m| s \u001bnd{aligned} Notice that the variance is multiplied by m^2 , not m . This occurs because variance involves squared deviations, so a multiplicative constant gets squared in the process. Combining Both Transformations For the full affine transformation Y = mX + c : \begin{aligned} ext{Mean of } Y &= m\bar{x} + c \\ ext{Variance of } Y &= m^2 s^2 \\ ext{Standard deviation of } Y &= |m| s \u001bnd{aligned} These formulas will prove invaluable as we develop more sophisticated statistical techniques."
  },
  {
    "objectID": "foundations-frequentist#the-frequentist-philosophy-repeated-sampling",
    "href": "/chapter/foundations-frequentist#the-frequentist-philosophy-repeated-sampling",
    "title": "Foundations of Frequentist Statistics",
    "section": "The Frequentist Philosophy: Repeated Sampling",
    "text": "We now arrive at the conceptual heart of frequentist statistics. The entire edifice of frequentist inference rests on an imaginary exercise: repeated sampling ."
  },
  {
    "objectID": "foundations-frequentist#section-7",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "The Thought Experiment",
    "text": "Imagine that we could: Draw a random sample from the population Calculate some statistic from that sample Return the sample to the population Draw another random sample Calculate the statistic again Repeat this process infinitely many times This thought experiment—sampling repeatedly from the same population—forms the foundation for how we evaluate estimators in frequentist statistics. Here’s the crucial point: in practice, we only sample once . But theoretically, we imagine what would happen if we could sample infinitely many times. The behavior of our estimator across these hypothetical repeated samples tells us whether it’s a good estimator or not. The Concept of an Estimator An estimator is simply a formula that we apply to sample data to estimate a population parameter. Importantly, there are infinitely many possible estimators for any given parameter. For example, suppose we want to estimate the population mean \\mu . Here are just a few of the infinitely many estimators we could choose: The first observation: \\hat{\\mu}_1 = y_1 The sum of the first two observations: \\hat{\\mu}_2 = y_1 + y_2 The cube of the first observation: \\hat{\\mu}_3 = y_1^3 The fourth power of the seventh observation times the sine of the second: \\hat{\\mu}_4 = y_7^4 imes \\sin(y_2) The sample mean: \\hat{\\mu}_5 = \bar{y} = rac{1}{n}\\sum_{i=1}^{n} y_i Most of these are obviously terrible estimators. But the point is that we can construct any formula we want, and each formula defines a different estimator. The set of all possible estimators is infinite. So how do we choose among them? How do we determine which estimators are “good” and which are “bad”? The answer lies in examining the sampling distribution of each estimator. Sampling Distributions For any estimator, we can imagine the repeated sampling process: Draw a sample of size n Apply the estimator to get an estimate Record that estimate Repeat infinitely many times The distribution of all these estimates is called the sampling distribution"
  },
  {
    "objectID": "foundations-frequentist#section-8",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Key Insight",
    "text": "The sampling distribution is a theoretical construct. We never actually observe it because we only sample once in practice. But by imagining what it would look like, we can develop mathematical criteria for judging the quality of different estimators."
  },
  {
    "objectID": "foundations-frequentist#a-concrete-example-estimating-from-a-simple-population",
    "href": "/chapter/foundations-frequentist#a-concrete-example-estimating-from-a-simple-population",
    "title": "Foundations of Frequentist Statistics",
    "section": "A Concrete Example: Estimating from a Simple Population",
    "text": "To make these abstract ideas concrete, let’s work through a simple example. Consider a population with only three values: \\{1, 2, 3\\} . Since there’s one of each value, each has probability 1/3 of being selected if we draw randomly from this population. The True Population Parameters This is a discrete uniform distribution, and we can easily calculate the true population mean and variance: $$ = E[Y] = 1"
  },
  {
    "objectID": "graphing",
    "href": "/chapter/graphing",
    "title": "Graphing",
    "section": "",
    "text": "Graphing This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "intro-data-analytics",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "",
    "text": "The Purpose of Data Analytics In this chapter, we’ll explore the fundamental purpose and scope of data analytics. By the end of this chapter, you will understand: The distinction between correlation and causation How patterns emerge from randomness The difference between population and sample data The two primary goals of statistical analysis The philosophical divide between frequentist and Bayesian approaches"
  },
  {
    "objectID": "intro-data-analytics#section-2",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Key Question",
    "text": "What is the ultimate goal of data analytics? Data analytics is fundamentally about understanding cause and effect relationships in the world. While it’s easy to observe that two variables move together—that they are correlated—establishing causation is far more challenging and far more valuable. Consider a simple example: we might observe that ice cream sales and drowning incidents are correlated. They both increase during summer months. But does ice cream cause drowning? Of course not. Both are caused by a third factor: warm weather, which leads people to buy ice cream and also to swim more frequently."
  },
  {
    "objectID": "intro-data-analytics#section-3",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Important Distinction",
    "text": "Correlation does not imply causation. Two variables can move together without one causing the other. Establishing causal relationships requires careful analysis and often experimental design. The distinction between correlation and causation is not merely academic—it has profound implications for how we understand the world and make decisions. Consider the famous closing lines of Robert Frost’s poem “The Road Not Taken”: Two roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference. Frost claims that taking the road less traveled “made all the difference” to his life. But as statisticians, we must ask: how does he know? To establish causation, we would need a counterfactual —an alternative version of his life where he took the other road. Without observing this counterfactual, Frost cannot definitively claim that his choice caused the difference in his life’s trajectory. Perhaps his life would have turned out similarly regardless of which road he chose. Or perhaps taking the more traveled road would have led to even better outcomes. This challenge—the impossibility of observing counterfactuals in our own lives—is precisely what makes causal inference so difficult and why rigorous statistical methods are essential. In policy work—especially environmental policy and climate science—we need causal understanding. When we ask “how much warming will occur if we add X more tons of carbon dioxide to the atmosphere?”, we’re asking a causal question. The relationship between greenhouse gas concentrations and temperature change is incredibly complicated, random, and stochastic. Yet climate scientists have developed good estimates of what is called the global warming potential of different greenhouse gases. These estimates are based on a causal understanding of physical processes, not mere correlation. This is why data analytics matters: we want to establish cause and effect, not just observe patterns. We’re here to understand how th"
  },
  {
    "objectID": "intro-data-analytics#from-randomness-to-pattern",
    "href": "/chapter/intro-data-analytics#from-randomness-to-pattern",
    "title": "The Purpose of Data Analytics",
    "section": "From Randomness to Pattern",
    "text": "One of the most remarkable features of statistical analysis is how patterns emerge from what initially appears to be pure randomness. When we look at individual observations, they often seem chaotic and unpredictable. But when we collect enough observations, macro-level patterns begin to reveal themselves."
  },
  {
    "objectID": "intro-data-analytics#section-5",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Conceptual Question",
    "text": "How can predictable patterns emerge from random individual events?"
  },
  {
    "objectID": "intro-data-analytics#section-6",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "While individual events may be unpredictable, the aggregate behavior of many random events often follows predictable patterns. This is the fundamental insight of probability theory—that randomness at the micro level produces regularity at the macro level. Consider the classic example of a Galton board (sometimes called a bean machine). When a single ball drops through the board, hitting pegs as it falls, its path is essentially random—at each peg, it bounces left or right unpredictably. We cannot predict where any individual ball will land. However, when we drop hundreds or thousands of balls, a clear pattern emerges: they pile up in the shape of a bell curve, forming what statisticians call the normal distribution . The randomness of individual ball drops gives way to a predictable aggregate pattern. This emergence of order from randomness is not magic—it’s mathematics. And it’s the foundation of statistical inference."
  },
  {
    "objectID": "intro-data-analytics#section-8",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Beware of Normalitis",
    "text": "One common misconception in statistics is that every pattern follows the normal distribution (the familiar bell curve). This is simply not true. While the normal distribution is important and widely applicable, it is just one of dozens of probability distributions used in statistics. I call the mistaken belief that everything is normally distributed normalitis —and it’s a condition to avoid. Different real-world phenomena follow different distributions: Bernoulli distribution : Events with only two possible outcomes (coin flip: heads or tails; ball at a peg: left or right) Binomial distribution : The number of successes in a fixed number of independent Bernoulli trials (how many heads in 10 coin flips?) Poisson distribution : Count data and waiting times (how long you wait for the bus each day; how many customers arrive per hour) Normal distribution : Many continuous phenomena in nature and society (heights, test scores, measurement errors) These distributions are often mathematically related. For instance, when you sum up many independent Bernoulli trials (each ball on the Galton board making left-right decisions), you get a binomial distribution. And when the number of trials becomes very large, that binomial distribution approximates the normal distribution."
  },
  {
    "objectID": "intro-data-analytics#section-9",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Conceptual Question",
    "text": "The word “Poisson” comes from French. What does it mean, and who was Poisson?"
  },
  {
    "objectID": "intro-data-analytics#section-10",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "“Poisson” means “fish” in French (related to “Pisces,” the astrological sign). Siméon Denis Poisson was a French mathematician and physicist who discovered this particular distribution, which describes the probability of a given number of events occurring in a fixed interval of time or space. Throughout this course, we’ll work with many different distributions. Each captures a different kind of pattern in data. The key is learning to recognize which pattern fits which situation—and to never assume that one pattern applies universally."
  },
  {
    "objectID": "intro-data-analytics#population-and-sample",
    "href": "/chapter/intro-data-analytics#population-and-sample",
    "title": "The Purpose of Data Analytics",
    "section": "Population and Sample",
    "text": "In statistical analysis, we make a crucial distinction between two types of data:"
  },
  {
    "objectID": "intro-data-analytics#section-12",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Definitions",
    "text": "Population : All possible data points that exist in the world for a given phenomenon. This includes data that has been collected, data that could be collected, and data that will exist in the future. Sample : A subset of the population that we have actually collected and can analyze. The sample is always smaller—often infinitesimally smaller—than the population. Consider studying human height. The population would include the heights of all humans who have ever lived, are living now, and will live in the future. That’s an enormous—indeed, infinite—amount of data. Your sample might be the heights of 1,000 people surveyed in a particular city during a particular year. No matter how large your sample, it remains tiny compared to the population. Even if you collect data on millions of individuals, that’s still just a tiny fraction of the theoretical population. As a mathematical principle: \\lim_{n o \\infty} ext{Sample} = ext{Population} As the sample size approaches infinity, it approaches the population. But in practice, our samples are always finite and small relative to the population."
  },
  {
    "objectID": "intro-data-analytics#two-goals-of-statistical-analysis",
    "href": "/chapter/intro-data-analytics#two-goals-of-statistical-analysis",
    "title": "The Purpose of Data Analytics",
    "section": "Two Goals of Statistical Analysis",
    "text": "What do we do with sample data once we collect it? We pursue one or both of two fundamental goals: 1. Description The first goal is to describe the data we have collected. This is called descriptive statistics . We might: Calculate the average (mean) age in our sample Determine the most common (mode) educational level Find the middle value (median) of family incomes Measure the spread (variance or standard deviation) of environmental commitment scores Descriptive statistics summarize and organize data in meaningful ways. They help us understand what our sample looks like. When we describe sample data, we’re making statements only about that specific set of observations. 2. Inference The second, more ambitious goal is to infer patterns and relationships that extend beyond our sample to the broader population. This is called inferential statistics or statistical inference . Suppose we collect sample data on 25 different variables for each person: age, education level, commitment to environmental causes, family income, transportation choices, and so on. We might discover relationships among these variables in our sample—for instance, that people with higher education levels tend to show stronger commitment to environmental causes. The question then becomes: Can we extrapolate this relationship from our tiny sample to the entire population? Can we say with confidence that the relationship we found in this specific dataset also exists more broadly? This is the central challenge of inferential statistics. We observe patterns in our sample and attempt to make general claims about the population. The entire machinery of statistical inference—hypothesis tests, confidence intervals, p-values, regression analysis—exists to help us make this logical leap from sample to population in a rigorous, quantifiable way."
  },
  {
    "objectID": "intro-data-analytics#section-14",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Reflective Question",
    "text": "Why is it more valuable to make inferences about the population than to simply describe our sample?"
  },
  {
    "objectID": "intro-data-analytics#section-15",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "Describing our sample tells us only about the specific observations we happened to collect. But policy decisions, scientific theories, and practical applications require understanding that extends beyond our particular sample. We need to know whether the patterns we observe are likely to hold generally, not just in the specific cases we studied. This is what makes statistical inference so powerful and so essential for decision-making. When we perform inference successfully—when we can say with justified confidence that our sample findings reflect population patterns—we achieve what statisticians call external validity . But before we can even attempt to generalize to the population, we must first ensure that our findings within the sample are sound. When our causal analysis within the sample is properly conducted and the relationships we identify are genuine (not artifacts of confounding variables or measurement error), we say our analysis has internal validity . Both forms of validity are essential for credible statistical work."
  },
  {
    "objectID": "intro-data-analytics#two-philosophical-approaches-to-inference",
    "href": "/chapter/intro-data-analytics#two-philosophical-approaches-to-inference",
    "title": "The Purpose of Data Analytics",
    "section": "Two Philosophical Approaches to Inference",
    "text": "How many fundamentally different approaches exist for making statistical inferences? The answer is two: the frequentist approach and the Bayesian approach . These represent two distinct philosophical frameworks for reasoning about probability and uncertainty. The Frequentist Approach The frequentist approach, which has dominated statistical practice for much of the 20th century, interprets probability in terms of long-run frequencies. From this perspective, probability statements only make sense for events that can be repeated many times. Consider flipping a coin. A frequentist interprets “the probability of heads is 0.5” to mean: if we flip this coin infinitely many times, heads will appear in 50% of the flips. Probability, in this view, is an objective property of the world—a statement about what would happen if we could repeat an experiment indefinitely. This philosophical stance has important implications. Imagine I flip a coin and catch it in my hand, concealing the result. I know how it landed, but you don’t. What is the probability that it landed heads?"
  },
  {
    "objectID": "intro-data-analytics#section-17",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Thought Experiment",
    "text": "I’ve just flipped a coin and caught it in my closed hand. I can see the result, but you cannot. What is the probability that the coin shows heads? A frequentist would say: the probability is either 0 or 1, depending on how it actually landed. If it landed heads, the probability is 1 (certainty). If it landed tails, the probability is 0 (impossibility). The coin has already landed—there’s nothing probabilistic about it anymore. The event has occurred, and its outcome is now a fact of the world, even if you don’t know what that fact is. This reveals a key feature of frequentist thinking: probabilities apply to events that haven’t happened yet , not to events that have already occurred but whose outcomes we simply don’t know. From a frequentist perspective, once the coin has landed, talking about the “probability” of how it landed is meaningless. It landed some particular way. The uncertainty you feel is about your knowledge, not about the event itself. The Bayesian Approach The Bayesian approach takes a fundamentally different view. Bayesians interpret probability as a measure of our degree of belief or state of knowledge about an event. Probability, from this perspective, is subjective—it represents how confident we are, given the information we have. Let’s return to the coin in my hand. A Bayesian would say: given that you don’t know how it landed and you have no reason to believe the coin is unfair, your probability assessment should be 0.5. This doesn’t mean the coin is somehow in a superposition of states. Rather, it means that given your current state of knowledge, you should be equally uncertain about whether it shows heads or tails. If I were to give you a hint—say, “It’s not tails”—a Bayesian would immediately update your probability to 1 for heads. Your degree of belief changes as you gain new information, even though the physical state of the coin hasn’t changed at all."
  },
  {
    "objectID": "intro-data-analytics#section-18",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Fundamental Philosophical Difference",
    "text": "Frequentist view : Probability is an objective property of repeatable events. It doesn’t make sense to assign probabilities to fixed but unknown quantities. Bayesian view : Probability represents our degree of belief or state of knowledge. We can assign probabilities to any uncertain proposition, including fixed but unknown quantities. This philosophical difference leads to very different statistical methodologies. Frequentists develop procedures that work well in the long run—if we used this test over and over, we’d make correct decisions most of the time. Bayesians explicitly incorporate prior knowledge and update their beliefs as new evidence arrives. Most practicing statisticians today are implicitly Bayesian in their everyday reasoning about uncertainty, even if they use frequentist methods in their formal analyses. When we say “there’s a 70% chance it will rain tomorrow,” we’re thinking like Bayesians—probability as degree of belief. When we conduct a hypothesis test with a significance level of 0.05, we’re using frequentist methodology—probability as long-run frequency. Which Approach Is “Right”? Neither approach is universally correct or incorrect. They answer different questions and serve different purposes. Frequentist methods provide objective procedures with well-understood long-run properties, which makes them particularly valuable in fields like medical research where regulatory decisions require clear standards. Bayesian methods allow us to explicitly incorporate prior knowledge and provide direct probability statements about hypotheses, which makes them particularly valuable in fields where we have genuine prior information and want to update our beliefs. Throughout this course, we’ll primarily use frequentist methods, as these remain the dominant framework in most applied fields and are what you’ll encounter in published research. However, we’ll also discuss Bayesian perspectives where they provide valuable insights or alternative ways of thinking a"
  },
  {
    "objectID": "intro-data-analytics#understanding-hypothesis-testing-concepts",
    "href": "/chapter/intro-data-analytics#understanding-hypothesis-testing-concepts",
    "title": "The Purpose of Data Analytics",
    "section": "Understanding Hypothesis Testing Concepts",
    "text": "Before we can intelligently discuss either frequentist or Bayesian inference, we need to understand some fundamental concepts that appear throughout statistical testing. These ideas—particularly around errors in decision-making—form the conceptual foundation for statistical inference. Types of Errors When we conduct a statistical test, we’re making a decision: either reject a hypothesis or fail to reject it. Like any decision made under uncertainty, we can make mistakes. There are two types of mistakes we might make:"
  },
  {
    "objectID": "intro-data-analytics#section-20",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Type I Error",
    "text": "A Type I error occurs when we reject a hypothesis that is actually correct. We declare that something is happening when, in fact, it is not. In medical testing: declaring a healthy patient is sick (false positive) In criminal justice: convicting an innocent person In scientific research: claiming we’ve found an effect when none exists"
  },
  {
    "objectID": "intro-data-analytics#section-21",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Type II Error",
    "text": "A Type II error occurs when we fail to reject a hypothesis that is actually false. We fail to detect something that is really happening. In medical testing: declaring a sick patient is healthy (false negative) In criminal justice: acquitting a guilty person In scientific research: failing to detect an effect that actually exists These two types of errors are in tension with each other. If we make it harder to commit a Type I error (by requiring very strong evidence before rejecting a hypothesis), we inevitably make it easier to commit a Type II error (we’ll fail to detect real effects more often). Conversely, if we’re very eager to detect effects (reducing Type II errors), we’ll end up making more Type I errors by seeing patterns that aren’t really there. The P-Value The p-value is the probability of making a Type I error—the probability of rejecting a correct hypothesis. More precisely, it’s the probability of observing data as extreme as (or more extreme than) what we actually observed, assuming the hypothesis we’re testing is true. The p-value is calculated from your data using statistical procedures. It’s an output of your analysis, not an input. In the old days, p-values were looked up in printed tables at the back of statistics textbooks. Today, statistical software calculates them instantly."
  },
  {
    "objectID": "intro-data-analytics#section-22",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Common Misconception",
    "text": "The p-value is not “the probability that our results are wrong” or “the probability that the hypothesis is true.” It is specifically the probability of observing our data (or more extreme data) if the hypothesis we’re testing is actually correct. The Significance Level (α) The significance level , denoted by the Greek letter α (alpha), is the threshold probability you choose before collecting data. It represents how much Type I error risk you’re willing to tolerate. Commonly used significance levels include: - α = 0.05 (5%): The most common choice in many fields - α = 0.01 (1%): Used when Type I errors are particularly costly - α = 0.10 (10%): Used when Type I errors are less concerning or when sample sizes are small Here’s the crucial point: you choose α before looking at your data . The significance level is an input to your analysis, while the p-value is an output. You then compare them: If p-value < α: Reject the hypothesis (the evidence is strong enough) If p-value ≥ α: Fail to reject the hypothesis (the evidence is not strong enough) Why We Never “Accept” Hypotheses Notice the careful language: we “reject” or “fail to reject” hypotheses. We never “accept” a hypothesis. Why this asymmetry? The reason is fundamental to the nature of scientific reasoning. Consider the history of physics. About 500 years ago, Isaac Newton developed his theory of gravity, which explained why objects fall to the ground. For over two centuries, Newton’s theory was supported by all available evidence. Scientists didn’t say “we accept Newton’s theory as correct”—they said “we fail to reject it; it’s the best explanation we have so far.” Then, about 100 years ago, Albert Einstein developed general relativity, which showed that Newton’s theory, while extremely useful for everyday purposes, is actually incorrect in important ways. Einstein’s theory superseded Newton’s. But does this mean Einstein’s theory is “correct”? Not necessarily. It’s the best explanation we have now, consistent wit"
  },
  {
    "objectID": "intro-data-analytics#section-23",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Scientific Humility",
    "text": "In science, we can demonstrate that theories are wrong or false (by finding contradictory evidence), but we can never prove that theories are correct or true (because future evidence might contradict them). This is why we never “accept” hypotheses—we only fail to reject them given current evidence. This principle, articulated by philosopher Karl Popper, is called falsificationism . Scientific theories can be falsified but never verified with absolute certainty. This is why statistical hypothesis testing is framed around rejection rather than acceptance. Statistical Power There’s one more important concept related to errors: statistical power . Power is defined as the probability of not making a Type II error—that is, the probability of correctly rejecting a false hypothesis. High statistical power is desirable: it means your test is good at detecting effects when they exist. Power depends on several factors: - Sample size (larger samples → higher power) - Effect size (larger effects → easier to detect → higher power) - Significance level (higher α → higher power, but also more Type I errors) - Variability in the data (less noise → higher power) While there’s no standard name for the “probability of making a Type II error” (parallel to how we call Type I error probability the “p-value”), it’s typically denoted β (beta). Then power = 1 - β."
  },
  {
    "objectID": "intro-data-analytics#looking-ahead",
    "href": "/chapter/intro-data-analytics#looking-ahead",
    "title": "The Purpose of Data Analytics",
    "section": "Looking Ahead",
    "text": "Throughout this course, we’ll develop both descriptive and inferential tools. We’ll learn to: Visualize data through graphs and charts Calculate summary statistics that capture essential features of datasets Recognize different probability distributions and understand when each applies Use sample data to make justified inferences about populations Establish cause-and-effect relationships through careful analysis Navigate the philosophical differences between frequentist and Bayesian approaches Most importantly, we’ll engage in abstract thinking about data and probability. Statistics is not just a collection of computational procedures—it’s a coherent framework for reasoning about uncertainty, variability, and inference. Understanding this framework will serve you in any field where data and evidence matter. The goal of this book is not merely to learn formulas and procedures, but to develop statistical intuition—to think clearly about randomness, patterns, causation, and inference. This kind of thinking is increasingly essential in environmental policy, climate science, economics, public health, and virtually every domain where evidence-based decision-making matters. We’ll build this understanding gradually, starting with the foundations of probability and working our way up to sophisticated inferential methods. Along the way, we’ll grapple with deep questions: How do we know what we know? What does it mean for evidence to support a claim? How much uncertainty should we tolerate in our conclusions? These aren’t just technical questions—they’re fundamental questions about knowledge itself, approached through the lens of mathematical reasoning."
  },
  {
    "objectID": "multivariate-regression",
    "href": "/chapter/multivariate-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "Multiple Regression In 1978, David Harrison and Daniel Rubinfeld published a groundbreaking study on housing values and air pollution in the Boston metropolitan area. Their work introduced what has become one of the most studied datasets in econometrics and demonstrated how hedonic pricing models can be used to value environmental amenities—specifically, how air quality affects property values. The Harrison-Rubinfeld model remains a cornerstone example in applied econometrics courses because it elegantly combines theory with empirical analysis, using a rich set of neighborhood characteristics to explain median home values across census tracts in the Boston area. What is a hedonic pricing model, and why is it useful for valuing environmental goods? A hedonic pricing model decomposes the price of a good into the value of its constituent characteristics. For housing, this means breaking down the home price into components attributable to structural features (number of rooms, age), neighborhood characteristics (crime rate, school quality), and environmental amenities (air quality, proximity to employment centers). This approach is particularly valuable for environmental economics because many environmental goods—like clean air—don’t have explicit market prices. By observing how home values change with air quality while controlling for other factors, we can infer people’s willingness to pay for cleaner air. This is crucial for cost-benefit analysis of environmental regulations."
  },
  {
    "objectID": "multivariate-regression#the-model-specification",
    "href": "/chapter/multivariate-regression#the-model-specification",
    "title": "Multiple Regression",
    "section": "The Model Specification",
    "text": "The Harrison-Rubinfeld model estimates the logarithm of median home value as a function of 13 explanatory variables: \begin{aligned} \\log( ext{MEDV}) = \beta_0 &+ \beta_1 ext{ NOX}^2 + \beta_2 ext{ RM}^2 + \beta_3 ext{ AGE} + \beta_4 ( ext{B} - 0.63)^2 \\ &+ \beta_5 \\log( ext{LSTAT}) + \beta_6 ext{ CRIM} + \beta_7 ext{ ZN} + \beta_8 ext{ INDUS} \\ &+ \beta_9 ext{ TAX} + \beta_{10} ext{ PTRATIO} + \beta_{11} ext{ CHAS} \\ &+ \beta_{12} \\log( ext{DIS}) + \beta_{13} \\log( ext{RAD}) + \u001bpsilon \u001bnd{aligned} Before interpreting the coefficients, let’s understand what each variable represents and why certain functional forms were chosen."
  },
  {
    "objectID": "multivariate-regression#the-variables-and-their-transformations",
    "href": "/chapter/multivariate-regression#the-variables-and-their-transformations",
    "title": "Multiple Regression",
    "section": "The Variables and Their Transformations",
    "text": "NOX (Nitric Oxide Concentration) : Annual average concentration of nitric oxides in parts per 10 million, measured at the census tract level. This is the key environmental variable in the study. The model includes NOX² rather than NOX itself. This quadratic specification allows for a nonlinear relationship between air pollution and housing values—suggesting that the marginal effect of pollution may increase at higher pollution levels. Why might the relationship between pollution and home values be nonlinear? There are several economic reasons to expect nonlinearity. First, at very low pollution levels, small increases may have minimal health impacts and thus little effect on property values. But at higher levels, additional pollution could have increasingly severe health consequences, making marginal increases more harmful. Second, there may be threshold effects—once pollution reaches certain levels, it becomes visibly obvious (as smog) or causes noticeable health effects, triggering a sharper decline in willingness to pay for homes in that area. Third, people who are highly sensitive to pollution likely already avoid high-pollution areas, so the remaining residents may be those who are relatively less concerned about pollution, leading to smaller marginal price effects at higher pollution levels. However, this selection effect would actually suggest the opposite of what Harrison and Rubinfeld found. RM (Average Number of Rooms) : The average number of rooms per dwelling in the census tract. This is a proxy for house size and quality. The model includes RM² to capture potential nonlinear effects of house size. Larger homes may command disproportionately higher prices, or there may be diminishing returns to additional rooms. AGE (Proportion of Old Units) : The proportion of owner-occupied units built prior to 1940. This captures the age composition of the housing stock. B (Proportion Black) : A transformation of the proportion of Black residents, specifically (1000(B"
  },
  {
    "objectID": "multivariate-regression#interpreting-the-coefficients",
    "href": "/chapter/multivariate-regression#interpreting-the-coefficients",
    "title": "Multiple Regression",
    "section": "Interpreting the Coefficients",
    "text": "Now that we understand the variables, let’s interpret what each coefficient tells us. The dependent variable is log(MEDV), which means we need to be careful about the interpretation depending on whether the independent variable is in levels, logs, or transformed. Environmental Quality: \beta_1 (NOX²) Since the model includes NOX², the effect of pollution on home values is: $$ rac{( ext{MEDV})}{ext{NOX}} = 2\beta_1"
  },
  {
    "objectID": "normal-distribution",
    "href": "/chapter/normal-distribution",
    "title": "The normal distribution",
    "section": "",
    "text": "The normal distribution This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "operators-properties",
    "href": "/chapter/operators-properties",
    "title": "Expectation and Variance Operators",
    "section": "",
    "text": "Expectation and Variance Operators Statistical operators are powerful tools that transform random variables in systematic ways. In this chapter, we’ll explore two fundamental operators: the expectation operator and the variance operator. These operators will appear throughout the rest of this book, so understanding their properties deeply will pay dividends as we tackle more complex statistical concepts. By the end of this chapter, you will be able to: Define what an operator is in the statistical context Calculate and interpret expected values Calculate and interpret variances Apply the properties of expectation and variance to simplify complex expressions Understand how these operators behave under linear transformations"
  },
  {
    "objectID": "operators-properties#section-2",
    "href": "/chapter/operators-properties",
    "title": "Expectation and Variance Operators",
    "section": "Definition",
    "text": "An operator is a mapping that takes elements from one space and produces elements in another space (which may be the same space). In statistics, operators act on random variables to produce new quantities. Think of an operator as a special kind of function that acts on random variables rather than on simple numbers. Just as the square root function takes a number and returns another number, statistical operators take random variables and return quantities that summarize key features of those variables."
  },
  {
    "objectID": "operators-properties#section-3",
    "href": "/chapter/operators-properties",
    "title": "Expectation and Variance Operators",
    "section": "Question: Why do we call them “operators” instead of just “functions”?",
    "text": "The term “operator” emphasizes that these mappings act on objects (random variables) that are themselves functions. This distinguishes them from ordinary functions that act on numbers. The expectation operator, for instance, takes an entire probability distribution and distills it down to a single number representing its center."
  },
  {
    "objectID": "operators-properties#the-expectation-operator",
    "href": "/chapter/operators-properties#the-expectation-operator",
    "title": "Expectation and Variance Operators",
    "section": "The Expectation Operator",
    "text": "Intuition and Definition Intuitively, a random variable’s expected value represents the average we would see if we observed many independent realizations of that variable. For example, if we roll a fair six-sided die thousands of times and compute the average of all the outcomes, that average will converge to 3.5. This value—3.5—is the expected value of the die roll."
  },
  {
    "objectID": "operators-properties#section-5",
    "href": "/chapter/operators-properties",
    "title": "Expectation and Variance Operators",
    "section": "Definition: Expected Value",
    "text": "The expected value (or expectation ) of a discrete random variable X is the probability-weighted average of all its possible values: \\mathbb{E}[X] = \\sum_{i=1}^n x_i p_i where x_i are the possible values and p_i = \\mathrm{P}(X = x_i) are their respective probabilities. More generally, we can write this as: \\mathbb{E}[X] = \\sum_{i=1}^n p_i X_i = \\mu where we often use the Greek letter \\mu (mu) to denote the expected value. For continuous random variables, the sum becomes an integral: \\mathbb{E}[X] = \\int_{\\mathbb{R}} x f(x) \\, dx where f(x) is the probability density function of X ."
  },
  {
    "objectID": "operators-properties#section-6",
    "href": "/chapter/operators-properties",
    "title": "Expectation and Variance Operators",
    "section": "Question: Can you give a concrete example of computing an expected value?",
    "text": "Consider a simple game where you flip a fair coin. If it lands heads, you win $10; if it lands tails, you lose $5. What are your expected winnings? Let X represent your winnings. Then: $$ [X] = 10"
  },
  {
    "objectID": "panel-data",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "",
    "text": "Panel Data Methods css: styles.css toc: true toc-depth: 3 number-sections: true Panel Data Methods Panel data—repeated observations on the same individuals over time—offers researchers a powerful tool for addressing one of the most vexing problems in observational research: unobserved heterogeneity. In this chapter, we’ll explore how the longitudinal structure of panel data allows us to control for time-invariant individual characteristics that would otherwise bias our estimates. Our running example throughout this chapter will draw from the National Longitudinal Survey of Youth 1979 (NLSY79), which has followed a cohort of young Americans since 1979. We’ll focus on a fundamental question in labor economics: What is the return to education? That is, how much more do workers earn for each additional year of schooling they complete? By the end of this chapter, you will understand: Why panel data helps address omitted variable bias Fixed effects estimation and the within transformation Random effects estimation and when it’s appropriate First-differencing as an alternative to fixed effects How to implement these methods in R The key assumptions underlying each approach Why can’t we just estimate the return to education by regressing wages on years of schooling using cross-sectional data? If we simply regress wages on education using a single cross-section of workers, we face a severe omitted variable bias problem. Workers with more education may differ from workers with less education in many unobserved ways that also affect earnings—ability, motivation, family background, social networks, and so on. If these unobserved characteristics are positively correlated with both education and wages, a simple OLS regression will overstate the causal effect of education on earnings."
  },
  {
    "objectID": "panel-data#the-nlsy79-data",
    "href": "/chapter/panel-data#the-nlsy79-data",
    "title": "Panel Data Methods",
    "section": "The NLSY79 Data",
    "text": "The National Longitudinal Survey of Youth 1979 began with 12,686 respondents aged 14-22 in 1979. These individuals have been surveyed repeatedly (annually through 1994, biennially since then), providing detailed information about their education, employment, earnings, family background, and test scores. For our analysis, we’ll focus on a subset of the data: male respondents observed during their prime working years (ages 25-35). This gives us multiple observations per person, typically spanning 5-10 years. Here’s what our data structure looks like: ```{r} #| echo: true #| eval: false Load required packages library(tidyverse) library(plm) # For panel data methods library(lfe) # For high-dimensional fixed effects library(stargazer) # For nice regression tables Load NLSY data (hypothetical structure) nlsy <- read_csv(“nlsy_panel.csv”) Look at the structure head(nlsy) id year age educ logwage experience union married region 1 1986 28 12 2.45 6 0 1 NE 1 1987 29 12 2.52 7 0 1 NE 1 1988 30 12 2.58 8 1 1 NE 2 1986 27 16 2.88 3 0 0 S 2 1987 28 16 2.95 4 0 1 S 2 1988 29 16 3.02 5 0 1 S ::: {.question} What features of this data structure make it \"panel data\"? ::: ::: {.answer} Panel data has two key features visible here: 1. **Multiple individuals**: Each person has a unique identifier (`id`) 2. **Multiple time periods**: Each person appears in multiple years This creates a two-dimensional structure: we observe variation both *across* individuals and *within* individuals over time. It's this within-person variation that we'll exploit to control for unobserved individual characteristics. ::: ## The Omitted Variable Bias Problem Let's start by understanding exactly what problem panel data helps us solve. Suppose we're interested in estimating the causal effect of education on log wages. We might write down a simple model: $$ \\log(wage_{it}) = \beta_0 + \beta_1 educ_i + u_{it} $$ where $i$ indexes individuals and $t$ indexes time periods. The parameter $\beta_1$ represents the retu"
  },
  {
    "objectID": "panel-data#section-2",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "The Key Insight",
    "text": "The cross-sectional estimate conflates two distinct effects: The causal effect of education on wages The correlation between education and unobserved ability Panel data methods allow us to separate these two effects by exploiting the longitudinal structure of the data."
  },
  {
    "objectID": "panel-data#fixed-effects-the-within-transformation",
    "href": "/chapter/panel-data#fixed-effects-the-within-transformation",
    "title": "Panel Data Methods",
    "section": "Fixed Effects: The Within Transformation",
    "text": "The fundamental insight of fixed effects estimation is surprisingly simple: if unobserved ability doesn’t change over time, we can eliminate it by looking at changes within individuals. The Fixed Effects Model We start with a more explicit model that separates time-invariant from time-varying factors: \\log(wage_{it}) = \beta_0 + \beta_1 educ_{it} + \beta_2 experience_{it} + \beta_3 experience_{it}^2 + \u0007lpha_i + arepsilon_{it} The key addition is \u0007lpha_i —an individual-specific intercept that captures all time-invariant characteristics of person i . This includes: Innate ability Family background Personality traits Network effects from childhood Anything else about person i that doesn’t change over our observation period If \u0007lpha_i is unobserved and correlated with education, why doesn’t this cause omitted variable bias just like before? The crucial difference is that \u0007lpha_i doesn’t vary over time. This allows us to eliminate it through a clever transformation. If we take the time average of our equation for each individual: \\overline{\\log(wage_i)} = \beta_0 + \beta_1 \\overline{educ_i} + \beta_2 \\overline{experience_i} + \beta_3 \\overline{experience_i^2} + \u0007lpha_i + \\overline{ arepsilon_i} and subtract this from the original equation, \u0007lpha_i disappears completely. This is called the within transformation or time-demeaning . The Within Transformation Let’s see this transformation explicitly. For each individual i , we compute the time averages: \begin{aligned} \\overline{\\log(wage_i)} &= rac{1}{T_i} \\sum_{t=1}^{T_i} \\log(wage_{it}) \\ \\overline{educ_i} &= rac{1}{T_i} \\sum_{t=1}^{T_i} educ_{it} \u001bnd{aligned} where T_i is the number of time periods we observe individual i . Now subtract these averages from the original equation: \\log(wage_{it}) - \\overline{\\log(wage_i)} = \beta_1(educ_{it} - \\overline{educ_i}) + \beta_2(experience_{it} - \\overline{experience_i}) + ... + ( arepsilon_{it} - \\overline{ arepsilon_i}) Notice what’s missing: \u0007lpha_i has completely disappeared! We can wri"
  },
  {
    "objectID": "panel-data#practical-considerations-and-robustness",
    "href": "/chapter/panel-data#practical-considerations-and-robustness",
    "title": "Panel Data Methods",
    "section": "Practical Considerations and Robustness",
    "text": "Clustered Standard Errors A critical issue in panel data analysis is that observations for the same individual are unlikely to be independent. Wage shocks might persist over time, leading to serial correlation in $ arepsilon_{it}$. This violates the standard OLS assumption and causes our standard errors to understate uncertainty. The solution is to compute cluster-robust standard errors , clustering at the individual level: ```{r} #| echo: true #| eval: false Fixed effects with clustered standard errors library(lmtest) library(sandwich) Compute robust covariance matrix fe_vcov_cluster <- vcovHC(fe_model, type = “HC1”, cluster = “group”) Get corrected standard errors and test statistics coeftest(fe_model, vcov = fe_vcov_cluster) Coefficients: Estimate Std. Error t value Pr(>|t|) educ 0.0523 0.0189 2.767 0.00566 ** experience 0.0812 0.0132 6.152 < 2e-16 * I(experience^2) -0.0024 0.0009 -2.667 0.00766 union 0.0654 0.0221 2.959 0.00309 ** married 0.0432 0.0198 2.182 0.02912 * Notice how the clustered standard errors are larger than the default standard errors, reflecting the within-person correlation in wage shocks. This is typical in panel data applications. ::: {.callout-important} ## Always Cluster Your Standard Errors In panel data applications, you should almost always compute cluster-robust standard errors, clustering at the individual (or higher) level. Failing to do so will lead to overstated precision and too-frequent rejection of null hypotheses. This is one of the most common errors in applied panel data analysis. ::: ### Time Fixed Effects Our model so far has assumed that there are no aggregate time effects—that is, nothing systematic happens to all workers' wages in particular years. This is unrealistic. Recessions, inflation, technological change, and policy reforms affect everyone. We can add **time fixed effects** (year dummies) to control for these aggregate shocks: $$ \\log(wage_{it}) = \beta_1 educ_{it} + \beta_2 experience_{it} + \beta_3 experience_{it}"
  },
  {
    "objectID": "panel-data#extensions-and-further-reading",
    "href": "/chapter/panel-data#extensions-and-further-reading",
    "title": "Panel Data Methods",
    "section": "Extensions and Further Reading",
    "text": "Dynamic Panel Data Our models have assumed that past wages don’t directly affect current wages (except through persistent individual effects and serially correlated shocks). But what if there’s true state dependence —where having high wages in the past directly causes high wages today? We could add a lagged dependent variable: \\log(wage_{it}) = ho \\log(wage_{i,t-1}) + \beta_1 educ_{it} + ... + \u0007lpha_i + arepsilon_{it} This creates serious econometric challenges. The within transformation produces bias because \\ddot{\\log(wage_{i,t-1})} is correlated with \\ddot{ arepsilon_{it}} by construction. Special methods like the Arellano-Bond GMM estimator are needed. Unbalanced Panels Our discussion assumed a balanced panel—the same individuals observed in all periods. Real panel datasets are typically unbalanced, with individuals entering and exiting the sample. The good news is that fixed effects and first-differencing naturally handle unbalanced panels, using all available observations. But attrition could cause selection bias if individuals’ exit depends on their wage trajectories. More on Identification We’ve focused on the mechanical aspects of panel data estimation, but the deeper questions are about identification: What variation in the data identifies our parameters? Is this the “right” variation for answering our causal question? What assumptions are required for a causal interpretation? For the NLSY education returns, we’re identifying \beta_1 from individuals whose education changes while working. This raises questions: Are these returns generalizable to traditional students? Might education changes while working be endogenous to wage trajectories? Could there be time-varying confounders we’re not controlling for? These questions don’t have purely statistical answers. They require economic reasoning about the context and careful consideration of what variation we’re exploiting."
  },
  {
    "objectID": "panel-data#summary",
    "href": "/chapter/panel-data#summary",
    "title": "Panel Data Methods",
    "section": "Summary",
    "text": "Panel data methods offer powerful tools for addressing omitted variable bias by exploiting repeated observations on the same individuals. Here are the key takeaways:"
  },
  {
    "objectID": "panel-data#section-7",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "Key Points",
    "text": "Fixed effects eliminates time-invariant unobserved heterogeneity by using within-person variation. It requires no assumptions about the relationship between \u0007lpha_i and regressors, but sacrifices the ability to estimate effects of time-invariant variables. Random effects uses both within- and between-person variation, gaining efficiency and allowing estimation of time-invariant effects. But it requires the strong assumption that \u0007lpha_i is uncorrelated with all regressors. First-differencing is an alternative to fixed effects that may be preferred when errors follow a random walk or when there are only two time periods. The Hausman test helps choose between fixed and random effects by testing whether they produce systematically different estimates. Always use cluster-robust standard errors in panel data applications to account for within-person correlation in errors. Time fixed effects should generally be included to control for aggregate time trends and shocks. The variation that identifies panel data estimates may be local —applying to specific subpopulations (like those whose treatment status changes). Extrapolation requires caution. Applied Lessons from the NLSY Our analysis of returns to education in the NLSY79 illustrates several important points: Cross-sectional estimates (10.8%) substantially overstate returns due to omitted ability bias Fixed effects estimates (5.2%) are roughly half the cross-sectional estimates, suggesting ability bias is large and positive These estimates apply specifically to workers who complete additional schooling while employed—a selected group The Hausman test strongly rejects the random effects assumption, confirming that ability is correlated with education The broader lesson: the source of identifying variation matters . Panel data methods don’t eliminate all endogeneity concerns—they only address time-invariant unobserved heterogeneity. Time-varying confounders, reverse causality, and measurement error remain potential threats "
  },
  {
    "objectID": "parameters-statistics",
    "href": "/chapter/parameters-statistics",
    "title": "Parameters and statistics",
    "section": "",
    "text": "Parameters and statistics This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "probability-distributions",
    "href": "/chapter/probability-distributions",
    "title": "Probability",
    "section": "",
    "text": "Probability In this chapter, we’ll introduce some fundamental concepts in probability theory. By the end of this chapter, you will be able to define the following: Sample space Outcome Event Probability Random variable"
  },
  {
    "objectID": "probability-distributions#what-is-probability",
    "href": "/chapter/probability-distributions#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability? Probability is a mathematical framework for quantifying uncertainty. It assigns numerical values between 0 and 1 to events, where 0 indicates impossibility and 1 indicates certainty. The sample space is the set of all possible outcomes of an experiment. For a coin flip, the sample space is \\{H, T\\} . Let’s consider a simple example. Imagine you’re flipping a fair coin. The sample space consists of two possible outcomes: heads (H) and tails (T). The probability of getting heads is: P(H) = rac{ ext{Number of favorable outcomes}}{ ext{Total number of possible outcomes}} = rac{1}{2} = 0.5"
  },
  {
    "objectID": "probability-distributions#random-variables",
    "href": "/chapter/probability-distributions#random-variables",
    "title": "Probability",
    "section": "Random Variables",
    "text": "A random variable is a function that assigns numerical values to the outcomes of a random experiment. Can you give an example of a random variable? Consider rolling a six-sided die. Let X be the random variable representing the number that appears on the top face. Then X can take values \\{1, 2, 3, 4, 5, 6\\} , each with probability 1/6 if the die is fair."
  },
  {
    "objectID": "probability-distributions#relationship-between-pdfs-and-cdfs",
    "href": "/chapter/probability-distributions#relationship-between-pdfs-and-cdfs",
    "title": "Probability",
    "section": "Relationship between PDFs and CDFs",
    "text": "The probability density function (PDF) and cumulative distribution function (CDF) are two fundamental ways of describing a probability distribution. The interactive visualization below demonstrates how these functions relate to each other. Use the dropdown menu to explore different distributions (Normal, Lognormal, and Uniform), and drag the slider to see how the PDF height at a point relates to the CDF value at that same point. As you move the slider, observe that: - The PDF ( f(Y) ) shows the height of the density at any given value - The CDF ( F(Y) ) shows the area under the PDF curve to the left of that value - The shaded region in the PDF panel corresponds exactly to the CDF value"
  },
  {
    "objectID": "probability-distributions#embedding-videos",
    "href": "/chapter/probability-distributions#embedding-videos",
    "title": "Probability",
    "section": "Embedding Videos",
    "text": "Here’s how to embed your Manim animations. Replace your_video.mp4 with the actual filename. Your browser does not support the video tag. Random Variables in Action: This animation demonstrates how a random variable maps outcomes from a sample space to numerical values."
  },
  {
    "objectID": "propensity-score",
    "href": "/chapter/propensity-score",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "Propensity Score Matching Imagine you’re tasked with evaluating whether a job training program actually helps people earn more money. You collect data on hundreds of workers—some who participated in the program and some who didn’t. You compare their earnings and find that, on average, those who went through the training actually earn less than those who didn’t. Should you conclude the program is harmful? Not so fast. The problem is that people don’t randomly stumble into job training programs. Those who seek out such programs often start from a position of disadvantage—they might have less education, weaker employment histories, or face other barriers to employment. In other words, the two groups aren’t comparable to begin with. This is the fundamental challenge of causal inference from observational data: when treatment isn’t randomly assigned, how can we estimate what would have happened to the treated individuals if they hadn’t received treatment? In this chapter, we’ll explore one elegant solution to this problem: propensity score matching . What makes propensity score matching different from simply comparing averages between treated and untreated groups? Propensity score matching explicitly accounts for the fact that treated and untreated individuals may differ systematically in their observable characteristics. Rather than comparing all treated individuals to all untreated individuals, it finds pairs (or small groups) of individuals who look similar in terms of their background characteristics but differ in whether they received treatment. This creates a more “apples-to-apples” comparison."
  },
  {
    "objectID": "propensity-score#the-national-supported-work-demonstration",
    "href": "/chapter/propensity-score#the-national-supported-work-demonstration",
    "title": "Propensity Score Matching",
    "section": "The National Supported Work Demonstration",
    "text": "To make these ideas concrete, we’ll work with data from the National Supported Work (NSW) Demonstration, a job training program implemented in the 1970s. The program provided work experience to disadvantaged workers—individuals with histories of drug use, criminal records, or long-term unemployment—in an effort to help them transition to regular employment. What makes this dataset particularly valuable for learning about causal inference is that the NSW program actually was randomized for a subset of participants. This means we know the “ground truth”—the actual causal effect of the program. We can then see how well observational methods like propensity score matching can recover this effect when we pretend we don’t have the benefit of randomization. Let’s start by looking at the data. We have information on 445 individuals: 185 who participated in the NSW program (the treated group) and 260 who did not (the control group). For each person, we observe: Outcome : Real earnings in 1978 (after the program) Pre-treatment characteristics : Age Years of education Race and ethnicity (Black, Hispanic) Marital status High school degree indicator Real earnings in 1974 (before the program) Real earnings in 1975 (before the program) Employment status in 1974 (whether earnings were zero) Employment status in 1975 (whether earnings were zero) Here’s a glimpse of what the data looks like: ```{python} #| echo: false import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyBboxPatch import seaborn as sns Set random seed for reproducibility np.random.seed(42) Load or create the LaLonde NSW dataset For demonstration, I’ll create a simplified version In practice, you would load the actual dataset Create NSW experimental data n_treated = 185 n_control = 260 Treatment group (disadvantaged background) treated_data = { ‘treat’: np.ones(n_treated), ‘age’: np.random.normal(25, 7, n_treated), ‘educ’: np.random.normal(10, 2, n_treated), ‘black’"
  },
  {
    "objectID": "propensity-score#assessing-balance-after-matching",
    "href": "/chapter/propensity-score#assessing-balance-after-matching",
    "title": "Propensity Score Matching",
    "section": "Assessing Balance After Matching",
    "text": "The key test of whether matching worked is whether it achieved balance—that is, whether the matched treated and control groups now look similar in terms of their pre-treatment characteristics. Let’s check: ```{python} #| echo: false # Create matched sample matched_treated_idx = matches[‘treated_idx’].values matched_control_idx = matches[‘control_idx’].values matched_treated = df_obs.loc[matched_treated_idx] matched_control = df_obs.loc[matched_control_idx] Balance table for matched sample print(” Balance After Matching:“) balance_data_matched = [] for var in covariates: treated_val = matched_treated[var].mean() control_val = matched_control[var].mean() diff = treated_val - control_val # Also compute standardized difference pooled_sd = np.sqrt((matched_treated[var].std()**2 + matched_control[var].std()**2) / 2) std_diff = diff / pooled_sd if pooled_sd > 0 else 0 balance_data_matched.append({ 'Variable': var, 'Treated': f'{treated_val:.2f}', 'Control': f'{control_val:.2f}', 'Difference': f'{diff:.2f}', 'Std. Diff.': f'{std_diff:.3f}' }) balance_df_matched = pd.DataFrame(balance_data_matched) print(balance_df_matched.to_string(index=False)) Much better! The standardized differences are now much smaller. A common rule of thumb is that standardized differences should be less than 0.1 (or sometimes 0.25) for adequate balance. While not perfect, matching has substantially reduced the imbalance between treated and control groups. ::: {.question} What is a standardized difference, and why do we use it instead of just looking at raw differences? ::: ::: {.answer} A standardized difference expresses the difference between groups in units of standard deviations. It's calculated as the difference in means divided by the pooled standard deviation. We use it because it's scale-invariant—a difference of 2 years in age means something very different from a difference of $2,000 in earnings. By standardizing, we can assess balance consistently across variables measured in different un"
  },
  {
    "objectID": "propensity-score#estimating-the-treatment-effect",
    "href": "/chapter/propensity-score#estimating-the-treatment-effect",
    "title": "Propensity Score Matching",
    "section": "Estimating the Treatment Effect",
    "text": "Now that we have a matched sample with good balance, we can estimate the treatment effect. The simplest approach is to compare average outcomes between the matched treated and control groups: ```{python} #| echo: false # Estimate treatment effect on matched sample matched_treated_outcome = matched_treated[‘re78’].mean() matched_control_outcome = matched_control[‘re78’].mean() matched_effect = matched_treated_outcome - matched_control_outcome print(f” Treatment Effect Estimates:“) print(f”{‘Method’:<30} {‘Estimate’:>12}“) print(f”{‘-’*42}“) print(f”{‘Experimental benchmark’:<30} ${naive_effect:>11,.2f}“) print(f”{‘Naive (PSID controls)’:<30} ${naive_effect_obs:>11,.2f}“) print(f”{‘Propensity score matching’:<30} ${matched_effect:>11,.2f}“) ``` The propensity score matching estimate is much closer to the experimental benchmark than the naive comparison! This demonstrates the power of matching: by creating comparable groups, we can recover estimates that approximate what we would have found in a randomized experiment. Why isn’t the propensity score matching estimate exactly equal to the experimental benchmark? There are several reasons: (1) Matching only balances observed characteristics—if there are important unobserved differences between NSW participants and PSID controls, matching won’t eliminate that bias. (2) Even with the same data, different matching methods (nearest neighbor vs. kernel, different calipers, etc.) can produce slightly different estimates. (3) The experimental benchmark itself has sampling variability. The key point is that matching gets us much closer to the truth than naive comparisons."
  },
  {
    "objectID": "propensity-score#the-fundamental-assumption-unconfoundedness",
    "href": "/chapter/propensity-score#the-fundamental-assumption-unconfoundedness",
    "title": "Propensity Score Matching",
    "section": "The Fundamental Assumption: Unconfoundedness",
    "text": "All of this analysis rests on a critical assumption called unconfoundedness or selection on observables . This assumption states that, conditional on the observed covariates X , treatment assignment is independent of potential outcomes: (Y_1, Y_0) \\perp ext{Treat} \\mid X In plain English: once we account for all the observed characteristics, there are no remaining systematic differences between treated and control groups that affect outcomes. This is a strong assumption, and it’s fundamentally untestable. We can check whether we’ve achieved balance on observed characteristics, but we can never know whether there are unobserved confounders lurking in the background. When is the unconfoundedness assumption most plausible? The assumption is most credible when: (1) We have rich data on pre-treatment characteristics that are likely to affect both treatment assignment and outcomes. (2) We understand the treatment assignment process well enough to know what variables matter. (3) The treatment decision is based primarily on factors we can observe. In the NSW example, if individuals selected into the program based solely on observable characteristics like employment history and demographics, unconfoundedness is plausible. If they also selected based on unobservable factors like motivation or family support, we may still have bias."
  },
  {
    "objectID": "propensity-score#sensitivity-analysis-how-robust-are-our-results",
    "href": "/chapter/propensity-score#sensitivity-analysis-how-robust-are-our-results",
    "title": "Propensity Score Matching",
    "section": "Sensitivity Analysis: How Robust Are Our Results?",
    "text": "Given that we can never be certain about unconfoundedness, it’s important to conduct sensitivity analyses. These ask: how strong would unobserved confounding need to be to change our conclusions? One approach, developed by Rosenbaum (2002), examines how much the odds of treatment would need to differ between matched individuals to overturn our findings. If only a small amount of confounding could change our conclusions, we should be cautious. If it would take substantial confounding, we can be more confident. Another approach is to examine whether our results are stable when we: - Use different matching methods - Change the caliper width - Include or exclude specific covariates - Trim observations with extreme propensity scores Robust findings that hold across multiple specifications are more credible than fragile results that change dramatically with small methodological choices."
  },
  {
    "objectID": "propensity-score#when-to-use-propensity-score-matching",
    "href": "/chapter/propensity-score#when-to-use-propensity-score-matching",
    "title": "Propensity Score Matching",
    "section": "When to Use Propensity Score Matching",
    "text": "Propensity score matching is a powerful tool, but it’s not always the best choice. Here’s when it works well: Use PSM when: - You have rich pre-treatment covariate data - You believe selection is primarily on observables - You need to assess and demonstrate balance - You have reasonable overlap in covariate distributions - You want an intuitive, transparent analysis Consider alternatives when: - You have limited covariate data (unconfoundedness less plausible) - Overlap is very poor (few good matches available) - You have panel data with pre-treatment outcomes (difference-in-differences may be better) - You have an instrumental variable (IV estimation may be better) - You need to model the outcome function carefully (regression adjustment may be better)"
  },
  {
    "objectID": "propensity-score#extensions-and-variations",
    "href": "/chapter/propensity-score#extensions-and-variations",
    "title": "Propensity Score Matching",
    "section": "Extensions and Variations",
    "text": "The basic propensity score matching framework we’ve covered can be extended in several ways: Matching with replacement : Each control can be matched to multiple treated units, which improves balance but reduces efficiency. Matching with multiple controls : Each treated unit is matched to k controls (e.g., k=3 ) and the treatment effect is the difference between the treated unit’s outcome and the average outcome of its matches. Kernel matching and local linear matching : Instead of discrete matches, use weighted averages of all controls, with weights depending on propensity score distance. Doubly robust estimation : Combine propensity score matching with regression adjustment. This approach is “doubly robust” in that it yields consistent estimates if either the propensity score model or the outcome regression model is correctly specified (though not necessarily both). Covariate balancing propensity score (CBPS) : Instead of just maximizing likelihood, estimate propensity scores to directly optimize covariate balance. Each of these extensions involves tradeoffs between bias and variance, and the choice depends on the specific application."
  },
  {
    "objectID": "propensity-score#practical-guidelines",
    "href": "/chapter/propensity-score#practical-guidelines",
    "title": "Propensity Score Matching",
    "section": "Practical Guidelines",
    "text": "Based on the LaLonde example and broader research, here are some practical guidelines for implementing propensity score matching: Start with descriptive analysis : Examine covariate distributions before matching to understand the selection process and assess overlap. Choose covariates carefully : Include variables that affect both treatment assignment and outcomes. Avoid including post-treatment variables or instruments. Check for common support : Trim observations with extreme propensity scores or use calipers to enforce overlap. Assess balance explicitly : Use standardized differences and visual diagnostics like love plots. Be transparent about choices : Report results under multiple specifications to demonstrate robustness. Acknowledge limitations : Discuss the unconfoundedness assumption and conduct sensitivity analyses. Compare to other methods : If possible, compare PSM estimates to results from other causal inference methods as a robustness check."
  },
  {
    "objectID": "propensity-score#conclusion",
    "href": "/chapter/propensity-score#conclusion",
    "title": "Propensity Score Matching",
    "section": "Conclusion",
    "text": "Propensity score matching provides an elegant solution to the challenge of causal inference from observational data. By summarizing many covariates into a single score and using it to create balanced comparison groups, we can approximate the conditions of a randomized experiment—at least with respect to observed characteristics. The LaLonde dataset beautifully illustrates both the power and the limitations of this approach. When we have good overlap and rich covariate data, matching can recover estimates close to experimental benchmarks. But matching is only as good as the data we have: it cannot control for unobserved confounders, and it requires sufficient overlap to find good comparisons. As you apply these methods to your own data, remember that propensity score matching is a tool, not a magic wand. It requires careful implementation, thorough diagnostics, and honest acknowledgment of assumptions. Used thoughtfully, it can help us learn about causal effects from observational data. Used carelessly, it can create a false sense of confidence in potentially biased estimates. The next chapter will explore related methods for causal inference from observational data, including inverse probability weighting, difference-in-differences, and regression discontinuity designs. Each has its own strengths and weaknesses, and understanding the full toolkit allows us to choose the right tool for each problem."
  },
  {
    "objectID": "propensity-score#further-reading",
    "href": "/chapter/propensity-score#further-reading",
    "title": "Propensity Score Matching",
    "section": "Further Reading",
    "text": "Original propensity score paper : Rosenbaum, P. R., & Rubin, D. B. (1983). “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika , 70(1), 41-55. LaLonde’s evaluation : LaLonde, R. J. (1986). “Evaluating the Econometric Evaluations of Training Programs with Experimental Data.” American Economic Review , 76(4), 604-620. Practical guide : Caliendo, M., & Kopeinig, S. (2008). “Some Practical Guidance for the Implementation of Propensity Score Matching.” Journal of Economic Surveys , 22(1), 31-72. Modern causal inference : Imbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction . Cambridge University Press."
  },
  {
    "objectID": "summary-statistics",
    "href": "/chapter/summary-statistics",
    "title": "Summary statistics",
    "section": "",
    "text": "Summary statistics This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-mean-large",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "",
    "text": "Testing a claim about a population mean In this chapter, we’ll develop a comprehensive understanding of hypothesis testing through a detailed worked example. We’ll build the theoretical foundation step by step, introducing key concepts like standard error, test statistics, and p-values along the way. By the end of this chapter, you will understand how to conduct hypothesis tests for population means, interpret their results, and recognize the crucial differences between large and small sample tests."
  },
  {
    "objectID": "testing-mean-large#the-problem-evaluating-a-new-curriculum",
    "href": "/chapter/testing-mean-large#the-problem-evaluating-a-new-curriculum",
    "title": "Testing a claim about a population mean",
    "section": "The Problem: Evaluating a New Curriculum",
    "text": "Let’s begin with a concrete problem that will guide our exploration of hypothesis testing. Suppose we’re trying to improve the logical ability of students through a new curriculum. The old curriculum, which has been in use for many years, produces an average test score of 80 points. We’ve developed a new curriculum and trained a large group of students using this approach. The central question we want to answer is: Is the new curriculum more effective at raising average test scores? To investigate this question, we randomly sample 38 students from those trained under the new curriculum and record their test scores. Our sample yields a mean score of 83 points. Our data: Test scores from 38 randomly selected students"
  },
  {
    "objectID": "testing-mean-large#section-2",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🤔 Initial Observation",
    "text": "While our sample mean of 83 is higher than the old curriculum mean of 80, we cannot immediately conclude that the population mean of all students trained under the new curriculum exceeds 80. Why not? Because our sample is just one of many possible samples we could have drawn, and sample means vary due to random sampling. The Logic of Hypothesis Testing Frequentist hypothesis testing operates on a principle analogous to proof by contradiction in mathematics. We temporarily assume the opposite of what we hope to demonstrate, then show that this assumption leads to implausible results. The nature of hypothesis testing: the probabilistic equivalent of proof by contradiction The intuition is straightforward: we set up a hypothesis about a population parameter, assume it’s correct, then calculate the conditional probability of observing our sample data. If this probability is sufficiently small, we reject the hypothesis. Intuition underlying frequentist hypothesis testing"
  },
  {
    "objectID": "testing-mean-large#understanding-standard-error",
    "href": "/chapter/testing-mean-large#understanding-standard-error",
    "title": "Testing a claim about a population mean",
    "section": "Understanding Standard Error",
    "text": "Before we can conduct a proper hypothesis test, we need to understand a crucial concept: standard error ."
  },
  {
    "objectID": "testing-mean-large#section-4",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 What is Standard Error?",
    "text": "Standard error measures how far, on average , a sample mean deviates from the population mean across repeated samples. It quantifies the precision of our estimator. Mathematically, the standard error of the sample mean is: SE = rac{\\sigma}{\\sqrt{n}} where \\sigma is the population standard deviation and n is the sample size. The Concept of Repeated Sampling To truly understand standard error, we need to embrace a core principle of frequentist statistics: repeated sampling . Imagine we could draw not just one sample of 38 students, but millions of such samples from our population. Each sample would give us a different sample mean. Here’s the remarkable thing: if we computed all these sample means and calculated their average, that average would equal the true population mean. This property is called unbiasedness , and it’s why we use the sample mean as our estimator. But these individual sample means would vary around the population mean. The standard error tells us the typical size of this variation. In our example, with a sample size of 38 and a calculated standard error of 1.64 points, we know that a typical sample mean deviates from the population mean by about 1.64 points."
  },
  {
    "objectID": "testing-mean-large#section-5",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Key Insight",
    "text": "Smaller standard error = More precision = Greater reliability When the standard error is small, we can be more confident that our single observed sample mean is close to the true population mean. The standard error decreases as sample size increases, which is why larger samples give us more reliable estimates. The Relationship: Population Variance to Sample Mean Variance There’s a fundamental relationship connecting the population variance to the variance of the sample mean: ext{Var}(\bar{Y}) = rac{\\sigma^2}{n} Taking the square root of both sides gives us the standard error formula. This relationship tells us that: The variance of sample means is smaller than the population variance This variance decreases as sample size increases The relationship is inverse with sample size (doubling n doesn’t double precision)"
  },
  {
    "objectID": "testing-mean-large#the-three-stages-of-hypothesis-testing",
    "href": "/chapter/testing-mean-large#the-three-stages-of-hypothesis-testing",
    "title": "Testing a claim about a population mean",
    "section": "The Three Stages of Hypothesis Testing",
    "text": "Now that we understand standard error, we can proceed with our hypothesis test. We’ll work through this systematically in three stages. We will follow a three-stage process to test hypotheses Stage 1: Formulating the Hypotheses Stage I: Setup The first step in any hypothesis test is to clearly state what we’re testing. We need two competing hypotheses. Expressing Our Claim in English Elucidate claim and its complement in English Claim: Students trained under the new curriculum will score, on average, higher than 80 on the test. Complement: Students trained under the new curriculum will not score, on average, higher than 80 on the test. The Law of the Excluded Middle In formulating our hypotheses, we’re invoking a fundamental principle of logic: the law of the excluded middle . The law of the excluded middle This law states that a statement is either true or false—there is no middle ground between truth and falsity. Either the new curriculum improves scores beyond 80, or it doesn’t. Our job is to determine which is more likely given our data. Symbolic Representation Express claim and its complement symbolically Let the mean score of students trained under the new curriculum be \\mu . Claim: \\mu > 80 Complement: \\mu \\leq 80 Assigning Null and Alternative Hypotheses Specify null and alternative hypotheses Null Hypothesis ( H_0 ): The population mean test score under the new curriculum is less than or equal to 80. H_0: \\mu \\leq 80 Alternative Hypothesis ( H_A ): The population mean test score under the new curriculum exceeds 80. H_A: \\mu > 80 The null hypothesis represents the status quo or the claim we’re trying to find evidence against. The alternative hypothesis represents what we hope to demonstrate with our data. By convention, we assign the complement of our claim to the null hypothesis—this is what we will attempt to falsify. We also need to choose a significance level \u0007lpha , which represents our tolerance for making a Type I error (rejecting a true null hypothes"
  },
  {
    "objectID": "testing-mean-large#section-7",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "📊 Why This Setup?",
    "text": "Notice that our null hypothesis includes the equality. This is a one-sided test because we’re only interested in whether the new curriculum is better , not just different. If we cared about any difference (better or worse), we’d use a two-sided test. Understanding Type I and Type II Errors Before proceeding, we must acknowledge that hypothesis testing involves the possibility of error. There are two types of errors we might make: Anticipating the possibility of erring Type I Error: Rejecting a true null hypothesis (false positive) Type II Error: Failing to reject a false null hypothesis (false negative) It’s crucial to understand that we cannot make both errors simultaneously: We cannot make both errors simultaneously Each error is associated with a unique decision If we reject the null, we can make only a Type I error If we don’t reject the null, we can make only a Type II error Choosing the Significance Level We also need to choose a significance level \u0007lpha , which represents our tolerance for making a Type I error (rejecting a true null hypothesis). The level of significance is the largest probability of making a Type I error that a researcher is willing to tolerate In many academic papers, the level of significance is set at either 5% or 1%. But where do these specific values come from? Why are these specific values used commonly? The answer involves both history and convention. The story begins with an afternoon tea party and R.A. Fischer. The Lady Tasting Tea Fischer’s work on experimental design, inspired by a colleague who claimed she could tell whether milk was added before or after tea, led to the development of significance testing as we know it today. Stage 2: Estimating the Sampling Distribution In this stage, we need to characterize the distribution of our test statistic under the assumption that the null hypothesis is true. Step 1: Choose an Estimator We use the sample mean \bar{Y} as our estimator of the population mean \\mu . Our observed value is \ba"
  },
  {
    "objectID": "testing-mean-large#section-8",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 The Sampling Distribution",
    "text": "We now have a complete picture of the sampling distribution under H_0 : \bar{Y} \\sim N(80, 1.64^2) This means if the null hypothesis is true, sample means from repeated samples would be normally distributed around 80 with a standard deviation of 1.64. Visualizing the Distribution Imagine a bell curve centered at 80. This represents all possible sample means we could observe if the true population mean were 80. Some sample means would be less than 80, some greater, but they’d cluster around 80 with most values falling within a few standard errors of the center. Our observed sample mean of 83 lies to the right of this center. The question is: is it far enough to the right that we should doubt the null hypothesis? Stage 3: Computing the Test Statistic and P-value To answer our question, we need to standardize our observed value and determine how unusual it is. The Test Statistic: Zeta (ζ) We define a test statistic called zeta (ζ) as: \\zeta = rac{\bar{Y} - \\mu_0}{SE} where \\mu_0 is the hypothesized population mean under the null (80 in our case). This standardization accomplishes two things: 1. It converts our result to a unit-free measure 2. It tells us how many standard errors our observed mean is from the hypothesized mean"
  },
  {
    "objectID": "testing-mean-large#section-9",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Interpretation of ζ",
    "text": "The value of ζ represents the number of standard deviations (or standard errors) that the observed sample mean is from the hypothesized population mean. If our observed sample had a mean of 83 kg, the population mean were 80 kg, and the standard error were 1.64 kg, then: \\zeta = rac{83 - 80}{1.64} = 1.83 The “kg” units cancel out, leaving us with a pure number: 1.83 standard errors above the hypothesized mean. Calculating Our Test Statistic For our problem: \\zeta = rac{83 - 80}{1.64} = rac{3}{1.64} \u0007pprox 1.83 Our observed sample mean is 1.83 standard errors above the hypothesized mean of 80. The Distribution of Zeta When the sample size is large and we know (or can estimate) the population standard deviation, the test statistic ζ follows a standard normal distribution (also called a Z-distribution). This is the same as the Z-scores you may have encountered before. \\zeta \\sim N(0, 1) Computing the P-value The p-value answers the question: “If the null hypothesis were true, what is the probability of observing a test statistic as extreme as or more extreme than what we actually observed?” For our one-sided test: p ext{-value} = P(\\zeta \\geq 1.83 \\mid H_0 ext{ is true}) Using a standard normal table or software, we find: p ext{-value} \u0007pprox 0.034 ext{ or } 3.4\\% Making the Decision We compare our p-value to our significance level: - p-value = 3.4% - \u0007lpha = 4% Since the p-value (3.4%) is less than our significance level (4%), we reject the null hypothesis ."
  },
  {
    "objectID": "testing-mean-large#section-10",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "📊 Conclusion",
    "text": "We have sufficient evidence at the 4% significance level to conclude that the new curriculum improves average test scores beyond 80. The probability of observing a sample mean as high as 83 (or higher) purely by chance, if the true population mean were 80 or less, is only 3.4%."
  },
  {
    "objectID": "testing-mean-large#visual-interpretation",
    "href": "/chapter/testing-mean-large#visual-interpretation",
    "title": "Testing a claim about a population mean",
    "section": "Visual Interpretation",
    "text": "Let’s visualize what we’ve done. Picture the sampling distribution under the null hypothesis: a normal curve centered at 80 with standard deviation 1.64. Our observed sample mean of 83 falls in the right tail of this distribution. The p-value is the area under this curve to the right of 83—it represents how much of the distribution lies at or beyond our observed value. This area is relatively small (3.4%), indicating that our observation would be quite unusual if the null hypothesis were true."
  },
  {
    "objectID": "testing-mean-large#the-small-sample-case-when-n-30",
    "href": "/chapter/testing-mean-large#the-small-sample-case-when-n-30",
    "title": "Testing a claim about a population mean",
    "section": "The Small Sample Case: When n < 30",
    "text": "Everything we’ve done so far assumes a large sample (typically n \\geq 30 ). But what happens when we have a small sample? The mathematics changes in an important way. The Problem with Small Samples Consider the same problem, but now suppose we only have n = 24 students in our sample. The sample mean is still 83, and the sample standard deviation is still 10.1. The key difference: when we use the sample standard deviation s to estimate the population standard deviation \\sigma , we introduce additional uncertainty. This uncertainty becomes problematic when the sample size is small. William Gosset’s T-Distribution In the early 1900s, William Sealy Gosset (writing under the pseudonym “Student” because his employer, Guinness Brewery, didn’t allow employees to publish) discovered that for small samples, the test statistic doesn’t follow a normal distribution—it follows a t-distribution . The test statistic is still calculated the same way: \\zeta = rac{\bar{Y} - \\mu_0}{s/\\sqrt{n}} But now, instead of following a Z-distribution, ζ follows a t-distribution with $ u = n-1$ degrees of freedom : \\zeta \\sim t_{ u}"
  },
  {
    "objectID": "testing-mean-large#section-13",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 Why Degrees of Freedom?",
    "text": "We lose one degree of freedom because we used one piece of information from our sample to estimate the population standard deviation. We “expended” one observation to estimate the mean, which we then used to calculate the standard deviation. In our example with n = 24 , we have $ u = 23$ degrees of freedom. Properties of the T-Distribution The t-distribution looks similar to the normal distribution—it’s symmetric and bell-shaped—but it has heavier tails . This reflects the additional uncertainty from estimating the standard deviation. Key properties: 1. As the degrees of freedom increase, the t-distribution approaches the normal distribution 2. For small degrees of freedom, the tails are much heavier than the normal 3. By $ u \u0007pprox 30$, the t-distribution is virtually indistinguishable from the normal Comparing the Two Ratios Let’s clarify the distinction between two similar-looking ratios: Ratio A (with known σ): ext{Andrew} = rac{\bar{Y} - \\mu_0}{\\sigma/\\sqrt{n}} Ratio B (with estimated s): ext{Ben} = rac{\bar{Y} - \\mu_0}{s/\\sqrt{n}}"
  },
  {
    "objectID": "testing-mean-large#section-14",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🤔 Question",
    "text": "Which ratio fluctuates more from sample to sample—Andrew or Ben? Answer: Ben fluctuates more because both the numerator AND the denominator are random variables. In Andrew, only the numerator ( \bar{Y} ) varies; the denominator ( \\sigma/\\sqrt{n} ) is a known constant. In Ben, both \bar{Y} and s vary from sample to sample, creating additional volatility. This extra volatility is exactly what the t-distribution accounts for. Small Sample Analysis: Our Example Let’s return to our curriculum problem with the small sample of 24 students: n = 24 \bar{y} = 83 s = 10.1 SE = 10.1/\\sqrt{24} \u0007pprox 2.06 Test statistic: \\zeta = rac{83 - 80}{2.06} \u0007pprox 1.46 This time, ζ follows a t-distribution with 23 degrees of freedom. Looking up this value in a t-table or using software: p ext{-value} \u0007pprox 0.08 ext{ or } 8\\% The Decision Changes Now our p-value (8%) exceeds our significance level (4%). We fail to reject the null hypothesis ."
  },
  {
    "objectID": "testing-mean-large#section-15",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "⚠️ Critical Insight",
    "text": "Notice what happened: With the same sample mean (83) and the same sample standard deviation (10.1), we reached opposite conclusions depending on our sample size! Large sample ( n=38 ): Reject H_0 (p = 3.4%) Small sample ( n=24 ): Fail to reject H_0 (p = 8%) The difference lies in the additional uncertainty from estimating σ with a small sample. The t-distribution’s heavier tails mean we need more extreme evidence to reject the null hypothesis. When to Use Each Distribution Use the Z-distribution (normal) when: - Sample size is large ( n \\geq 30 ) - Population standard deviation σ is known (rare in practice) Use the t-distribution when: - Sample size is small ( n < 30 ) - Population standard deviation σ is unknown and must be estimated from the sample In practice, many statisticians use the t-distribution for all tests involving estimated standard deviations, regardless of sample size. As the degrees of freedom increase, the t-distribution becomes virtually identical to the normal, so using the t-distribution is a conservative choice that’s always appropriate."
  },
  {
    "objectID": "testing-mean-large#summary-the-hypothesis-testing-framework",
    "href": "/chapter/testing-mean-large#summary-the-hypothesis-testing-framework",
    "title": "Testing a claim about a population mean",
    "section": "Summary: The Hypothesis Testing Framework",
    "text": "Let’s review the complete process we’ve developed: Stage 1: Set Up 1. State the null and alternative hypotheses 2. Choose a significance level α 3. Identify the test type (one-sided or two-sided) Stage 2: Characterize the Sampling Distribution 1. Select an appropriate estimator 2. Use theory (CLT) to establish its distribution 3. Estimate the parameters of this distribution 4. Visualize the distribution under H_0 Stage 3: Test and Decide 1. Calculate the test statistic (ζ) 2. Determine its distribution (Z or t) 3. Compute the p-value 4. Compare p-value to α and make a decision 5. State your conclusion in context"
  },
  {
    "objectID": "testing-mean-large#section-17",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🔄 The Theoretical Foundation",
    "text": "Notice how our hypothesis testing framework rests on a foundation of theoretical results: Markov’s Inequality → proves Chebyshev’s Inequality Chebyshev’s Inequality → proves the Law of Large Numbers Law of Large Numbers → proves the Central Limit Theorem Central Limit Theorem → justifies the normality of sampling distributions Unbiasedness → tells us where distributions are centered Standard Error Formula → quantifies sampling variability Each piece plays a crucial role in the edifice we’ve constructed."
  },
  {
    "objectID": "testing-mean-large#looking-ahead-two-sample-tests-and-causality",
    "href": "/chapter/testing-mean-large#looking-ahead-two-sample-tests-and-causality",
    "title": "Testing a claim about a population mean",
    "section": "Looking Ahead: Two-Sample Tests and Causality",
    "text": "The hypothesis test we’ve developed compares a population mean to a known constant (80). While this is valuable for understanding the mechanics of hypothesis testing, it’s relatively rare in practice. More commonly, we want to compare two groups : a control group and a treatment group. This leads to two-sample tests, which will be our next topic. Two-sample tests look very similar mechanically to what we’ve done here, with a crucial philosophical difference: they allow us to investigate causality . By comparing a treatment group to a control group, we can begin to assess whether an intervention has a causal effect on an outcome. The foundation you’ve built here—understanding sampling distributions, standard errors, test statistics, and p-values—will carry forward directly to these more powerful tests. The transition is a relatively small mechanical extension, but it opens the door to answering causal questions."
  },
  {
    "objectID": "testing-mean-large#section-19",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Key Takeaways",
    "text": "Standard error measures the precision of an estimator across repeated samples Hypothesis testing provides a formal framework for making decisions under uncertainty The Central Limit Theorem justifies using normal distributions for large samples P-values quantify how unusual our observed data would be if H_0 were true Small samples require the t-distribution to account for additional uncertainty Larger samples provide more reliable estimates and more powerful tests Always visualize your distributions to develop intuition about your tests"
  },
  {
    "objectID": "testing-mean-large#section-21",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 1: Understanding Standard Error",
    "text": "Suppose you have two samples from the same population: - Sample A: n = 25 , s = 15 - Sample B: n = 100 , s = 15 Which sample provides more precise estimates of the population mean? Calculate the standard error for each and explain the difference. Answer: Sample B provides more precise estimates. For Sample A: SE_A = 15/\\sqrt{25} = 15/5 = 3 For Sample B: SE_B = 15/\\sqrt{100} = 15/10 = 1.5 Sample B has a standard error half the size of Sample A, even though they have the same sample standard deviation. The fourfold increase in sample size (from 25 to 100) results in a twofold increase in precision (standard error is halved). This demonstrates the principle that larger samples yield more reliable estimates."
  },
  {
    "objectID": "testing-mean-large#section-22",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 2: Interpreting Test Statistics",
    "text": "A researcher calculates a test statistic of ζ = 2.5 for a large sample test. Explain what this value means in plain language, and describe where this value would fall on a standard normal distribution. Answer: A test statistic of ζ = 2.5 means that the observed sample mean is 2.5 standard errors above the hypothesized population mean under the null hypothesis. On a standard normal distribution (bell curve), this value falls in the right tail. Approximately 99.4% of the distribution lies to the left of 2.5 standard deviations from the mean, so only about 0.6% lies beyond this point. This suggests the observation would be quite unusual if the null hypothesis were true. For a one-sided test, the p-value would be approximately 0.006 or 0.6%, which would lead to rejection of the null at common significance levels."
  },
  {
    "objectID": "testing-mean-large#section-23",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 3: Large vs. Small Sample Tests",
    "text": "You conduct the same hypothesis test twice: once with a sample size of 100 and once with a sample size of 20. In both cases, you observe the same sample mean and sample standard deviation. Will the p-values be the same? Why or why not? Answer: No, the p-values will not be the same, and the small sample will generally produce a larger p-value. With n = 100 , we use the Z-distribution (or t-distribution with 99 df, which is virtually identical). The standard error will be relatively small: SE = s/\\sqrt{100} = s/10 . With n = 20 , we must use the t-distribution with 19 degrees of freedom, which has heavier tails than the normal. Additionally, the standard error will be larger: SE = s/\\sqrt{20} \u0007pprox s/4.47 . Two effects compound: 1. The larger standard error makes the test statistic smaller 2. The t-distribution with few degrees of freedom makes any given test statistic less significant Both effects work against rejecting the null hypothesis, resulting in a larger p-value for the small sample test."
  },
  {
    "objectID": "testing-mean-large#section-24",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 4: Choosing Significance Levels",
    "text": "Explain why a researcher might choose a more stringent significance level (say, α = 0.01) rather than a more lenient one (say, α = 0.10). What are the tradeoffs involved? Answer: The choice of significance level involves a tradeoff between Type I and Type II errors: Choosing a more stringent α (like 0.01): - Benefit: Lower risk of Type I error (falsely rejecting a true null hypothesis) - Cost: Higher risk of Type II error (failing to reject a false null hypothesis) - When appropriate: When the cost of a false positive is high (e.g., approving an unsafe drug, implementing an expensive program that doesn’t work) Choosing a more lenient α (like 0.10): - Benefit: Lower risk of Type II error (more power to detect real effects) - Cost: Higher risk of Type I error - When appropriate: When we want to detect potentially important effects for further investigation, or when the cost of a false negative is high In fields where the consequences of a Type I error are severe (like medical research or engineering safety), researchers typically use stricter significance levels. In exploratory research or preliminary studies, more lenient levels might be acceptable."
  },
  {
    "objectID": "testing-mean-small",
    "href": "/chapter/testing-mean-small",
    "title": "Claim about a population mean (small sample)",
    "section": "",
    "text": "Claim about a population mean (small sample) This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-multiple-means",
    "href": "/chapter/testing-multiple-means",
    "title": "Claim about multiple population means",
    "section": "",
    "text": "Claim about multiple population means This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-two-means",
    "href": "/chapter/testing-two-means",
    "title": "Claims about two population means",
    "section": "",
    "text": "Claims about two population means This chapter is currently under development. Content coming soon."
  }
]