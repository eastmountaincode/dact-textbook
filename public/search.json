[
  {
    "objectID": "bayesian-inference",
    "href": "/chapter/bayesian-inference",
    "title": "Bayesian inference",
    "section": "",
    "text": "Bayesian inference This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "bivariate-regression",
    "href": "/chapter/bivariate-regression",
    "title": "Bivariate regression",
    "section": "",
    "text": "Bivariate regression This chapter is coming soon. Add your content here. You can add question/answer pairs, important boxes, videos, and other content following the same format as Chapter 1."
  },
  {
    "objectID": "bounding_outliers",
    "href": "/chapter/bounding_outliers",
    "title": "Bounding Outliers",
    "section": "",
    "text": "Bounding Outliers Understanding the behavior of extreme values, or outliers, is crucial in statistical analysis. In real-world data, we often encounter observations that lie far from the center of a distribution. While we may not know the exact probability of such extreme events, probability inequalities allow us to establish upper bounds on how likely they are to occur. This chapter introduces two fundamental inequalities—Markov’s inequality and Chebyshev’s inequality—that help us bound the probability of outliers using only basic distributional properties."
  },
  {
    "objectID": "bounding_outliers#why-bounding-outliers-matters",
    "href": "/chapter/bounding_outliers#why-bounding-outliers-matters",
    "title": "Bounding Outliers",
    "section": "Why Bounding Outliers Matters",
    "text": "Why do we need mathematical tools to bound the probability of outliers? In many practical situations, we don’t know the complete probability distribution of a random variable. However, we often know simpler properties like the mean or variance. Probability inequalities allow us to make rigorous statements about tail probabilities (the likelihood of extreme values) using only this limited information. This is invaluable for risk assessment, quality control, and understanding the reliability of statistical estimates. Consider a manufacturing process where you’re monitoring the weight of products. You know the average weight is 500 grams, but you don’t know the full distribution of weights. If a product weighs 1000 grams or more, it might indicate a defect. How can you bound the probability of such an outlier? This is precisely the type of question that Markov’s inequality addresses."
  },
  {
    "objectID": "bounding_outliers#markovs-inequality",
    "href": "/chapter/bounding_outliers#markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Markov’s Inequality",
    "text": "Markov’s inequality provides a remarkably simple bound on tail probabilities for non-negative random variables, requiring only knowledge of the mean. Markov’s Inequality Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} This inequality tells us that the probability of a non-negative random variable exceeding some value a is at most the mean divided by a . The larger the value of a relative to the mean, the smaller this upper bound becomes. What does Markov’s inequality tell us intuitively? Markov’s inequality formalizes the intuition that if a non-negative random variable has a small mean, it’s unlikely to take on very large values. For instance, if the average value is 10, the probability of seeing a value of 100 or more cannot exceed 10/100 = 0.1, or 10%. Example: Manufacturing Quality Control Let’s return to our manufacturing example. Suppose the average product weight is \\mathbb{E}(Y) = 500 grams, and all products have non-negative weight. We want to know: what’s the maximum probability that a randomly selected product weighs 1000 grams or more? Using Markov’s inequality with a = 1000 : \\mathrm{P}(Y \\geq 1000) \\leq \\frac{500}{1000} = 0.5 This tells us that at most 50% of products can weigh 1000 grams or more. While this bound might seem loose, remember that we derived it using only the mean—no other information about the distribution! We can also ask: what’s the probability of a product weighing at least twice the average? \\mathrm{P}(Y \\geq 1000) = \\mathrm{P}(Y \\geq 2 \\cdot 500) \\leq \\frac{1}{2} More generally, the probability of exceeding k times the mean is bounded by 1/k : \\mathrm{P}(Y \\geq k \\cdot \\mathbb{E}(Y)) \\leq \\frac{1}{k}"
  },
  {
    "objectID": "bounding_outliers#chebyshevs-inequality",
    "href": "/chapter/bounding_outliers#chebyshevs-inequality",
    "title": "Bounding Outliers",
    "section": "Chebyshev’s Inequality",
    "text": "While Markov’s inequality is powerful in its simplicity, we can obtain tighter bounds if we have more information. Chebyshev’s inequality leverages both the mean and variance to bound the probability of deviations from the mean in either direction. Chebyshev’s Inequality Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} This inequality bounds the probability that Y deviates from its mean \\mu by at least k standard deviations. Notice that the bound depends on k^2 rather than k , making it much tighter than Markov’s inequality for large deviations. How does Chebyshev’s inequality improve upon Markov’s inequality? Chebyshev’s inequality provides three key advantages: (1) it applies to any random variable, not just non-negative ones; (2) it bounds deviations in both directions from the mean; and (3) it typically provides tighter bounds because it incorporates information about variability through the variance. The 1/k^2 decay is much faster than the 1/k decay in Markov’s inequality. Example: Manufacturing Quality Control Revisited Let’s enhance our manufacturing example with variance information. Suppose products have mean weight \\mu = 500 grams and standard deviation \\sigma = 50 grams. We want to bound the probability that a product’s weight deviates from the mean by 100 grams or more (either heavier or lighter). Here, we’re asking about \\mathrm{P}(|Y - 500| \\geq 100) . Since 100 = 2 \\times 50 = 2\\sigma , we have k = 2 . Applying Chebyshev’s inequality: \\mathrm{P}(|Y - 500| \\geq 100) = \\mathrm{P}(|Y - \\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^2} = \\frac{1}{4} = 0.25 So at most 25% of products have weights outside the range [400, 600] grams. Let’s compare this to what Markov’s inequality would tell us. For the upper tail only, Markov’s inequality gives: \\mathrm{P}(Y \\geq 600) \\leq \\frac{500}{600} \\approx 0.833 Chebyshev’s inequality provides a much tigh"
  },
  {
    "objectID": "bounding_outliers#practical-implications",
    "href": "/chapter/bounding_outliers#practical-implications",
    "title": "Bounding Outliers",
    "section": "Practical Implications",
    "text": "These inequalities have far-reaching applications: Quality control: Set tolerance limits based on guaranteed maximum defect rates Risk management: Bound the probability of extreme losses without assuming specific distributions Algorithm analysis: Bound the probability that a randomized algorithm performs poorly Sample size determination: Ensure that sample means are close to population means with high probability The key insight is that even with minimal information (just the mean, or the mean and variance), we can make rigorous probabilistic statements about outliers. While these bounds may not be tight for specific distributions, their generality and simplicity make them indispensable tools in statistical reasoning. Proofs of Probability Inequalities"
  },
  {
    "objectID": "bounding_outliers#proof-of-markovs-inequality",
    "href": "/chapter/bounding_outliers#proof-of-markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Proof of Markov’s Inequality",
    "text": "Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} Proof. Since Y is defined over the positive subspace of \\mathbb{R}^1 , its expectation is \\mathbb{E}(Y) = \\int_0^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY Given some arbitrary positive constant a , the right-hand side of this equation can be partitioned as \\begin{aligned} \\mathbb{E}(Y) &= \\int_0^a Y \\cdot \\mathrm{P}(Y) \\, dY + \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} a \\cdot \\mathrm{P}(Y) \\, dY \\\\ &= a \\int_a^{\\infty} \\mathrm{P}(Y) \\, dY \\\\ &= a \\cdot \\mathrm{P}(Y \\geq a) \\end{aligned} The first inequality holds because we drop a non-negative term (the integral from 0 to a ). The second inequality holds because Y \\geq a in the region of integration, so Y \\cdot \\mathrm{P}(Y) \\geq a \\cdot \\mathrm{P}(Y) . Dividing both sides by a and rearranging terms, we obtain \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} ◻"
  },
  {
    "objectID": "bounding_outliers#proof-of-chebyshevs-inequality",
    "href": "/chapter/bounding_outliers#proof-of-chebyshevs-inequality",
    "title": "Bounding Outliers",
    "section": "Proof of Chebyshev’s Inequality",
    "text": "Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} Proof. Since Y has finite mean \\mu , the random variable (Y - \\mu)^2 is defined over the positive subspace of \\mathbb{R}^1 . By Markov’s inequality, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\mathbb{E}\\left((Y - \\mu)^2\\right)}{a} By definition, \\mathbb{E}((Y - \\mu)^2) = \\sigma^2 . Therefore, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\sigma^2}{a} For any real k > 0 , define a \\equiv k^2\\sigma^2 . Substituting for a , \\mathrm{P}\\left((Y - \\mu)^2 \\geq k^2\\sigma^2\\right) \\leq \\frac{\\sigma^2}{k^2\\sigma^2} = \\frac{1}{k^2} Since (Y - \\mu)^2 \\geq k^2\\sigma^2 is equivalent to |Y - \\mu| \\geq k\\sigma , we have \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} ◻"
  },
  {
    "objectID": "bounds-outliers",
    "href": "/chapter/bounds-outliers",
    "title": "Bounding Outliers",
    "section": "",
    "text": "Bounding Outliers Understanding the behavior of extreme values, or outliers, is crucial in statistical analysis. In real-world data, we often encounter observations that lie far from the center of a distribution. While we may not know the exact probability of such extreme events, probability inequalities allow us to establish upper bounds on how likely they are to occur. This chapter introduces two fundamental inequalities—Markov’s inequality and Chebyshev’s inequality—that help us bound the probability of outliers using only basic distributional properties."
  },
  {
    "objectID": "bounds-outliers#why-bounding-outliers-matters",
    "href": "/chapter/bounds-outliers#why-bounding-outliers-matters",
    "title": "Bounding Outliers",
    "section": "Why Bounding Outliers Matters",
    "text": "Why do we need mathematical tools to bound the probability of outliers? In many practical situations, we don’t know the complete probability distribution of a random variable. However, we often know simpler properties like the mean or variance. Probability inequalities allow us to make rigorous statements about tail probabilities (the likelihood of extreme values) using only this limited information. This is invaluable for risk assessment, quality control, and understanding the reliability of statistical estimates. Consider a manufacturing process where you’re monitoring the weight of products. You know the average weight is 500 grams, but you don’t know the full distribution of weights. If a product weighs 1000 grams or more, it might indicate a defect. How can you bound the probability of such an outlier? This is precisely the type of question that Markov’s inequality addresses."
  },
  {
    "objectID": "bounds-outliers#markovs-inequality",
    "href": "/chapter/bounds-outliers#markovs-inequality",
    "title": "Bounding Outliers",
    "section": "Markov’s Inequality",
    "text": "Markov’s inequality provides a remarkably simple bound on tail probabilities for non-negative random variables, requiring only knowledge of the mean. Markov’s Inequality Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} {#eq-markov} This inequality tells us that the probability of a non-negative random variable exceeding some value a is at most the mean divided by a . The larger the value of a relative to the mean, the smaller this upper bound becomes. What does Markov’s inequality tell us intuitively? Markov’s inequality formalizes the intuition that if a non-negative random variable has a small mean, it’s unlikely to take on very large values. For instance, if the average value is 10, the probability of seeing a value of 100 or more cannot exceed 10/100 = 0.1, or 10%. Example: Manufacturing Quality Control Let’s return to our manufacturing example. Suppose the average product weight is \\mathbb{E}(Y) = 500 grams, and all products have non-negative weight. We want to know: what’s the maximum probability that a randomly selected product weighs 1000 grams or more? Using Markov’s inequality with a = 1000 : \\mathrm{P}(Y \\geq 1000) \\leq \\frac{500}{1000} = 0.5 This tells us that at most 50% of products can weigh 1000 grams or more. While this bound might seem loose, remember that we derived it using only the mean—no other information about the distribution! We can also ask: what’s the probability of a product weighing at least twice the average? \\mathrm{P}(Y \\geq 1000) = \\mathrm{P}(Y \\geq 2 \\cdot 500) \\leq \\frac{1}{2} More generally, the probability of exceeding k times the mean is bounded by 1/k : \\mathrm{P}(Y \\geq k \\cdot \\mathbb{E}(Y)) \\leq \\frac{1}{k}"
  },
  {
    "objectID": "bounds-outliers#chebyshevs-inequality",
    "href": "/chapter/bounds-outliers#chebyshevs-inequality",
    "title": "Bounding Outliers",
    "section": "Chebyshev’s Inequality",
    "text": "While Markov’s inequality is powerful in its simplicity, we can obtain tighter bounds if we have more information. Chebyshev’s inequality leverages both the mean and variance to bound the probability of deviations from the mean in either direction. Chebyshev’s Inequality Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} {#eq-chebyshev} This inequality bounds the probability that Y deviates from its mean \\mu by at least k standard deviations. Notice that the bound depends on k^2 rather than k , making it much tighter than Markov’s inequality for large deviations. How does Chebyshev’s inequality improve upon Markov’s inequality? Chebyshev’s inequality provides three key advantages: (1) it applies to any random variable, not just non-negative ones; (2) it bounds deviations in both directions from the mean; and (3) it typically provides tighter bounds because it incorporates information about variability through the variance. The 1/k^2 decay is much faster than the 1/k decay in Markov’s inequality. Example: Manufacturing Quality Control Revisited Let’s enhance our manufacturing example with variance information. Suppose products have mean weight \\mu = 500 grams and standard deviation \\sigma = 50 grams. We want to bound the probability that a product’s weight deviates from the mean by 100 grams or more (either heavier or lighter). Here, we’re asking about \\mathrm{P}(|Y - 500| \\geq 100) . Since 100 = 2 \\times 50 = 2\\sigma , we have k = 2 . Applying Chebyshev’s inequality: \\mathrm{P}(|Y - 500| \\geq 100) = \\mathrm{P}(|Y - \\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^2} = \\frac{1}{4} = 0.25 So at most 25% of products have weights outside the range [400, 600] grams. Let’s compare this to what Markov’s inequality would tell us. For the upper tail only, Markov’s inequality gives: \\mathrm{P}(Y \\geq 600) \\leq \\frac{500}{600} \\approx 0.833 Chebyshev’s inequality prov"
  },
  {
    "objectID": "bounds-outliers#practical-implications",
    "href": "/chapter/bounds-outliers#practical-implications",
    "title": "Bounding Outliers",
    "section": "Practical Implications",
    "text": "These inequalities have far-reaching applications: Quality control: Set tolerance limits based on guaranteed maximum defect rates Risk management: Bound the probability of extreme losses without assuming specific distributions Algorithm analysis: Bound the probability that a randomized algorithm performs poorly Sample size determination: Ensure that sample means are close to population means with high probability The key insight is that even with minimal information (just the mean, or the mean and variance), we can make rigorous probabilistic statements about outliers. While these bounds may not be tight for specific distributions, their generality and simplicity make them indispensable tools in statistical reasoning."
  },
  {
    "objectID": "bounds-outliers#sec-proofs-inequalities",
    "href": "/chapter/bounds-outliers#sec-proofs-inequalities",
    "title": "Bounding Outliers",
    "section": "Appendix: Proofs of Probability Inequalities",
    "text": "Proof of Markov’s Inequality Let Y be a random variable defined over the positive subspace of \\mathbb{R}^1 . Then for any positive constant a > 0 , \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} Since Y is defined over the positive subspace of \\mathbb{R}^1 , its expectation is \\mathbb{E}(Y) = \\int_0^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY Given some arbitrary positive constant a , the right-hand side of this equation can be partitioned as \\begin{aligned} \\mathbb{E}(Y) &= \\int_0^a Y \\cdot \\mathrm{P}(Y) \\, dY + \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} Y \\cdot \\mathrm{P}(Y) \\, dY \\\\ &\\geq \\int_a^{\\infty} a \\cdot \\mathrm{P}(Y) \\, dY \\\\ &= a \\int_a^{\\infty} \\mathrm{P}(Y) \\, dY \\\\ &= a \\cdot \\mathrm{P}(Y \\geq a) \\end{aligned} The first inequality holds because we drop a non-negative term (the integral from 0 to a ). The second inequality holds because Y \\geq a in the region of integration, so Y \\cdot \\mathrm{P}(Y) \\geq a \\cdot \\mathrm{P}(Y) . Dividing both sides by a and rearranging terms, we obtain \\mathrm{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}(Y)}{a} Proof of Chebyshev’s Inequality Let Y be a random variable with finite mean \\mu and finite, non-zero variance \\sigma^2 . Then for any real number k > 0 , \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} Since Y has finite mean \\mu , the random variable (Y - \\mu)^2 is defined over the positive subspace of \\mathbb{R}^1 . By Markov’s inequality, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\mathbb{E}\\left((Y - \\mu)^2\\right)}{a} By definition, \\mathbb{E}((Y - \\mu)^2) = \\sigma^2 . Therefore, \\mathrm{P}\\left((Y - \\mu)^2 \\geq a\\right) \\leq \\frac{\\sigma^2}{a} For any real k > 0 , define a \\equiv k^2\\sigma^2 . Substituting for a , \\mathrm{P}\\left((Y - \\mu)^2 \\geq k^2\\sigma^2\\right) \\leq \\frac{\\sigma^2}{k^2\\sigma^2} = \\frac{1}{k^2} Since (Y - \\mu)^2 \\geq k^2\\sigma^2 is equivalent to |Y - \\mu| \\geq k\\sigma , we have \\mathrm{P}(|Y - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}"
  },
  {
    "objectID": "correlation",
    "href": "/chapter/correlation",
    "title": "Correlation",
    "section": "",
    "text": "Correlation This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "data",
    "href": "/chapter/data",
    "title": "Data",
    "section": "",
    "text": "Data This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "dichotomous-choice#introduction",
    "href": "/chapter/dichotomous-choice#introduction",
    "title": "Dichotomous Choice Modeling",
    "section": "Introduction",
    "text": "In the 1980s, researchers Ben-Akiva and Lerman interviewed commuters in Boston about their transportation choices. Their specific research question was simple but important: Does the difference in commute time between car and bus affect people’s mode choice? To answer this, they surveyed huundreds of commuters, collecting data on: - Their actual commute times by car and by bus - Their actual commuting choice (0 = drove to work, 1 = took the bus) Here’s a table showing these data for 21 of the commuters surveyed. While the authors collected data on a whole range of variables, we will just ignore them for the purpose of this chapter. In our model, we will not include any controls to keep things simple. In real life, of course, a person’s commuting choice will depend of many, many factors. ID Commute Time (minutes) Choice Auto Bus :–: :—-: :—: :——: 1 51.0 85.0 0 2 95.0 43.5 1 3 18.5 84.0 0 4 62.0 4.4 1 5 41.5 24.5 1 6 2.0 91.2 0 7 82.0 38.0 1 8 27.6 79.7 0 9 99.1 2.2 1 10 51.4 83.8 0 11 8.6 1.6 1 12 22.5 74.1 0 13 51.8 20.2 1 14 4.1 86.9 0 15 62.2 90.1 0 16 89.9 2.2 1 17 41.6 91.5 0 18 56.2 31.6 1 19 4.1 28.5 1 20 95.1 22.5 1 21 81.0 19.2 1 Some of the values in the table are very odd. And no, I double-checked, I transcribed them here correctly. The variable choice is coded as either 0 or 1, where 0 is the code for commuters who drove to work and 1 for those that took the bus. We can now sort these by this variable and calculate the difference in commute times. We will further assume that a person’s commuting choice depends only on the difference in commute time between the two options they have. Here’s the modified table. ID Commute Time (minutes) Choice Auto Bus 1 51.0 85.0 0 2 95.0 43.5 1 3 18.5 84.0 0 4 62.0 4.4 1 5 41.5 24.5 1 6 2.0 91.2 0 7 82.0 38.0 1 8 27.6 79.7 0 9 99.1 2.2 1 10 51.4 83.8 0 11 8.6 1.6 1 12 22.5 74.1 0 13 51.8 20.2 1 14 4.1 86.9 0 15 62.2 90.1 0 16 89.9 2.2 1 17 41.6 91.5 0 18 56.2 31.6 1 19 4.1 28.5 1 20 95.1 22.5 1 21 81.0 19.2 1 We can now m"
  },
  {
    "objectID": "dichotomous-choice#a-problem-of-transportation-planning",
    "href": "/chapter/dichotomous-choice#a-problem-of-transportation-planning",
    "title": "Dichotomous Choice Modeling",
    "section": "A Problem of Transportation Planning",
    "text": "In the mid-1960s, traffic congestion in the Bay Area had reached a critical juncture. The California Highway Commission faced a fundamental decision: should they continue investing in freeway expansion, or could a new mass transit system offer a better path forward? They proposed an ambitious solution—a network of buses and rail that would connect the region, fundamentally reshaping how people commuted. But there was a problem. Before committing billions in public resources, the Commission needed to answer a deceptively simple question: How many people would actually use this system? In 1969, with the first BART station under construction, the Commission faced a pilot phase evaluation. They needed to estimate ridership—not based on hunches or optimistic projections, but on actual data about people’s choices. So they conducted an extensive survey of Bay Area residents, asking a seemingly straightforward question: Would you take the bus instead of driving? Yes or no. This binary question—a dichotomous choice—would unlock something far more significant than transit planning. It would lead to the discovery of a new statistical framework that would transform how economists, marketers, and policymakers understand decision-making itself."
  },
  {
    "objectID": "dichotomous-choice#the-birth-of-a-framework-dan-mcfaddens-insight",
    "href": "/chapter/dichotomous-choice#the-birth-of-a-framework-dan-mcfaddens-insight",
    "title": "Dichotomous Choice Modeling",
    "section": "The Birth of a Framework: Dan McFadden’s Insight",
    "text": "The Commission’s first instinct was to use standard regression—treating the yes/no responses as if they were continuous measurements. Using this linear probability model, they estimated that about 15% of Bay Area residents would use the new transit system. But then they hired a young economist named Dan McFadden, recently arrived at UC Berkeley. McFadden looked at the problem differently. He recognized something fundamental: when people make discrete choices—yes or no, use transit or drive, buy or don’t buy—the standard tools of regression analysis were fundamentally mismatched to the problem. McFadden developed a new approach using what he called latent variable models . The insight was elegant: behind every observed choice lies an unobserved psychological disposition. When someone decides whether to take the bus, they’re processing information about commute time, cost, convenience, and comfort—all of which feed into a latent evaluation of the option. When that latent evaluation exceeds some threshold, they choose to use transit. Using this framework, McFadden predicted that only about 6.3% of residents would use BART. His colleagues dismissed this as too pessimistic. Yet when BART opened and ridership was measured, it came in at 6.2%—remarkably close to McFadden’s prediction. This work on discrete choice modeling was so significant that in 2000—more than three decades later—McFadden was awarded the Nobel Prize in Economics. The Nobel citation recognized his contribution: “he showed how to statistically handle fundamental aspects of microdata, namely data on the most important decisions we make in life: the choice of education, occupation, place of residence, marital status, number of children, so called discrete choices.” Today, the methods McFadden pioneered are used everywhere: predicting consumer behavior, understanding labor market decisions, analyzing election outcomes, and evaluating policy interventions."
  },
  {
    "objectID": "dichotomous-choice#back-to-boston-understanding-commuting-choices",
    "href": "/chapter/dichotomous-choice#back-to-boston-understanding-commuting-choices",
    "title": "Dichotomous Choice Modeling",
    "section": "Back to Boston: Understanding Commuting Choices",
    "text": "The data we plotted above tell a clear visual story. When the difference in commute time favors driving (negative values), people drive. When the difference favors the bus (positive values), people take transit. Yet there’s variation even within these patterns—some people take the bus despite longer commute times, and others drive even when the bus would be faster. So let’s get on with our task. We will build a model that answers the following question: If the commute time by transit could be reduce, by how much would the probability that someone will choose transit increase?"
  },
  {
    "objectID": "dichotomous-choice#the-binary-probability-function",
    "href": "/chapter/dichotomous-choice#the-binary-probability-function",
    "title": "Dichotomous Choice Modeling",
    "section": "The Binary Probability Function",
    "text": "To begin, let’s establish some basic foundations. When people make dichotomous (two-choice) decisions, we can describe the outcome using the Bernoulli distribution ."
  },
  {
    "objectID": "dichotomous-choice#section-6",
    "href": "/chapter/dichotomous-choice",
    "title": "Dichotomous Choice Modeling",
    "section": "The Bernoulli Distribution",
    "text": "For a dichotomous outcome Y that takes the value 1 with probability p and the value 0 with probability (1-p) , the probability function is: f(y) = p^y(1-p)^{1-y} The expected value of Y is simply: E(Y) = (1-p) \\times 0 + p \\times 1 = p In our commuting example, Y = 1 represents choosing transit and Y = 0 represents choosing a car. The probability p represents the probability that an individual will choose transit, given their specific circumstances. Following standard econometric practice, we decompose the observed outcome into a deterministic part (what we can predict) and a stochastic part (random variation): Y_i = p_i + \\epsilon_i where p_i is the predicted probability for individual i and \\epsilon_i is the error term. The key question becomes: How does the difference in commute times relate to p ?"
  },
  {
    "objectID": "dichotomous-choice#the-linear-probability-model-a-first-attempt",
    "href": "/chapter/dichotomous-choice#the-linear-probability-model-a-first-attempt",
    "title": "Dichotomous Choice Modeling",
    "section": "The Linear Probability Model: A First Attempt",
    "text": "The most straightforward approach is the Linear Probability Model (LPM) , which assumes a linear relationship between the commute time difference and the probability of choosing transit: p_i = \\beta_0 + \\beta_1 \\cdot \\text{diff}_i where \\text{diff}_i = \\text{car\\_time}_i - \\text{bus\\_time}_i is the difference in commute times for individual i ."
  },
  {
    "objectID": "dichotomous-choice#question",
    "href": "/chapter/dichotomous-choice#question",
    "title": "Dichotomous Choice Modeling",
    "section": "Question",
    "text": "What are the advantages of starting with a linear model?"
  },
  {
    "objectID": "dichotomous-choice#answer",
    "href": "/chapter/dichotomous-choice#answer",
    "title": "Dichotomous Choice Modeling",
    "section": "Answer",
    "text": "The linear model is computationally simple and has a clear interpretation: \\beta_1 represents the change in probability per unit increase in the commute time difference. It’s easy to estimate using OLS (ordinary least squares) regression, and students already understand how to interpret linear coefficients. Estimating the LPM When we estimate this model on the Ben-Akiva commuting data, we get: \\widehat{p}_i = 0.515 + 0.007031 \\cdot \\text{diff}_i The coefficient on diff is positive and statistically significant, confirming that a longer drive time relative to bus time increases the probability of choosing transit. The magnitude suggests that each additional minute of driving time (relative to bus time) increases the probability of choosing the bus by about 0.7 percentage points. We can also calculate the threshold difference at which someone is indifferent between modes: 0.5 = 0.515 + 0.007031 \\cdot \\text{diff} Solving for the threshold: \\text{diff} = -6.93 minutes. This means when the bus time exceeds car time by about 7 minutes, we’d predict a 50% probability of choosing transit. Problems with the Linear Probability Model Despite its simplicity, the LPM has serious problems that make it unsuitable for modeling binary choices: 1. Unbounded predictions: The model can predict values less than 0 or greater than 1, which are nonsensical for probabilities. Looking at our scatter plot, we can see that the fitted line would predict negative probabilities for very negative differences in commute times. 2. Constant marginal effects: The model assumes that each additional minute of commute time difference has the same effect on the choice probability, regardless of the current situation. But this seems unrealistic. The effect of an additional minute likely matters more when someone is on the fence (near 50% probability) than when they’re already strongly committed to one mode. 3. Heteroskedasticity: Since Y is binary, the variance of the error term is \\text{Var}(\\epsilon_i) ="
  },
  {
    "objectID": "dichotomous-choice#section-10",
    "href": "/chapter/dichotomous-choice",
    "title": "Dichotomous Choice Modeling",
    "section": "The Core Problem",
    "text": "The fundamental issue is that we’re using a tool designed for continuous outcomes to model a categorical choice. We need a fundamentally different approach—one that respects the binary nature of the outcome."
  },
  {
    "objectID": "dichotomous-choice#latent-variable-models-a-better-framework",
    "href": "/chapter/dichotomous-choice#latent-variable-models-a-better-framework",
    "title": "Dichotomous Choice Modeling",
    "section": "Latent Variable Models: A Better Framework",
    "text": "McFadden’s insight was to introduce a latent variable —an unobserved psychological disposition that drives the observed choice. Think of it this way: When a commuter considers transit versus driving, they’re mentally evaluating the overall utility (satisfaction) of each option. Let Y^*_i represent this latent evaluation of transit relative to driving: If Y^*_i > 0 : The net benefit of transit exceeds that of driving → choose transit If Y^*_i \\leq 0 : Driving is better → choose driving The observed choice is determined by an indicator function : Y_i = \\begin{cases} 1 & \\text{if } Y^*_i > 0 \\\\ 0 & \\text{if } Y^*_i \\leq 0 \\end{cases} Your browser does not support the video tag. Animation: This visualization shows how the probit and logit models create S-shaped probability curves that respect the bounds of probability (0 to 1), unlike the linear probability model which can predict impossible values outside this range. Watch how the sigmoid curves are steepest where probability is around 50%—exactly where marginal information matters most for decision-making. We assume the latent variable depends on observed commute times plus unobserved factors: Y^*_i = \\beta_0 + \\beta_1 \\cdot \\text{diff}_i + \\epsilon_i The error term \\epsilon_i captures all the unmeasured factors affecting choice—personal preferences, comfort sensitivity, environmental consciousness, and so on. Deriving the Probability Now we can derive the probability of observing Y_i = 1 : \\Pr(Y_i = 1) = \\Pr(Y^*_i > 0) = \\Pr(\\epsilon_i > -(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i)) Let F denote the cumulative distribution function (CDF) of \\epsilon_i . Assuming \\epsilon_i has zero mean and a symmetric distribution around that mean (standard assumptions), we can show: p_i = \\Pr(Y_i = 1) = F(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i) This is a beautiful result. The probability of choosing transit is a nonlinear function of the commute time difference, determined by whatever distribution we assume for \\epsilon_i ."
  },
  {
    "objectID": "dichotomous-choice#the-probit-model-using-the-normal-distribution",
    "href": "/chapter/dichotomous-choice#the-probit-model-using-the-normal-distribution",
    "title": "Dichotomous Choice Modeling",
    "section": "The Probit Model: Using the Normal Distribution",
    "text": "The probit model assumes \\epsilon_i follows a standard normal distribution: f(\\epsilon_i) = \\phi(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\epsilon_i^2} F(\\epsilon_i) = \\Phi(\\epsilon_i) = \\int_{-\\infty}^{\\epsilon_i} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2} dx Therefore: p_i = \\Phi(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i) where \\Phi is the standard normal CDF (familiar from introductory statistics as the standard normal cumulative probability). Interpreting Probit Coefficients The probit coefficients don’t have a direct probability interpretation. Instead, they tell us the effect on the latent variable Y^*_i . The actual effect on the probability of choosing transit is the marginal effect : \\frac{dp_i}{d(\\text{diff}_i)} = \\phi(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i) \\times \\beta_1 where \\phi is the standard normal probability density function (PDF). Notice that this marginal effect varies depending on the value of the commute time difference—it’s largest when \\beta_0 + \\beta_1 \\cdot \\text{diff}_i \\approx 0 (near 50% probability) and smaller at the extremes."
  },
  {
    "objectID": "dichotomous-choice#the-logit-model-using-the-logistic-distribution",
    "href": "/chapter/dichotomous-choice#the-logit-model-using-the-logistic-distribution",
    "title": "Dichotomous Choice Modeling",
    "section": "The Logit Model: Using the Logistic Distribution",
    "text": "An alternative to probit is the logit model , which assumes \\epsilon_i follows a logistic distribution: f(\\epsilon_i) = \\lambda(\\epsilon_i) = \\frac{e^{\\epsilon_i}}{(1+e^{\\epsilon_i})^2} F(\\epsilon_i) = \\Lambda(\\epsilon_i) = \\frac{e^{\\epsilon_i}}{1+e^{\\epsilon_i}} Therefore: p_i = \\Lambda(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i) = \\frac{e^{\\beta_0 + \\beta_1 \\cdot \\text{diff}_i}}{1 + e^{\\beta_0 + \\beta_1 \\cdot \\text{diff}_i}} This is the logistic function , which has a particularly nice property. If we take the ratio of the probability of choosing transit to the probability of choosing driving: \\frac{p_i}{1-p_i} = e^{\\beta_0 + \\beta_1 \\cdot \\text{diff}_i} Taking natural logarithms: \\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{diff}_i The left side is the log-odds . This means logit coefficients directly represent changes in the log-odds, which is why epidemiologists and health researchers prefer logit. Marginal Effects in Logit The marginal effect in logit is: \\frac{dp_i}{d(\\text{diff}_i)} = \\Lambda(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i) \\times (1-\\Lambda(\\beta_0 + \\beta_1 \\cdot \\text{diff}_i)) \\times \\beta_1 This can also be written as p_i(1-p_i) \\times \\beta_1 , showing how the effect of the commute difference depends on the current probability."
  },
  {
    "objectID": "dichotomous-choice#estimation-maximum-likelihood",
    "href": "/chapter/dichotomous-choice#estimation-maximum-likelihood",
    "title": "Dichotomous Choice Modeling",
    "section": "Estimation: Maximum Likelihood",
    "text": "How do we actually estimate the parameters \\beta_0 and \\beta_1 in probit and logit models? We can’t use OLS because the model isn’t linear. Instead, we use Maximum Likelihood Estimation (MLE) . The core idea is intuitive: choose the parameter values that make the observed data most likely. For binary choice data, the likelihood function is: \\mathcal{L} = \\prod_{i: Y_i=1} p_i \\times \\prod_{i: Y_i=0} (1-p_i) In plain language: the likelihood is the product of the predicted probabilities of choosing transit (for those who did) and predicted probabilities of choosing driving (for those who didn’t)."
  },
  {
    "objectID": "dichotomous-choice#question",
    "href": "/chapter/dichotomous-choice#question",
    "title": "Dichotomous Choice Modeling",
    "section": "Question",
    "text": "Why is maximizing the likelihood the right approach for binary choice models?"
  },
  {
    "objectID": "dichotomous-choice#answer",
    "href": "/chapter/dichotomous-choice#answer",
    "title": "Dichotomous Choice Modeling",
    "section": "Answer",
    "text": "Maximizing likelihood means finding the parameter values that make our observed data as probable as possible. If our model is correct, the parameters that generated the data should make that data more likely than alternative parameter values. This is a fundamental principle of statistical inference that works even when OLS assumptions are violated. How MLE Works (Conceptually) Start with an initial guess about the parameter values For each observation , calculate the predicted probability of their observed choice Compute the likelihood as the product of these probabilities Adjust the parameters to increase the likelihood Repeat until the likelihood stops increasing (convergence) In practice, software implements sophisticated optimization algorithms (like Newton-Raphson) to perform this search efficiently."
  },
  {
    "objectID": "dichotomous-choice#results-comparing-lpm-probit-and-logit",
    "href": "/chapter/dichotomous-choice#results-comparing-lpm-probit-and-logit",
    "title": "Dichotomous Choice Modeling",
    "section": "Results: Comparing LPM, Probit, and Logit",
    "text": "When we estimate these three models on the Ben-Akiva commuting data, here’s what we find: Linear Probability Model: - Coefficient on diff: 0.007031 (p < 0.001) - Intercept: 0.515 Probit Model: - Coefficient on diff: 0.030000 (p = 0.004) - Intercept: 0.064434 - Pseudo-R²: 0.5758 Logit Model: - Coefficient on diff: 0.053110 (p = 0.010) - Intercept: 0.237575 - Pseudo-R²: 0.5757 Note that the probit and logit coefficients are much larger than the LPM coefficient. This isn’t because the effects are actually larger—it’s because these coefficients represent changes in the latent variable, not changes in probability."
  },
  {
    "objectID": "dichotomous-choice#marginal-effects-the-real-story",
    "href": "/chapter/dichotomous-choice#marginal-effects-the-real-story",
    "title": "Dichotomous Choice Modeling",
    "section": "Marginal Effects: The Real Story",
    "text": "To compare effects across models, we must look at marginal effects . Let’s evaluate each at the sample mean commute time difference (approximately 1.22 minutes): LPM Marginal Effect: 0.007031 (constant for all) Probit Marginal Effect at Mean: 0.0119 Logit Marginal Effect at Mean: 0.0130 These are much more comparable! At the average commute time difference, a one-minute increase in the bus time advantage increases the probability of choosing transit by about 0.012 to 0.013 percentage points—much smaller than the LPM’s constant effect. The probit and logit marginal effects are similar but not identical. This is typical—they differ slightly because of the different distributional assumptions, but the differences usually matter less than we might think."
  },
  {
    "objectID": "dichotomous-choice#section-20",
    "href": "/chapter/dichotomous-choice",
    "title": "Dichotomous Choice Modeling",
    "section": "Choosing Between Probit and Logit",
    "text": "For most applications, the choice between probit and logit is a matter of taste. Both models typically produce similar results and marginal effects. The differences are usually small compared to specification issues (like omitted variables or functional form choices). Exceptions and conventions: - Probit is preferred when there’s reason to believe errors are normally distributed or when working with panel data where you want to control for unobserved heteroskedasticity - Logit is preferred in epidemiology because coefficients have a natural interpretation as log-odds ratios - Multinomial logit is the standard choice for multi-choice problems (more than two alternatives) - Ordered probit is conventional for ordinal outcomes (like survey responses on a scale)"
  },
  {
    "objectID": "dichotomous-choice#a-deeper-look-the-visualization",
    "href": "/chapter/dichotomous-choice#a-deeper-look-the-visualization",
    "title": "Dichotomous Choice Modeling",
    "section": "A Deeper Look: The Visualization",
    "text": "When we plot the fitted probabilities from our models against the commute time difference, we see a crucial difference from the LPM: The probit and logit curves are S-shaped (sigmoid), respecting the bounds of probability (0 to 1) They’re steepest in the middle where probability is around 50%, reflecting how information matters most when we’re uncertain They flatten at the extremes , where strongly positive or negative commute differences make the choice obvious The LPM line carelessly violates these bounds, predicting negative probabilities for large negative commute differences."
  },
  {
    "objectID": "dichotomous-choice#computing-marginal-effects-in-practice",
    "href": "/chapter/dichotomous-choice#computing-marginal-effects-in-practice",
    "title": "Dichotomous Choice Modeling",
    "section": "Computing Marginal Effects in Practice",
    "text": "When estimating these models, most software packages can compute marginal effects automatically. In Stata, for example: probit choice diff margins, dydx ( diff ) This gives marginal effects at the sample means. You can also evaluate marginal effects at specific values: margins, dydx ( diff ) at ( diff =30) This would show the effect of the commute time difference when it equals 30 minutes."
  },
  {
    "objectID": "dichotomous-choice#threshold-analysis-the-break-even-point",
    "href": "/chapter/dichotomous-choice#threshold-analysis-the-break-even-point",
    "title": "Dichotomous Choice Modeling",
    "section": "Threshold Analysis: The Break-Even Point",
    "text": "Beyond average marginal effects, policy makers often want to know: At what commute time difference would someone be indifferent between modes? In the probit model, this corresponds to where \\beta_0 + \\beta_1 \\cdot \\text{diff} = 0 (where the latent utility equals zero, giving 50% predicted probability). Solving: \\text{diff}^* = -\\beta_0 / \\beta_1 With our estimates: \\text{diff}^* = -0.064434 / 0.030000 = -2.15 minutes This means a commuter is indifferent when the bus is about 2 minutes faster than driving. Making the bus even 1 minute faster shifts the probability toward transit; making it 1 minute slower shifts probability toward driving. This kind of threshold analysis is tremendously useful for policy evaluation: How much faster does transit need to be to attract a meaningful share of riders?"
  },
  {
    "objectID": "dichotomous-choice#extensions-and-related-models",
    "href": "/chapter/dichotomous-choice#extensions-and-related-models",
    "title": "Dichotomous Choice Modeling",
    "section": "Extensions and Related Models",
    "text": "The framework you’ve learned generalizes in several important directions: Multinomial Logit extends to cases with more than two alternatives (e.g., car, bus, bike, carpool). Ordered Probit handles ordinal outcomes (e.g., survey responses: strongly disagree, disagree, neutral, agree, strongly agree). Nested Logit models choice hierarchies (e.g., first decide public vs. private transport, then choose specific mode). Mixed Logit allows parameter heterogeneity—different people have different preferences, not just different observed characteristics. Conditional Logit handles choice-specific attributes (different modes have different costs and times, not just person-specific traits). Each of these respects the fundamental insight: when people make discrete choices, we need methods that account for the categorical nature of the outcome and the bounded nature of probability."
  },
  {
    "objectID": "dichotomous-choice#summary-from-boston-to-a-general-framework",
    "href": "/chapter/dichotomous-choice#summary-from-boston-to-a-general-framework",
    "title": "Dichotomous Choice Modeling",
    "section": "Summary: From Boston to a General Framework",
    "text": "We started with a real problem: How many people would use a new transit system? A simple question about choice led to a revolution in econometric practice. Dan McFadden recognized that modeling discrete choices required fundamentally different statistical tools. The latent variable framework—where observed choices reflect thresholds of underlying continuous utilities—proved to be exactly the right abstraction. Today, when a company wants to predict product adoption, a city wants to forecast ridership, a researcher wants to understand voting behavior, or a policymaker wants to evaluate a program’s effects on people’s decisions, they use the tools McFadden developed. The methods you’ve learned—probit and logit, maximum likelihood estimation, marginal effect calculation—are the workhorses of modern applied economics. They appear in policy analysis, business strategy, epidemiology, political science, and marketing. Understanding them deeply, as you now do, opens doors to analyzing and understanding human choice across virtually every domain."
  },
  {
    "objectID": "dichotomous-choice#key-formulas-reference",
    "href": "/chapter/dichotomous-choice#key-formulas-reference",
    "title": "Dichotomous Choice Modeling",
    "section": "Key Formulas Reference",
    "text": "Probit Model: p_i = \\Phi(\\beta_0 + \\beta_1 X_i) Logit Model: p_i = \\frac{e^{\\beta_0 + \\beta_1 X_i}}{1 + e^{\\beta_0 + \\beta_1 X_i}} Marginal Effect (Probit): \\frac{dp_i}{dX_i} = \\phi(\\beta_0 + \\beta_1 X_i) \\cdot \\beta_1 Marginal Effect (Logit): \\frac{dp_i}{dX_i} = p_i(1-p_i) \\cdot \\beta_1 Log-Odds (Logit): \\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 X_i"
  },
  {
    "objectID": "estimating-mean",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "",
    "text": "Estimating the population mean In this chapter, we’ll explore three fundamental properties of statistical estimators that form the backbone of statistical inference. We’ll prove that the sample mean is an unbiased estimator of the population mean, demonstrate that it’s the most efficient among all unbiased estimators, and examine why the sample variance requires a correction factor. These proofs are not merely mathematical exercises—they reveal deep truths about how we can reliably learn about populations from samples. By the end of this chapter, you will understand: What makes an estimator “unbiased” and why this matters How to compare estimators using the criterion of efficiency Why the sample variance formula uses n-1 instead of n The relationship between sample statistics and population parameters"
  },
  {
    "objectID": "estimating-mean#the-unbiasedness-of-the-sample-mean",
    "href": "/chapter/estimating-mean#the-unbiasedness-of-the-sample-mean",
    "title": "Estimating the population mean",
    "section": "The Unbiasedness of the Sample Mean",
    "text": "Let’s begin with a fundamental question that underlies all of statistical inference."
  },
  {
    "objectID": "estimating-mean#section-2",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Question",
    "text": "How do we know that the sample mean is a reliable estimator of the population mean? Could there be systematic error in our estimates?"
  },
  {
    "objectID": "estimating-mean#section-3",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Answer",
    "text": "The sample mean is unbiased , meaning that if we could take infinitely many samples and calculate the mean for each one, the average of all those sample means would exactly equal the population mean. This property holds regardless of sample size or the shape of the population distribution—it’s assumption-free except for requiring that the population mean is finite. Understanding Unbiasedness An estimator is unbiased if its expected value equals the parameter it’s trying to estimate. For the sample mean \\bar{Y} estimating the population mean \\mu , we want to show: E[\\bar{Y}] = \\mu This is a powerful property because it guarantees that our estimator has no systematic tendency to overestimate or underestimate the true parameter. Some samples will give us values above \\mu , others below, but on average—across infinitely many samples—we hit the target exactly. The Proof The proof is remarkably elegant. Let’s work through it step by step. We start with the definition of the sample mean: \\bar{Y} = \\frac{Y_1 + Y_2 + \\cdots + Y_n}{n} Taking the expected value of both sides: E[\\bar{Y}] = E\\left[\\frac{Y_1 + Y_2 + \\cdots + Y_n}{n}\\right] Since n is a constant (our fixed sample size), we can factor it out using the linearity of expectation: E[\\bar{Y}] = \\frac{1}{n} E[Y_1 + Y_2 + \\cdots + Y_n] Using the linearity property again, the expectation of a sum equals the sum of expectations: E[\\bar{Y}] = \\frac{1}{n} \\left(E[Y_1] + E[Y_2] + \\cdots + E[Y_n]\\right) Now comes the key insight. Each observation Y_i is drawn from the same population, so each has the same expected value—the population mean \\mu . Think about it this way: if you could observe infinitely many “first observations” from different samples, their distribution would look exactly like the population distribution, and their mean would be \\mu . The same holds for the second observation, the third, and all others. Therefore: E[\\bar{Y}] = \\frac{1}{n}(\\mu + \\mu + \\cdots + \\mu) = \\frac{1}{n}(n\\mu) = \\mu The proof is complete."
  },
  {
    "objectID": "estimating-mean#section-4",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Important: Assumption-Free Result",
    "text": "This proof makes no assumptions about: The sample size n (it can be as small as 2) The distribution of the population (it can be skewed, multimodal, or anything) The variance of the population (it doesn’t even need to exist) The only requirement is that \\mu is finite. There are some exotic distributions (like the Cauchy distribution) whose mean is undefined, but for all practical purposes, if the population has a finite mean, the sample mean is an unbiased estimator of it."
  },
  {
    "objectID": "estimating-mean#the-efficiency-of-the-sample-mean",
    "href": "/chapter/estimating-mean#the-efficiency-of-the-sample-mean",
    "title": "Estimating the population mean",
    "section": "The Efficiency of the Sample Mean",
    "text": "Proving unbiasedness is just the first step. After all, there are infinitely many unbiased estimators of the population mean. We need another criterion to choose among them."
  },
  {
    "objectID": "estimating-mean#section-6",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Question",
    "text": "If there are infinitely many unbiased estimators, how do we decide which one to use? What makes the sample mean special?"
  },
  {
    "objectID": "estimating-mean#section-7",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Answer",
    "text": "Among all unbiased estimators, the sample mean has the smallest variance —a property called efficiency. This means it gives us the most precise estimates on average. While other unbiased estimators might occasionally give better results in particular samples, the sample mean is most reliable in the long run. The Problem of Too Many Estimators Consider this: the first observation Y_1 by itself is an unbiased estimator of \\mu since E[Y_1] = \\mu . So is the second observation. So is any weighted average of your observations, as long as the weights sum to one. For example: T = \\frac{1}{4}Y_1 + \\frac{3}{4}Y_2 This is unbiased (you can verify that E[T] = \\mu ). But it completely ignores observations 3 through n if you have more data! Intuitively, this seems wasteful. We need a way to filter these infinitely many unbiased estimators. The Efficiency Criterion We use efficiency as our second filter. An efficient estimator is one that has the minimum variance among all unbiased estimators. Why variance? Because variance measures precision—how much our estimates vary from sample to sample. Lower variance means more consistent, reliable estimates. Setting Up the Proof Let’s define a general unbiased estimator as a weighted combination of our observations: T = \\sum_{i=1}^{n} A_i Y_i where the A_i are arbitrary weights (constants). Since we’re restricting attention to unbiased estimators, we require: E[T] = \\mu Let’s see what this constraint implies. Taking expectations: E[T] = E\\left[\\sum_{i=1}^{n} A_i Y_i\\right] = \\sum_{i=1}^{n} A_i E[Y_i] = \\sum_{i=1}^{n} A_i \\mu = \\mu \\sum_{i=1}^{n} A_i For this to equal \\mu , we need: \\sum_{i=1}^{n} A_i = 1 This is our first constraint: the weights must sum to one. The sample mean satisfies this with A_i = 1/n for all i . Finding the Variance Now let’s calculate the variance of our general estimator T : \\text{Var}(T) = \\text{Var}\\left(\\sum_{i=1}^{n} A_i Y_i\\right) Since the A_i are constants and the observations are independent: \\text{Var}(T"
  },
  {
    "objectID": "estimating-mean#section-8",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "The Meaning of Efficiency",
    "text": "The sample mean is efficient because it makes optimal use of all available information. Giving equal weight to each observation minimizes the variance of our estimate. Any other weighting scheme—whether it’s emphasizing early observations, ignoring some data, or using unequal weights—will produce a less precise estimator. This result has a beautiful interpretation: democracy in data is optimal. No observation deserves more weight than any other when they’re all drawn from the same population."
  },
  {
    "objectID": "estimating-mean#the-bias-of-the-sample-variance",
    "href": "/chapter/estimating-mean#the-bias-of-the-sample-variance",
    "title": "Estimating the population mean",
    "section": "The Bias of the Sample Variance",
    "text": "Having established the virtues of the sample mean, we now turn to a more subtle problem: estimating the population variance \\sigma^2 ."
  },
  {
    "objectID": "estimating-mean#section-10",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Question",
    "text": "The natural estimator of population variance would seem to be the average squared deviation from the sample mean: \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 . Why do we use n-1 instead of n in the denominator? Is this just a convention, or is there a deeper reason?"
  },
  {
    "objectID": "estimating-mean#section-11",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Answer",
    "text": "The formula with n in the denominator is actually biased —it systematically underestimates the true population variance. Using n-1 corrects this bias, giving us an unbiased estimator. This isn’t arbitrary; it follows from a careful mathematical analysis of how sample statistics relate to population parameters. Defining the Sample Variance Let’s define what we’ll call the “uncorrected” sample variance: S^2 = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 This is the natural definition—it’s the average squared deviation from the sample mean. But is it unbiased? To answer this, we need to calculate E[S^2] and see if it equals \\sigma^2 . A Clever Algebraic Manipulation The key to this proof is recognizing that we can rewrite each deviation (Y_i - \\bar{Y}) in terms of deviations from the true population mean \\mu : Y_i - \\bar{Y} = (Y_i - \\mu) - (\\bar{Y} - \\mu) This is just adding and subtracting \\mu . Now let’s square both sides: (Y_i - \\bar{Y})^2 = [(Y_i - \\mu) - (\\bar{Y} - \\mu)]^2 Expanding the square: (Y_i - \\bar{Y})^2 = (Y_i - \\mu)^2 - 2(Y_i - \\mu)(\\bar{Y} - \\mu) + (\\bar{Y} - \\mu)^2 Summing Over All Observations Now sum both sides from i=1 to n : \\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 = \\sum_{i=1}^{n}(Y_i - \\mu)^2 - 2(\\bar{Y} - \\mu)\\sum_{i=1}^{n}(Y_i - \\mu) + \\sum_{i=1}^{n}(\\bar{Y} - \\mu)^2 Let’s examine each term carefully. For the middle term, notice that: \\sum_{i=1}^{n}(Y_i - \\mu) = \\sum_{i=1}^{n}Y_i - n\\mu = n\\bar{Y} - n\\mu = n(\\bar{Y} - \\mu) So the middle term becomes: -2(\\bar{Y} - \\mu) \\cdot n(\\bar{Y} - \\mu) = -2n(\\bar{Y} - \\mu)^2 For the last term, (\\bar{Y} - \\mu)^2 doesn’t depend on i , so: \\sum_{i=1}^{n}(\\bar{Y} - \\mu)^2 = n(\\bar{Y} - \\mu)^2 Putting it all together: \\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 = \\sum_{i=1}^{n}(Y_i - \\mu)^2 - 2n(\\bar{Y} - \\mu)^2 + n(\\bar{Y} - \\mu)^2 = \\sum_{i=1}^{n}(Y_i - \\mu)^2 - n(\\bar{Y} - \\mu)^2 Therefore, dividing both sides by n : S^2 = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\mu)^2 - (\\bar{Y} - \\mu)^2 Taking Expectations Now we take the expected value"
  },
  {
    "objectID": "estimating-mean#section-12",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Why the Bias Occurs",
    "text": "The bias arises because we use \\bar{Y} instead of \\mu in our formula. The sample mean is calculated from the same data we’re using to measure variation, so it’s “closer” to the data points than the true mean would be. The deviations (Y_i - \\bar{Y}) are systematically smaller than the deviations (Y_i - \\mu) would be, leading to underestimation. Using n-1 in the denominator exactly corrects for this bias. The unbiased sample variance is: s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 since E[s^2] = \\frac{n}{n-1} \\cdot E[S^2] = \\frac{n}{n-1} \\cdot \\sigma^2 \\cdot \\frac{n-1}{n} = \\sigma^2 ."
  },
  {
    "objectID": "estimating-mean#a-challenge-for-you",
    "href": "/chapter/estimating-mean#a-challenge-for-you",
    "title": "Estimating the population mean",
    "section": "A Challenge for You",
    "text": "Now that we’ve proven the sample variance with n in the denominator is biased, here’s a thought question:"
  },
  {
    "objectID": "estimating-mean#section-14",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Challenge Question",
    "text": "We know that S^2 = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2 is a biased estimator of \\sigma^2 , with E[S^2] = \\frac{n-1}{n}\\sigma^2 . Can you propose an unbiased estimator of the population variance? Don’t look it up—use what we’ve learned in this chapter to construct one yourself. Hint : If you know the expected value of S^2 , what simple transformation would make it unbiased?"
  },
  {
    "objectID": "estimating-mean#synthesis-and-reflection",
    "href": "/chapter/estimating-mean#synthesis-and-reflection",
    "title": "Estimating the population mean",
    "section": "Synthesis and Reflection",
    "text": "Let’s step back and consider what these three proofs reveal about the nature of statistical estimation. The sample mean emerged as the gold standard estimator not by accident, but because it possesses two fundamental virtues: it’s unbiased (correct on average) and efficient (most precise). These aren’t just mathematical curiosities—they have practical implications. When you calculate a sample mean, you can trust that you’re using the best possible estimator given your data. The sample variance case is more subtle. The natural estimator S^2 turns out to be biased, but the bias is systematic and predictable, allowing us to correct it. This illustrates an important principle: not all biases are equal. A systematic, known bias that we can correct is far less problematic than an unknown or random error. Moreover, these proofs showcase the power of mathematical statistics. We’re not guessing or using intuition—we’re proving with logical certainty that our estimators have desirable properties. This rigor is what allows statistics to be a reliable tool for scientific inference."
  },
  {
    "objectID": "estimating-mean#section-16",
    "href": "/chapter/estimating-mean",
    "title": "Estimating the population mean",
    "section": "Key Takeaways",
    "text": "The sample mean is unbiased : E[\\bar{Y}] = \\mu , regardless of sample size or population distribution (as long as \\mu is finite). The sample mean is efficient : Among all unbiased estimators, \\bar{Y} has the minimum variance. Equal weighting is optimal. The sample variance needs correction : The natural estimator \\frac{1}{n}\\sum(Y_i - \\bar{Y})^2 underestimates \\sigma^2 by a factor of (n-1)/n . Dividing by n-1 instead of n corrects this bias. Degrees of freedom matter : The n-1 denominator reflects that we “lose” one degree of freedom by using \\bar{Y} instead of \\mu . These results form the foundation for much of what follows in statistical inference. Understanding why they’re true—not just memorizing the formulas—will serve you well as we build toward more sophisticated techniques."
  },
  {
    "objectID": "estimating-variance",
    "href": "/chapter/estimating-variance",
    "title": "Estimating the population variance",
    "section": "",
    "text": "Estimating the population variance This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "foundations-frequentist",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "",
    "text": "Foundations of Frequentist Statistics In this chapter, we embark on a journey into the heart of frequentist statistical inference—a framework that dominates modern empirical research. At its core, frequentist statistics is about making observations from a sample and then drawing inferences about the broader population from which that sample was drawn. The fundamental question we seek to answer is: How confident can we be that the patterns we observe in our limited sample reflect true patterns in the population? By the end of this chapter, you will understand the foundational concepts that underpin frequentist inference, including the philosophy of repeated sampling, the nature of estimators, and the mathematical criteria we use to distinguish good estimators from poor ones."
  },
  {
    "objectID": "foundations-frequentist#section-2",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "What Are We Really Doing?",
    "text": "Inferential statistics is fundamentally about making observations in sample data and then attempting to extrapolate causal connections or patterns to the population data . When we successfully extrapolate these connections, we say our results are statistically significant . When we cannot extrapolate with confidence, we say our results are not statistically significant . This distinction—between what we observe in our sample and what we can confidently claim about the population—lies at the heart of all inferential statistics. But what exactly are we making claims about when we talk about populations? Population Parameters vs. Sample Statistics When we make claims about a population, we are not making claims about individual observations. After all, populations are conceptually infinite in size. Instead, we make claims about specific parameters of the population’s distribution. The two parameters we encounter most frequently are: The population mean ( \\mu ): This is by far the most common parameter we test hypotheses about in applied statistics. The population variance ( \\sigma^2 ): This parameter is crucial because tests for the population mean often depend on our ability to estimate the population variance. Because we never truly know the values of \\mu or \\sigma^2 , we must estimate them using sample data. The corresponding quantities we calculate from our sample are: Sample mean ( \\bar{y} ): The analog to the population mean Sample variance ( s^2 ): The analog to the population variance"
  },
  {
    "objectID": "foundations-frequentist#section-3",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "A Critical Distinction",
    "text": "When we call the sample mean and sample variance “analogs” or “counterparts” to their population equivalents, we mean only that they correspond conceptually. We are not claiming they are equal or even necessarily good estimates. Establishing which sample statistics make good estimators of population parameters is precisely what this chapter is about."
  },
  {
    "objectID": "foundations-frequentist#transformations-of-random-variables",
    "href": "/chapter/foundations-frequentist#transformations-of-random-variables",
    "title": "Foundations of Frequentist Statistics",
    "section": "Transformations of Random Variables",
    "text": "Before we dive into the philosophy of estimation, we need to develop some mathematical machinery. In statistics, we routinely transform data—we take numbers, apply formulas to them, and generate new numbers. Understanding how these transformations affect the mean and variance of our data is essential. Affine Transformations Consider a simple but powerful type of transformation called an affine transformation . If we have a random variable X with mean \\bar{x} and variance s^2 , we might create a new variable: Y = mX + c where m is a multiplicative constant (the slope) and c is an additive constant (the intercept). This is exactly the form of a linear equation you’ve seen since high school algebra. The question is: if we know the mean and variance of X , what are the mean and variance of Y ? We can decompose this affine transformation into two simpler operations: Translation : X \\rightarrow X + c (adding a constant) Linear transformation : X \\rightarrow mX (multiplying by a constant) Properties of Translation When you add a constant c to every value in your dataset, creating Y = X + c : \\begin{aligned} \\text{Mean of } Y &= \\bar{x} + c \\\\ \\text{Variance of } Y &= s^2 \\end{aligned} The mean shifts by exactly c , but the variance remains unchanged. Why? Because variance measures the spread of data around the mean, and when you shift all values by the same amount, their relative positions don’t change."
  },
  {
    "objectID": "foundations-frequentist#section-5",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Connecting to Earlier Concepts",
    "text": "You’ve already encountered this idea when we discussed the z -transformation. When we subtract the mean from a variable, we’re performing a translation that shifts the entire distribution to have mean zero. The shape and spread of the distribution remain the same. Properties of Linear Transformation When you multiply every value by a constant m , creating Y = mX : \\begin{aligned} \\text{Mean of } Y &= m\\bar{x} \\\\ \\text{Variance of } Y &= m^2 s^2 \\\\ \\text{Standard deviation of } Y &= |m| s \\end{aligned} Notice that the variance is multiplied by m^2 , not m . This occurs because variance involves squared deviations, so a multiplicative constant gets squared in the process. Combining Both Transformations For the full affine transformation Y = mX + c : \\begin{aligned} \\text{Mean of } Y &= m\\bar{x} + c \\\\ \\text{Variance of } Y &= m^2 s^2 \\\\ \\text{Standard deviation of } Y &= |m| s \\end{aligned} These formulas will prove invaluable as we develop more sophisticated statistical techniques."
  },
  {
    "objectID": "foundations-frequentist#the-frequentist-philosophy-repeated-sampling",
    "href": "/chapter/foundations-frequentist#the-frequentist-philosophy-repeated-sampling",
    "title": "Foundations of Frequentist Statistics",
    "section": "The Frequentist Philosophy: Repeated Sampling",
    "text": "We now arrive at the conceptual heart of frequentist statistics. The entire edifice of frequentist inference rests on an imaginary exercise: repeated sampling ."
  },
  {
    "objectID": "foundations-frequentist#section-7",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "The Thought Experiment",
    "text": "Imagine that we could: Draw a random sample from the population Calculate some statistic from that sample Return the sample to the population Draw another random sample Calculate the statistic again Repeat this process infinitely many times This thought experiment—sampling repeatedly from the same population—forms the foundation for how we evaluate estimators in frequentist statistics. Here’s the crucial point: in practice, we only sample once . But theoretically, we imagine what would happen if we could sample infinitely many times. The behavior of our estimator across these hypothetical repeated samples tells us whether it’s a good estimator or not. The Concept of an Estimator An estimator is simply a formula that we apply to sample data to estimate a population parameter. Importantly, there are infinitely many possible estimators for any given parameter. For example, suppose we want to estimate the population mean \\mu . Here are just a few of the infinitely many estimators we could choose: The first observation: \\hat{\\mu}_1 = y_1 The sum of the first two observations: \\hat{\\mu}_2 = y_1 + y_2 The cube of the first observation: \\hat{\\mu}_3 = y_1^3 The fourth power of the seventh observation times the sine of the second: \\hat{\\mu}_4 = y_7^4 \\times \\sin(y_2) The sample mean: \\hat{\\mu}_5 = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i Most of these are obviously terrible estimators. But the point is that we can construct any formula we want, and each formula defines a different estimator. The set of all possible estimators is infinite. So how do we choose among them? How do we determine which estimators are “good” and which are “bad”? The answer lies in examining the sampling distribution of each estimator. Sampling Distributions For any estimator, we can imagine the repeated sampling process: Draw a sample of size n Apply the estimator to get an estimate Record that estimate Repeat infinitely many times The distribution of all these estimates is called the sampling distrib"
  },
  {
    "objectID": "foundations-frequentist#section-8",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Key Insight",
    "text": "The sampling distribution is a theoretical construct. We never actually observe it because we only sample once in practice. But by imagining what it would look like, we can develop mathematical criteria for judging the quality of different estimators."
  },
  {
    "objectID": "foundations-frequentist#a-concrete-example-estimating-from-a-simple-population",
    "href": "/chapter/foundations-frequentist#a-concrete-example-estimating-from-a-simple-population",
    "title": "Foundations of Frequentist Statistics",
    "section": "A Concrete Example: Estimating from a Simple Population",
    "text": "To make these abstract ideas concrete, let’s work through a simple example. Consider a population with only three values: \\{1, 2, 3\\} . Since there’s one of each value, each has probability 1/3 of being selected if we draw randomly from this population. The True Population Parameters This is a discrete uniform distribution, and we can easily calculate the true population mean and variance: \\mu = E[Y] = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = 2 For the variance, we first calculate the expected value of Y^2 : E[Y^2] = 1^2 \\cdot \\frac{1}{3} + 2^2 \\cdot \\frac{1}{3} + 3^2 \\cdot \\frac{1}{3} = \\frac{14}{3} Then, using the formula \\text{Var}(Y) = E[Y^2] - (E[Y])^2 : \\sigma^2 = \\frac{14}{3} - 2^2 = \\frac{14}{3} - 4 = \\frac{2}{3} We can also verify this directly by calculating the squared deviations: Value (y) Deviation (y - \\mu) Squared Deviation (y - \\mu)^2 1 -1 1 2 0 0 3 1 1 \\sigma^2 = \\frac{1 + 0 + 1}{3} = \\frac{2}{3} So we know that \\mu = 2 and \\sigma^2 = 2/3 . The Frequentist Approach: Pretending We Don’t Know Now comes the frequentist thought experiment. Imagine we don’t know these population parameters. Our goal is to estimate \\mu using only sample data. We’ll draw samples of size n = 2 with replacement. Sampling with replacement means: Draw an observation and record its value Return it to the population Draw a second observation This ensures that our two observations are independent—the value of the second observation doesn’t depend on the first."
  },
  {
    "objectID": "foundations-frequentist#section-10",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Why Sample With Replacement?",
    "text": "In real research, we don’t literally sample with replacement. But we make the assumption that our samples are small enough relative to the population that each observation is effectively independent of the others. Since populations are conceptually infinite, even a sample of 5,000 is negligible compared to the population size, justifying the independence assumption. Enumerating All Possible Samples With a population of size 3 and sample size 2, how many possible samples can we draw (with replacement, where order matters)? First observation: 3 possibilities Second observation: 3 possibilities Total: 3 \\times 3 = 9 possible samples Here are all nine possible samples and their means: Sample Values Sample Mean \\bar{y} 1 (1, 1) 1.0 2 (1, 2) 1.5 3 (1, 3) 2.0 4 (2, 1) 1.5 5 (2, 2) 2.0 6 (2, 3) 2.5 7 (3, 1) 2.0 8 (3, 2) 2.5 9 (3, 3) 3.0 The Sampling Distribution of the Sample Mean Now we can construct the sampling distribution of \\bar{y} . We have nine equally likely samples, so each sample mean has probability 1/9 : Sample Mean \\bar{y} Frequency Probability 1.0 1 1/9 1.5 2 2/9 2.0 3 3/9 2.5 2 2/9 3.0 1 1/9 The expected value of this sampling distribution is: E[\\bar{y}] = 1.0 \\cdot \\frac{1}{9} + 1.5 \\cdot \\frac{2}{9} + 2.0 \\cdot \\frac{3}{9} + 2.5 \\cdot \\frac{2}{9} + 3.0 \\cdot \\frac{1}{9} = 2 This equals the true population mean! This is our first glimpse of an important property: unbiasedness ."
  },
  {
    "objectID": "foundations-frequentist#properties-of-good-estimators",
    "href": "/chapter/foundations-frequentist#properties-of-good-estimators",
    "title": "Foundations of Frequentist Statistics",
    "section": "Properties of Good Estimators",
    "text": "We began with infinitely many possible estimators. To choose among them, we need criteria for what makes an estimator “good.” Statisticians have identified many desirable properties, but we’ll focus on three fundamental ones: Unbiasedness Efficiency Consistency Property 1: Unbiasedness An estimator is unbiased if the expected value of its sampling distribution equals the true population parameter being estimated."
  },
  {
    "objectID": "foundations-frequentist#section-12",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Definition: Unbiased Estimator",
    "text": "An estimator \\hat{\\theta} of a population parameter \\theta is unbiased if: E[\\hat{\\theta}] = \\theta In words: if we could sample repeatedly and average all our estimates, we would get the correct answer. For the sample mean as an estimator of the population mean: E[\\bar{y}] = \\mu We can prove this generally. For a sample y_1, y_2, \\ldots, y_n drawn from a population with mean \\mu : \\begin{aligned} E[\\bar{y}] &= E\\left[\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} E[y_i] \\\\ &= \\frac{1}{n}\\sum_{i=1}^{n} \\mu \\\\ &= \\frac{1}{n} \\cdot n\\mu \\\\ &= \\mu \\end{aligned} The sample mean is an unbiased estimator of the population mean. It gives us the correct answer “on average” across repeated samples."
  },
  {
    "objectID": "foundations-frequentist#section-13",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "A Crucial Point",
    "text": "We can prove unbiasedness without knowing the actual value of \\mu . We only need to know that the population has a well-defined mean. This is the power of mathematical proof—we can establish properties of estimators without knowing the specific parameters we’re estimating. But here’s the challenge: infinitely many estimators are unbiased . Applying the filter of unbiasedness still leaves us with infinitely many candidates. We need additional criteria. Property 2: Efficiency Among all unbiased estimators, we prefer the one with the smallest variance in its sampling distribution."
  },
  {
    "objectID": "foundations-frequentist#section-14",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Definition: Efficiency",
    "text": "Among unbiased estimators, the most efficient estimator is the one with the smallest variance in its sampling distribution. An estimator with smaller variance produces estimates in a narrower band around the true parameter value. Why do we care about efficiency? If two estimators are both unbiased (correct on average), but one has a tighter sampling distribution, we have more confidence in estimates from the tighter distribution. Each individual estimate is more likely to be close to the true parameter value. Imagine two unbiased estimators, A and B: Estimator A: E[\\hat{\\mu}_A] = \\mu and \\text{Var}(\\hat{\\mu}_A) = 0.5 Estimator B: E[\\hat{\\mu}_B] = \\mu and \\text{Var}(\\hat{\\mu}_B) = 2.0 Both are unbiased, but A is more efficient. Estimates from A will cluster more tightly around \\mu than estimates from B. The remarkable result : Among all unbiased estimators of the population mean, the sample mean has the smallest variance. It is the most efficient unbiased estimator."
  },
  {
    "objectID": "foundations-frequentist#section-15",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Why We Use Sample Means",
    "text": "This is why we routinely use sample averages to estimate population averages. It’s not arbitrary—it’s mathematically optimal. The sample mean is both unbiased and most efficient among unbiased estimators. Variance of the Sample Mean For a sample of size n drawn from a population with variance \\sigma^2 , the variance of the sampling distribution of \\bar{y} is: \\text{Var}(\\bar{y}) = \\frac{\\sigma^2}{n} We can derive this using our transformation rules: \\begin{aligned} \\text{Var}(\\bar{y}) &= \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) \\\\ &= \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} y_i\\right) \\\\ &= \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(y_i) \\quad \\text{(assuming independence)} \\\\ &= \\frac{1}{n^2} \\cdot n\\sigma^2 \\\\ &= \\frac{\\sigma^2}{n} \\end{aligned} This formula will become crucial in our next property. Property 3: Consistency Both unbiasedness and efficiency are properties that hold for a fixed sample size n . Our third property asks: what happens as n increases?"
  },
  {
    "objectID": "foundations-frequentist#section-16",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Definition: Consistency",
    "text": "An estimator is consistent if its sampling distribution becomes increasingly concentrated around the true parameter value as the sample size increases. In the limit as n \\rightarrow \\infty , the distribution collapses to a point at the true parameter. This property is formalized by the Law of Large Numbers , which states that as the sample size grows, the sample mean converges to the population mean. Looking at our variance formula: \\text{Var}(\\bar{y}) = \\frac{\\sigma^2}{n} As n increases, the variance decreases. As n \\rightarrow \\infty : \\lim_{n \\rightarrow \\infty} \\text{Var}(\\bar{y}) = \\lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} = 0 The distribution collapses to a single point. When our sample size equals the entire population, every sample mean equals the population mean exactly. The sample mean is a consistent estimator."
  },
  {
    "objectID": "foundations-frequentist#section-17",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Interpreting Consistency",
    "text": "Consistency gives us confidence that more data leads to better estimates (assuming the data are properly collected). This is the mathematical justification for why researchers pursue larger sample sizes—not just for statistical power, but because larger samples yield more precise estimates."
  },
  {
    "objectID": "foundations-frequentist#summary-why-the-sample-mean",
    "href": "/chapter/foundations-frequentist#summary-why-the-sample-mean",
    "title": "Foundations of Frequentist Statistics",
    "section": "Summary: Why the Sample Mean?",
    "text": "We can now answer definitively why we use the sample mean to estimate the population mean: Unbiasedness : The sample mean is correct on average across repeated samples Efficiency : Among all unbiased estimators, it has the smallest variance Consistency : As sample size increases, the sample mean converges to the population mean These three properties—proven mathematically, not assumed—make the sample mean the optimal choice for estimating population means in the frequentist framework."
  },
  {
    "objectID": "foundations-frequentist#section-20",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Question 1",
    "text": "Explain in your own words why we can establish that the sample mean is unbiased without knowing the actual value of \\mu . What does this tell us about the power of mathematical reasoning in statistics?"
  },
  {
    "objectID": "foundations-frequentist#section-21",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Question 2",
    "text": "Consider two researchers studying the same population. Researcher A collects a sample of size 50, while Researcher B collects a sample of size 200. Both use the sample mean as their estimator. How do their sampling distributions differ? Which researcher should have more confidence in their estimate, and why?"
  },
  {
    "objectID": "foundations-frequentist#section-22",
    "href": "/chapter/foundations-frequentist",
    "title": "Foundations of Frequentist Statistics",
    "section": "Question 3",
    "text": "The frequentist approach relies on imagining repeated sampling, even though we only sample once in practice. Does this make the approach purely theoretical, or does it provide practical value for real-world inference? Defend your answer."
  },
  {
    "objectID": "foundations-frequentist#looking-ahead",
    "href": "/chapter/foundations-frequentist#looking-ahead",
    "title": "Foundations of Frequentist Statistics",
    "section": "Looking Ahead",
    "text": "In this chapter, we’ve established the fundamental philosophy of frequentist statistics and proven that the sample mean is an optimal estimator of the population mean. In the next chapter, we’ll turn our attention to estimating the population variance and develop the tools necessary for hypothesis testing—the framework that allows us to make formal claims about statistical significance. The journey from these theoretical foundations to practical hypothesis testing may seem long, but every step is essential. Statistical inference is not a collection of arbitrary formulas—it is a coherent mathematical framework built on rigorous principles. Understanding these principles will make you a more thoughtful and capable analyst."
  },
  {
    "objectID": "graphing",
    "href": "/chapter/graphing",
    "title": "Graphing",
    "section": "",
    "text": "Graphing This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "intro-data-analytics",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "",
    "text": "The Purpose of Data Analytics In this chapter, we’ll explore the fundamental purpose and scope of data analytics. By the end of this chapter, you will understand: The distinction between correlation and causation How patterns emerge from randomness The difference between population and sample data The two primary goals of statistical analysis The philosophical divide between frequentist and Bayesian approaches"
  },
  {
    "objectID": "intro-data-analytics#section-2",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Key Question",
    "text": "What is the ultimate goal of data analytics? Data analytics is fundamentally about understanding cause and effect relationships in the world. While it’s easy to observe that two variables move together—that they are correlated—establishing causation is far more challenging and far more valuable. Consider a simple example: we might observe that ice cream sales and drowning incidents are correlated. They both increase during summer months. But does ice cream cause drowning? Of course not. Both are caused by a third factor: warm weather, which leads people to buy ice cream and also to swim more frequently."
  },
  {
    "objectID": "intro-data-analytics#section-3",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Important Distinction",
    "text": "Correlation does not imply causation. Two variables can move together without one causing the other. Establishing causal relationships requires careful analysis and often experimental design. The distinction between correlation and causation is not merely academic—it has profound implications for how we understand the world and make decisions. Consider the famous closing lines of Robert Frost’s poem “The Road Not Taken”: Two roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference. Frost claims that taking the road less traveled “made all the difference” to his life. But as statisticians, we must ask: how does he know? To establish causation, we would need a counterfactual —an alternative version of his life where he took the other road. Without observing this counterfactual, Frost cannot definitively claim that his choice caused the difference in his life’s trajectory. Perhaps his life would have turned out similarly regardless of which road he chose. Or perhaps taking the more traveled road would have led to even better outcomes. This challenge—the impossibility of observing counterfactuals in our own lives—is precisely what makes causal inference so difficult and why rigorous statistical methods are essential. In policy work—especially environmental policy and climate science—we need causal understanding. When we ask “how much warming will occur if we add X more tons of carbon dioxide to the atmosphere?”, we’re asking a causal question. The relationship between greenhouse gas concentrations and temperature change is incredibly complicated, random, and stochastic. Yet climate scientists have developed good estimates of what is called the global warming potential of different greenhouse gases. These estimates are based on a causal understanding of physical processes, not mere correlation. This is why data analytics matters: we want to establish cause and effect, not just observe patterns. We’re here to understand how th"
  },
  {
    "objectID": "intro-data-analytics#from-randomness-to-pattern",
    "href": "/chapter/intro-data-analytics#from-randomness-to-pattern",
    "title": "The Purpose of Data Analytics",
    "section": "From Randomness to Pattern",
    "text": "One of the most remarkable features of statistical analysis is how patterns emerge from what initially appears to be pure randomness. When we look at individual observations, they often seem chaotic and unpredictable. But when we collect enough observations, macro-level patterns begin to reveal themselves."
  },
  {
    "objectID": "intro-data-analytics#section-5",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Conceptual Question",
    "text": "How can predictable patterns emerge from random individual events?"
  },
  {
    "objectID": "intro-data-analytics#section-6",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "While individual events may be unpredictable, the aggregate behavior of many random events often follows predictable patterns. This is the fundamental insight of probability theory—that randomness at the micro level produces regularity at the macro level. Consider the classic example of a Galton board (sometimes called a bean machine). When a single ball drops through the board, hitting pegs as it falls, its path is essentially random—at each peg, it bounces left or right unpredictably. We cannot predict where any individual ball will land. However, when we drop hundreds or thousands of balls, a clear pattern emerges: they pile up in the shape of a bell curve, forming what statisticians call the normal distribution . The randomness of individual ball drops gives way to a predictable aggregate pattern. This emergence of order from randomness is not magic—it’s mathematics. And it’s the foundation of statistical inference."
  },
  {
    "objectID": "intro-data-analytics#section-8",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Beware of Normalitis",
    "text": "One common misconception in statistics is that every pattern follows the normal distribution (the familiar bell curve). This is simply not true. While the normal distribution is important and widely applicable, it is just one of dozens of probability distributions used in statistics. I call the mistaken belief that everything is normally distributed normalitis —and it’s a condition to avoid. Different real-world phenomena follow different distributions: Bernoulli distribution : Events with only two possible outcomes (coin flip: heads or tails; ball at a peg: left or right) Binomial distribution : The number of successes in a fixed number of independent Bernoulli trials (how many heads in 10 coin flips?) Poisson distribution : Count data and waiting times (how long you wait for the bus each day; how many customers arrive per hour) Normal distribution : Many continuous phenomena in nature and society (heights, test scores, measurement errors) These distributions are often mathematically related. For instance, when you sum up many independent Bernoulli trials (each ball on the Galton board making left-right decisions), you get a binomial distribution. And when the number of trials becomes very large, that binomial distribution approximates the normal distribution."
  },
  {
    "objectID": "intro-data-analytics#section-9",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Conceptual Question",
    "text": "The word “Poisson” comes from French. What does it mean, and who was Poisson?"
  },
  {
    "objectID": "intro-data-analytics#section-10",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "“Poisson” means “fish” in French (related to “Pisces,” the astrological sign). Siméon Denis Poisson was a French mathematician and physicist who discovered this particular distribution, which describes the probability of a given number of events occurring in a fixed interval of time or space. Throughout this course, we’ll work with many different distributions. Each captures a different kind of pattern in data. The key is learning to recognize which pattern fits which situation—and to never assume that one pattern applies universally."
  },
  {
    "objectID": "intro-data-analytics#population-and-sample",
    "href": "/chapter/intro-data-analytics#population-and-sample",
    "title": "The Purpose of Data Analytics",
    "section": "Population and Sample",
    "text": "In statistical analysis, we make a crucial distinction between two types of data:"
  },
  {
    "objectID": "intro-data-analytics#section-12",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Definitions",
    "text": "Population : All possible data points that exist in the world for a given phenomenon. This includes data that has been collected, data that could be collected, and data that will exist in the future. Sample : A subset of the population that we have actually collected and can analyze. The sample is always smaller—often infinitesimally smaller—than the population. Consider studying human height. The population would include the heights of all humans who have ever lived, are living now, and will live in the future. That’s an enormous—indeed, infinite—amount of data. Your sample might be the heights of 1,000 people surveyed in a particular city during a particular year. No matter how large your sample, it remains tiny compared to the population. Even if you collect data on millions of individuals, that’s still just a tiny fraction of the theoretical population. As a mathematical principle: \\lim_{n \\to \\infty} \\text{Sample} = \\text{Population} As the sample size approaches infinity, it approaches the population. But in practice, our samples are always finite and small relative to the population."
  },
  {
    "objectID": "intro-data-analytics#two-goals-of-statistical-analysis",
    "href": "/chapter/intro-data-analytics#two-goals-of-statistical-analysis",
    "title": "The Purpose of Data Analytics",
    "section": "Two Goals of Statistical Analysis",
    "text": "What do we do with sample data once we collect it? We pursue one or both of two fundamental goals: 1. Description The first goal is to describe the data we have collected. This is called descriptive statistics . We might: Calculate the average (mean) age in our sample Determine the most common (mode) educational level Find the middle value (median) of family incomes Measure the spread (variance or standard deviation) of environmental commitment scores Descriptive statistics summarize and organize data in meaningful ways. They help us understand what our sample looks like. When we describe sample data, we’re making statements only about that specific set of observations. 2. Inference The second, more ambitious goal is to infer patterns and relationships that extend beyond our sample to the broader population. This is called inferential statistics or statistical inference . Suppose we collect sample data on 25 different variables for each person: age, education level, commitment to environmental causes, family income, transportation choices, and so on. We might discover relationships among these variables in our sample—for instance, that people with higher education levels tend to show stronger commitment to environmental causes. The question then becomes: Can we extrapolate this relationship from our tiny sample to the entire population? Can we say with confidence that the relationship we found in this specific dataset also exists more broadly? This is the central challenge of inferential statistics. We observe patterns in our sample and attempt to make general claims about the population. The entire machinery of statistical inference—hypothesis tests, confidence intervals, p-values, regression analysis—exists to help us make this logical leap from sample to population in a rigorous, quantifiable way."
  },
  {
    "objectID": "intro-data-analytics#section-14",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Reflective Question",
    "text": "Why is it more valuable to make inferences about the population than to simply describe our sample?"
  },
  {
    "objectID": "intro-data-analytics#section-15",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Answer",
    "text": "Describing our sample tells us only about the specific observations we happened to collect. But policy decisions, scientific theories, and practical applications require understanding that extends beyond our particular sample. We need to know whether the patterns we observe are likely to hold generally, not just in the specific cases we studied. This is what makes statistical inference so powerful and so essential for decision-making. When we perform inference successfully—when we can say with justified confidence that our sample findings reflect population patterns—we achieve what statisticians call external validity . But before we can even attempt to generalize to the population, we must first ensure that our findings within the sample are sound. When our causal analysis within the sample is properly conducted and the relationships we identify are genuine (not artifacts of confounding variables or measurement error), we say our analysis has internal validity . Both forms of validity are essential for credible statistical work."
  },
  {
    "objectID": "intro-data-analytics#two-philosophical-approaches-to-inference",
    "href": "/chapter/intro-data-analytics#two-philosophical-approaches-to-inference",
    "title": "The Purpose of Data Analytics",
    "section": "Two Philosophical Approaches to Inference",
    "text": "How many fundamentally different approaches exist for making statistical inferences? The answer is two: the frequentist approach and the Bayesian approach . These represent two distinct philosophical frameworks for reasoning about probability and uncertainty. The Frequentist Approach The frequentist approach, which has dominated statistical practice for much of the 20th century, interprets probability in terms of long-run frequencies. From this perspective, probability statements only make sense for events that can be repeated many times. Consider flipping a coin. A frequentist interprets “the probability of heads is 0.5” to mean: if we flip this coin infinitely many times, heads will appear in 50% of the flips. Probability, in this view, is an objective property of the world—a statement about what would happen if we could repeat an experiment indefinitely. This philosophical stance has important implications. Imagine I flip a coin and catch it in my hand, concealing the result. I know how it landed, but you don’t. What is the probability that it landed heads?"
  },
  {
    "objectID": "intro-data-analytics#section-17",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Thought Experiment",
    "text": "I’ve just flipped a coin and caught it in my closed hand. I can see the result, but you cannot. What is the probability that the coin shows heads? A frequentist would say: the probability is either 0 or 1, depending on how it actually landed. If it landed heads, the probability is 1 (certainty). If it landed tails, the probability is 0 (impossibility). The coin has already landed—there’s nothing probabilistic about it anymore. The event has occurred, and its outcome is now a fact of the world, even if you don’t know what that fact is. This reveals a key feature of frequentist thinking: probabilities apply to events that haven’t happened yet , not to events that have already occurred but whose outcomes we simply don’t know. From a frequentist perspective, once the coin has landed, talking about the “probability” of how it landed is meaningless. It landed some particular way. The uncertainty you feel is about your knowledge, not about the event itself. The Bayesian Approach The Bayesian approach takes a fundamentally different view. Bayesians interpret probability as a measure of our degree of belief or state of knowledge about an event. Probability, from this perspective, is subjective—it represents how confident we are, given the information we have. Let’s return to the coin in my hand. A Bayesian would say: given that you don’t know how it landed and you have no reason to believe the coin is unfair, your probability assessment should be 0.5. This doesn’t mean the coin is somehow in a superposition of states. Rather, it means that given your current state of knowledge, you should be equally uncertain about whether it shows heads or tails. If I were to give you a hint—say, “It’s not tails”—a Bayesian would immediately update your probability to 1 for heads. Your degree of belief changes as you gain new information, even though the physical state of the coin hasn’t changed at all."
  },
  {
    "objectID": "intro-data-analytics#section-18",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Fundamental Philosophical Difference",
    "text": "Frequentist view : Probability is an objective property of repeatable events. It doesn’t make sense to assign probabilities to fixed but unknown quantities. Bayesian view : Probability represents our degree of belief or state of knowledge. We can assign probabilities to any uncertain proposition, including fixed but unknown quantities. This philosophical difference leads to very different statistical methodologies. Frequentists develop procedures that work well in the long run—if we used this test over and over, we’d make correct decisions most of the time. Bayesians explicitly incorporate prior knowledge and update their beliefs as new evidence arrives. Most practicing statisticians today are implicitly Bayesian in their everyday reasoning about uncertainty, even if they use frequentist methods in their formal analyses. When we say “there’s a 70% chance it will rain tomorrow,” we’re thinking like Bayesians—probability as degree of belief. When we conduct a hypothesis test with a significance level of 0.05, we’re using frequentist methodology—probability as long-run frequency. Which Approach Is “Right”? Neither approach is universally correct or incorrect. They answer different questions and serve different purposes. Frequentist methods provide objective procedures with well-understood long-run properties, which makes them particularly valuable in fields like medical research where regulatory decisions require clear standards. Bayesian methods allow us to explicitly incorporate prior knowledge and provide direct probability statements about hypotheses, which makes them particularly valuable in fields where we have genuine prior information and want to update our beliefs. Throughout this course, we’ll primarily use frequentist methods, as these remain the dominant framework in most applied fields and are what you’ll encounter in published research. However, we’ll also discuss Bayesian perspectives where they provide valuable insights or alternative ways of thinking a"
  },
  {
    "objectID": "intro-data-analytics#understanding-hypothesis-testing-concepts",
    "href": "/chapter/intro-data-analytics#understanding-hypothesis-testing-concepts",
    "title": "The Purpose of Data Analytics",
    "section": "Understanding Hypothesis Testing Concepts",
    "text": "Before we can intelligently discuss either frequentist or Bayesian inference, we need to understand some fundamental concepts that appear throughout statistical testing. These ideas—particularly around errors in decision-making—form the conceptual foundation for statistical inference. Types of Errors When we conduct a statistical test, we’re making a decision: either reject a hypothesis or fail to reject it. Like any decision made under uncertainty, we can make mistakes. There are two types of mistakes we might make:"
  },
  {
    "objectID": "intro-data-analytics#section-20",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Type I Error",
    "text": "A Type I error occurs when we reject a hypothesis that is actually correct. We declare that something is happening when, in fact, it is not. In medical testing: declaring a healthy patient is sick (false positive) In criminal justice: convicting an innocent person In scientific research: claiming we’ve found an effect when none exists"
  },
  {
    "objectID": "intro-data-analytics#section-21",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Type II Error",
    "text": "A Type II error occurs when we fail to reject a hypothesis that is actually false. We fail to detect something that is really happening. In medical testing: declaring a sick patient is healthy (false negative) In criminal justice: acquitting a guilty person In scientific research: failing to detect an effect that actually exists These two types of errors are in tension with each other. If we make it harder to commit a Type I error (by requiring very strong evidence before rejecting a hypothesis), we inevitably make it easier to commit a Type II error (we’ll fail to detect real effects more often). Conversely, if we’re very eager to detect effects (reducing Type II errors), we’ll end up making more Type I errors by seeing patterns that aren’t really there. The P-Value The p-value is the probability of making a Type I error—the probability of rejecting a correct hypothesis. More precisely, it’s the probability of observing data as extreme as (or more extreme than) what we actually observed, assuming the hypothesis we’re testing is true. The p-value is calculated from your data using statistical procedures. It’s an output of your analysis, not an input. In the old days, p-values were looked up in printed tables at the back of statistics textbooks. Today, statistical software calculates them instantly."
  },
  {
    "objectID": "intro-data-analytics#section-22",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Common Misconception",
    "text": "The p-value is not “the probability that our results are wrong” or “the probability that the hypothesis is true.” It is specifically the probability of observing our data (or more extreme data) if the hypothesis we’re testing is actually correct. The Significance Level (α) The significance level , denoted by the Greek letter α (alpha), is the threshold probability you choose before collecting data. It represents how much Type I error risk you’re willing to tolerate. Commonly used significance levels include: - α = 0.05 (5%): The most common choice in many fields - α = 0.01 (1%): Used when Type I errors are particularly costly - α = 0.10 (10%): Used when Type I errors are less concerning or when sample sizes are small Here’s the crucial point: you choose α before looking at your data . The significance level is an input to your analysis, while the p-value is an output. You then compare them: If p-value < α: Reject the hypothesis (the evidence is strong enough) If p-value ≥ α: Fail to reject the hypothesis (the evidence is not strong enough) Why We Never “Accept” Hypotheses Notice the careful language: we “reject” or “fail to reject” hypotheses. We never “accept” a hypothesis. Why this asymmetry? The reason is fundamental to the nature of scientific reasoning. Consider the history of physics. About 500 years ago, Isaac Newton developed his theory of gravity, which explained why objects fall to the ground. For over two centuries, Newton’s theory was supported by all available evidence. Scientists didn’t say “we accept Newton’s theory as correct”—they said “we fail to reject it; it’s the best explanation we have so far.” Then, about 100 years ago, Albert Einstein developed general relativity, which showed that Newton’s theory, while extremely useful for everyday purposes, is actually incorrect in important ways. Einstein’s theory superseded Newton’s. But does this mean Einstein’s theory is “correct”? Not necessarily. It’s the best explanation we have now, consistent wit"
  },
  {
    "objectID": "intro-data-analytics#section-23",
    "href": "/chapter/intro-data-analytics",
    "title": "The Purpose of Data Analytics",
    "section": "Scientific Humility",
    "text": "In science, we can demonstrate that theories are wrong or false (by finding contradictory evidence), but we can never prove that theories are correct or true (because future evidence might contradict them). This is why we never “accept” hypotheses—we only fail to reject them given current evidence. This principle, articulated by philosopher Karl Popper, is called falsificationism . Scientific theories can be falsified but never verified with absolute certainty. This is why statistical hypothesis testing is framed around rejection rather than acceptance. Statistical Power There’s one more important concept related to errors: statistical power . Power is defined as the probability of not making a Type II error—that is, the probability of correctly rejecting a false hypothesis. High statistical power is desirable: it means your test is good at detecting effects when they exist. Power depends on several factors: - Sample size (larger samples → higher power) - Effect size (larger effects → easier to detect → higher power) - Significance level (higher α → higher power, but also more Type I errors) - Variability in the data (less noise → higher power) While there’s no standard name for the “probability of making a Type II error” (parallel to how we call Type I error probability the “p-value”), it’s typically denoted β (beta). Then power = 1 - β."
  },
  {
    "objectID": "intro-data-analytics#looking-ahead",
    "href": "/chapter/intro-data-analytics#looking-ahead",
    "title": "The Purpose of Data Analytics",
    "section": "Looking Ahead",
    "text": "Throughout this course, we’ll develop both descriptive and inferential tools. We’ll learn to: Visualize data through graphs and charts Calculate summary statistics that capture essential features of datasets Recognize different probability distributions and understand when each applies Use sample data to make justified inferences about populations Establish cause-and-effect relationships through careful analysis Navigate the philosophical differences between frequentist and Bayesian approaches Most importantly, we’ll engage in abstract thinking about data and probability. Statistics is not just a collection of computational procedures—it’s a coherent framework for reasoning about uncertainty, variability, and inference. Understanding this framework will serve you in any field where data and evidence matter. The goal of this book is not merely to learn formulas and procedures, but to develop statistical intuition—to think clearly about randomness, patterns, causation, and inference. This kind of thinking is increasingly essential in environmental policy, climate science, economics, public health, and virtually every domain where evidence-based decision-making matters. We’ll build this understanding gradually, starting with the foundations of probability and working our way up to sophisticated inferential methods. Along the way, we’ll grapple with deep questions: How do we know what we know? What does it mean for evidence to support a claim? How much uncertainty should we tolerate in our conclusions? These aren’t just technical questions—they’re fundamental questions about knowledge itself, approached through the lens of mathematical reasoning."
  },
  {
    "objectID": "multivariate-regression",
    "href": "/chapter/multivariate-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "Multiple Regression In 1978, David Harrison and Daniel Rubinfeld published a groundbreaking study on housing values and air pollution in the Boston metropolitan area. Their work introduced what has become one of the most studied datasets in econometrics and demonstrated how hedonic pricing models can be used to value environmental amenities—specifically, how air quality affects property values. The Harrison-Rubinfeld model remains a cornerstone example in applied econometrics courses because it elegantly combines theory with empirical analysis, using a rich set of neighborhood characteristics to explain median home values across census tracts in the Boston area. What is a hedonic pricing model, and why is it useful for valuing environmental goods? A hedonic pricing model decomposes the price of a good into the value of its constituent characteristics. For housing, this means breaking down the home price into components attributable to structural features (number of rooms, age), neighborhood characteristics (crime rate, school quality), and environmental amenities (air quality, proximity to employment centers). This approach is particularly valuable for environmental economics because many environmental goods—like clean air—don’t have explicit market prices. By observing how home values change with air quality while controlling for other factors, we can infer people’s willingness to pay for cleaner air. This is crucial for cost-benefit analysis of environmental regulations."
  },
  {
    "objectID": "multivariate-regression#the-model-specification",
    "href": "/chapter/multivariate-regression#the-model-specification",
    "title": "Multiple Regression",
    "section": "The Model Specification",
    "text": "The Harrison-Rubinfeld model estimates the logarithm of median home value as a function of 13 explanatory variables: \\begin{aligned} \\log(\\text{MEDV}) = \\beta_0 &+ \\beta_1 \\text{ NOX}^2 + \\beta_2 \\text{ RM}^2 + \\beta_3 \\text{ AGE} + \\beta_4 (\\text{B} - 0.63)^2 \\\\ &+ \\beta_5 \\log(\\text{LSTAT}) + \\beta_6 \\text{ CRIM} + \\beta_7 \\text{ ZN} + \\beta_8 \\text{ INDUS} \\\\ &+ \\beta_9 \\text{ TAX} + \\beta_{10} \\text{ PTRATIO} + \\beta_{11} \\text{ CHAS} \\\\ &+ \\beta_{12} \\log(\\text{DIS}) + \\beta_{13} \\log(\\text{RAD}) + \\epsilon \\end{aligned} Before interpreting the coefficients, let’s understand what each variable represents and why certain functional forms were chosen."
  },
  {
    "objectID": "multivariate-regression#the-variables-and-their-transformations",
    "href": "/chapter/multivariate-regression#the-variables-and-their-transformations",
    "title": "Multiple Regression",
    "section": "The Variables and Their Transformations",
    "text": "NOX (Nitric Oxide Concentration) : Annual average concentration of nitric oxides in parts per 10 million, measured at the census tract level. This is the key environmental variable in the study. The model includes NOX² rather than NOX itself. This quadratic specification allows for a nonlinear relationship between air pollution and housing values—suggesting that the marginal effect of pollution may increase at higher pollution levels. Why might the relationship between pollution and home values be nonlinear? There are several economic reasons to expect nonlinearity. First, at very low pollution levels, small increases may have minimal health impacts and thus little effect on property values. But at higher levels, additional pollution could have increasingly severe health consequences, making marginal increases more harmful. Second, there may be threshold effects—once pollution reaches certain levels, it becomes visibly obvious (as smog) or causes noticeable health effects, triggering a sharper decline in willingness to pay for homes in that area. Third, people who are highly sensitive to pollution likely already avoid high-pollution areas, so the remaining residents may be those who are relatively less concerned about pollution, leading to smaller marginal price effects at higher pollution levels. However, this selection effect would actually suggest the opposite of what Harrison and Rubinfeld found. RM (Average Number of Rooms) : The average number of rooms per dwelling in the census tract. This is a proxy for house size and quality. The model includes RM² to capture potential nonlinear effects of house size. Larger homes may command disproportionately higher prices, or there may be diminishing returns to additional rooms. AGE (Proportion of Old Units) : The proportion of owner-occupied units built prior to 1940. This captures the age composition of the housing stock. B (Proportion Black) : A transformation of the proportion of Black residents, specifically (1000(B"
  },
  {
    "objectID": "multivariate-regression#interpreting-the-coefficients",
    "href": "/chapter/multivariate-regression#interpreting-the-coefficients",
    "title": "Multiple Regression",
    "section": "Interpreting the Coefficients",
    "text": "Now that we understand the variables, let’s interpret what each coefficient tells us. The dependent variable is log(MEDV), which means we need to be careful about the interpretation depending on whether the independent variable is in levels, logs, or transformed. Environmental Quality: \\beta_1 (NOX²) Since the model includes NOX², the effect of pollution on home values is: \\frac{\\partial \\log(\\text{MEDV})}{\\partial \\text{NOX}} = 2\\beta_1 \\cdot \\text{NOX} If \\beta_1 < 0 (which Harrison and Rubinfeld found), then higher pollution reduces home values, and this effect intensifies at higher pollution levels. If \\beta_1 = -0.008 and the mean NOX level is 0.55, what does this tell us about the effect of pollution? At the mean pollution level, the marginal effect is: \\frac{\\partial \\log(\\text{MEDV})}{\\partial \\text{NOX}} = 2(-0.008)(0.55) = -0.0088 This means a one-unit increase in NOX (from 0.55 to 1.55 parts per 10 million) would decrease log(MEDV) by 0.0088, which corresponds to approximately a 0.88% decrease in median home value. However, this interpretation requires careful thought about units. A one-unit change in NOX is actually quite large given the scale of the variable. A more realistic interpretation considers smaller changes—say, a 0.1 unit increase in NOX: \\Delta \\log(\\text{MEDV}) \\approx 2(-0.008)(0.55)(0.1) = -0.00088 This suggests a 0.1 unit increase in NOX is associated with about a 0.09% decrease in home values. The key insight from this coefficient is that air quality has a measurable, statistically significant effect on property values—providing evidence that people value clean air and are willing to pay for it through higher home prices. Housing Stock: \\beta_2 (RM²) and \\beta_3 (AGE) The coefficient on RM² captures how house size affects values: \\frac{\\partial \\log(\\text{MEDV})}{\\partial \\text{RM}} = 2\\beta_2 \\cdot \\text{RM} If \\beta_2 > 0 , larger homes command higher prices, with the effect strengthening for larger homes. This makes economic sense: go"
  },
  {
    "objectID": "multivariate-regression#the-intercept-beta_0",
    "href": "/chapter/multivariate-regression#the-intercept-beta_0",
    "title": "Multiple Regression",
    "section": "The Intercept: \\beta_0",
    "text": "The intercept \\beta_0 represents the predicted log median home value when all explanatory variables are zero. This has no meaningful interpretation in this context because: Many variables can’t actually be zero (like RM—houses need at least some rooms) The model includes squared and logged terms that make zero points artificial No census tract in the data has all variables at zero The intercept’s role is to shift the entire prediction function to match the observed range of home values given the actual distribution of the explanatory variables. It’s a necessary component for accurate predictions but doesn’t have a substantive economic interpretation."
  },
  {
    "objectID": "multivariate-regression#the-error-term-epsilon",
    "href": "/chapter/multivariate-regression#the-error-term-epsilon",
    "title": "Multiple Regression",
    "section": "The Error Term: \\epsilon",
    "text": "The error term captures all determinants of home values not included in the model: - Idiosyncratic features of specific properties - Measurement error in the variables - Omitted neighborhood characteristics - Unobservable location amenities - Time-specific shocks The key assumption for causal interpretation is that \\mathbb{E}[\\epsilon | X] = 0 —the error term is uncorrelated with the included explanatory variables. This requires that all confounding factors are either included in the model or are uncorrelated with both home values and the variables of interest. What are the main threats to the identification assumption in the Harrison-Rubinfeld model? Several potential violations could bias the coefficients: Omitted variables : Any characteristic that affects both home values and is correlated with included variables will bias estimates. For example: School quality measures beyond PTRATIO Crime type (violent vs. property) Local amenities (parks, restaurants, retail) Housing unit quality within tracts Reverse causality : Some variables might be endogenous. For example: High home values might attract more property tax revenue, potentially affecting TAX Expensive neighborhoods might invest more in crime prevention, affecting CRIM Demand for housing might influence zoning decisions (ZN) Measurement error : Variables are measured at the census tract level but ideally would be property-specific: Not all houses in a tract face the same crime risk Distance to employment varies within tracts Air quality can have micro-scale variation Sample selection : The data covers only the Boston metropolitan area in 1970. Results may not generalize to other cities, time periods, or housing markets. Spatial correlation : Nearby census tracts likely have correlated unobservables, violating the assumption of independent errors and potentially biasing standard errors even if point estimates are unbiased. Modern applications often address these concerns through instrumental variables, spatia"
  },
  {
    "objectID": "multivariate-regression#synthesizing-the-results",
    "href": "/chapter/multivariate-regression#synthesizing-the-results",
    "title": "Multiple Regression",
    "section": "Synthesizing the Results",
    "text": "The Harrison-Rubinfeld model demonstrates how hedonic pricing methods can decompose property values into their constituent parts. Each coefficient tells us how much people are willing to pay—through higher home prices—for marginal improvements in that characteristic. Key insights from the coefficient interpretations: Environmental quality is valued : The negative coefficient on NOX² shows that air pollution reduces property values, providing a revealed-preference measure of willingness to pay for clean air. Neighborhood socioeconomics matter : The coefficients on LSTAT and the racial composition variable show strong effects of neighborhood demographics, reflecting either preferences or the correlation of these variables with unobserved neighborhood quality. Safety commands a premium : The negative coefficient on CRIM quantifies how much people pay to live in safer neighborhoods. Local public finance is capitalized : Both property taxes and school quality (PTRATIO) significantly affect home values, consistent with theories of fiscal capitalization. Location and accessibility matter : The distance to employment centers and highway accessibility coefficients show the value of reduced commuting costs. Natural amenities are priced : The Charles River dummy shows that even simple geographical features command measurable premiums. Understanding these interpretations is crucial not just for applied econometrics, but for policy analysis. When regulators conduct cost-benefit analysis of environmental regulations, hedonic pricing studies like Harrison and Rubinfeld’s provide the benefit estimates. When local governments make zoning decisions or invest in public services, understanding how these choices are capitalized into property values helps predict impacts on community composition and tax revenues."
  },
  {
    "objectID": "multivariate-regression#technical-notes-on-functional-form",
    "href": "/chapter/multivariate-regression#technical-notes-on-functional-form",
    "title": "Multiple Regression",
    "section": "Technical Notes on Functional Form",
    "text": "The Harrison-Rubinfeld specification makes several functional form choices that affect interpretation: Log-linear model : When the dependent variable is logged but some independent variables are in levels, the coefficient represents the approximate percentage change in Y for a one-unit change in X. Log-log model : When both the dependent and independent variables are logged, the coefficient represents an elasticity—the percentage change in Y for a one percent change in X. Quadratic terms : When a variable appears squared, its effect is nonlinear, and the marginal effect depends on the level of the variable. The choice among these functional forms should ideally be guided by economic theory and tested using the data. Harrison and Rubinfeld likely experimented with various specifications, choosing functional forms that: 1. Fit the data well (high R², low residual variance) 2. Made economic sense (appropriate signs, reasonable magnitudes) 3. Satisfied statistical diagnostics (no heteroskedasticity, normality of residuals) Modern practice often uses more flexible functional forms (splines, polynomials, or nonparametric methods) or machine learning approaches that let the data reveal the functional relationships without imposing strong restrictions."
  },
  {
    "objectID": "multivariate-regression#conclusion",
    "href": "/chapter/multivariate-regression#conclusion",
    "title": "Multiple Regression",
    "section": "Conclusion",
    "text": "The Harrison-Rubinfeld hedonic pricing model exemplifies how careful empirical analysis can reveal the implicit prices of characteristics that don’t trade in explicit markets. By interpreting each coefficient correctly—accounting for functional form, units, and identification assumptions—we transform a regression equation into a rich description of how various neighborhood and housing characteristics are valued in the housing market. These interpretations matter for: - Policy makers evaluating the benefits of environmental regulations - Urban planners understanding the trade-offs residents make regarding location, density, and amenities - Economists studying how households sort across neighborhoods and how local public goods are valued - Real estate professionals predicting how property values respond to changes in neighborhood characteristics The model’s enduring influence in economics stems not just from its methodological contributions, but from its demonstration that even complex, multifaceted goods like housing can be decomposed and understood through careful economic analysis—revealing the preferences and values that lie beneath market prices."
  },
  {
    "objectID": "normal-distribution",
    "href": "/chapter/normal-distribution",
    "title": "The normal distribution",
    "section": "",
    "text": "The normal distribution This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "operators-properties",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "",
    "text": "Statistical operators are powerful tools that transform random variables in systematic ways. In this chapter, we’ll explore two fundamental operators: the expectation operator and the variance operator. These operators will appear throughout the rest of this book, so understanding their properties deeply will pay dividends as we tackle more complex statistical concepts. By the end of this chapter, you will be able to: Define what an operator is in the statistical context Calculate and interpret expected values Calculate and interpret variances Apply the properties of expectation and variance to simplify complex expressions Understand how these operators behave under linear transformations"
  },
  {
    "objectID": "operators-properties#section-2",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Definition",
    "text": "An operator is a mapping that takes elements from one space and produces elements in another space (which may be the same space). In statistics, operators act on random variables to produce new quantities. Think of an operator as a special kind of function that acts on random variables rather than on simple numbers. Just as the square root function takes a number and returns another number, statistical operators take random variables and return quantities that summarize key features of those variables."
  },
  {
    "objectID": "operators-properties#section-3",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: Why do we call them “operators” instead of just “functions”?",
    "text": "The term “operator” emphasizes that these mappings act on objects (random variables) that are themselves functions. This distinguishes them from ordinary functions that act on numbers. The expectation operator, for instance, takes an entire probability distribution and distills it down to a single number representing its center."
  },
  {
    "objectID": "operators-properties#the-expectation-operator",
    "href": "/chapter/operators-properties#the-expectation-operator",
    "title": "operators properties",
    "section": "The Expectation Operator",
    "text": "Intuition and Definition Intuitively, a random variable’s expected value represents the average we would see if we observed many independent realizations of that variable. For example, if we roll a fair six-sided die thousands of times and compute the average of all the outcomes, that average will converge to 3.5. This value—3.5—is the expected value of the die roll."
  },
  {
    "objectID": "operators-properties#section-5",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Definition: Expected Value",
    "text": "The expected value (or expectation ) of a discrete random variable X is the probability-weighted average of all its possible values: \\mathbb{E}[X] = \\sum_{i=1}^n x_i p_i where x_i are the possible values and p_i = \\mathrm{P}(X = x_i) are their respective probabilities. More generally, we can write this as: \\mathbb{E}[X] = \\sum_{i=1}^n p_i X_i = \\mu where we often use the Greek letter \\mu (mu) to denote the expected value. For continuous random variables, the sum becomes an integral: \\mathbb{E}[X] = \\int_{\\mathbb{R}} x f(x) \\, dx where f(x) is the probability density function of X ."
  },
  {
    "objectID": "operators-properties#section-6",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: Can you give a concrete example of computing an expected value?",
    "text": "Consider a simple game where you flip a fair coin. If it lands heads, you win $10; if it lands tails, you lose $5. What are your expected winnings? Let X represent your winnings. Then: \\mathbb{E}[X] = 10 \\cdot \\mathrm{P}(H) + (-5) \\cdot \\mathrm{P}(T) = 10 \\cdot \\frac{1}{2} + (-5) \\cdot \\frac{1}{2} = \\$2.50 On average, you expect to win $2.50 per game. This doesn’t mean you’ll ever actually win $2.50 in any single game—you’ll either win $10 or lose $5. But over many games, your average winnings will approach $2.50 per game. Properties of the Expectation Operator The expectation operator has several important properties that make it remarkably useful for statistical analysis. These properties allow us to simplify complex calculations and derive important results. Property 1: Non-negativity If X is a random variable such that \\mathrm{P}(X \\geq 0) = 1 (that is, X is always non-negative), then \\mathbb{E}[X] \\geq 0 ."
  },
  {
    "objectID": "operators-properties#section-7",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "If \\mathrm{P}(X \\geq 0) = 1 , then the probability mass function satisfies p_X(x) = 0 for all x < 0 . Therefore: \\mathbb{E}[X] = \\sum_x x p_X(x) = \\sum_{x: x \\geq 0} x p_X(x) \\geq 0 since we’re summing only non-negative terms ( x \\geq 0 and p_X(x) \\geq 0 ). This property formalizes an intuitive idea: if a random variable can only take non-negative values, its average must also be non-negative. Property 2: Expectation of a Constant If X is a random variable such that \\mathrm{P}(X = r) = 1 for some fixed number r , then \\mathbb{E}[X] = r . In other words, the expectation of a constant equals that constant."
  },
  {
    "objectID": "operators-properties#section-8",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "If \\mathrm{P}(X = r) = 1 , then p_X(r) = 1 and p_X(x) = 0 for all x \\neq r . Therefore: \\mathbb{E}[X] = \\sum_x x p_X(x) = r \\cdot 1 = r This property tells us that constants behave exactly as we’d expect under the expectation operator—their “average” value is simply themselves. Property 3: Linearity The expectation operator is linear . Given two random variables X and Y and two real constants a and b : \\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]"
  },
  {
    "objectID": "operators-properties#section-9",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "For discrete random variables with joint probability mass function p_{X,Y}(x,y) : \\begin{aligned} \\mathbb{E}[aX + bY] &= \\sum_{x,y}(ax+by)p_{X,Y}(x,y) \\\\ &= a\\sum_x x \\sum_y p_{X,Y}(x,y) + b\\sum_y y \\sum_x p_{X,Y}(x,y) \\\\ &= a\\sum_x x \\, p_{X}(x) + b\\sum_y y \\, p_{Y}(y) \\\\ &= a\\mathbb{E}[X] + b\\mathbb{E}[Y] \\end{aligned} where in the third line we used the fact that \\sum_y p_{X,Y}(x,y) = p_X(x) (the marginal distribution)."
  },
  {
    "objectID": "operators-properties#section-10",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Why Linearity Matters",
    "text": "Linearity is perhaps the most important property of expectation. It allows us to break complex random variables into simpler parts, compute expectations of the parts separately, and combine them. Moreover, linearity holds regardless of whether the random variables are independent —a remarkable and powerful feature."
  },
  {
    "objectID": "operators-properties#section-11",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: How can we use linearity in practice?",
    "text": "Suppose you’re analyzing a portfolio with investments in three different assets. Let X_1, X_2, X_3 represent the returns on these assets, and suppose you invest amounts w_1, w_2, w_3 in each. Your total return is R = w_1 X_1 + w_2 X_2 + w_3 X_3 . By linearity: \\mathbb{E}[R] = w_1 \\mathbb{E}[X_1] + w_2 \\mathbb{E}[X_2] + w_3 \\mathbb{E}[X_3] This means you can calculate your expected portfolio return simply by taking a weighted average of the expected returns of the individual assets—no need to work out the entire joint distribution of all three assets together. Additional properties that follow from linearity include: \\begin{aligned} \\mathbb{E}[kY] &= k\\mathbb{E}[Y] \\quad \\text{(scaling)} \\\\ \\mathbb{E}[X + Y] &= \\mathbb{E}[X] + \\mathbb{E}[Y] \\quad \\text{(additivity)} \\end{aligned}"
  },
  {
    "objectID": "operators-properties#the-variance-operator",
    "href": "/chapter/operators-properties#the-variance-operator",
    "title": "operators properties",
    "section": "The Variance Operator",
    "text": "Intuition and Definition While the expected value tells us about the center of a distribution, it says nothing about the spread. Consider two random variables: one that always equals 10, and one that equals 0 half the time and 20 half the time. Both have an expected value of 10, but they behave very differently. The variance operator captures this difference. Variance measures how far a set of random values typically lie from their expected value. A small variance indicates that values cluster tightly around the mean; a large variance indicates that values are more dispersed."
  },
  {
    "objectID": "operators-properties#section-13",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Definition: Variance",
    "text": "The variance of a random variable X is the expected value of the squared deviation from the mean: \\mathrm{Var}(X) = \\mathbb{E}[(X - \\mu)^2] where \\mu = \\mathbb{E}[X] is the mean of X . We often denote variance as \\sigma^2 (sigma squared). For a discrete random variable, we can write this explicitly as: \\mathrm{Var}(Y) = \\sum_{i=1}^n p_i (Y_i - \\mu)^2 = \\sigma^2"
  },
  {
    "objectID": "operators-properties#section-14",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: Why do we square the deviations? Why not just take the absolute value?",
    "text": "Squaring serves several purposes. First, it ensures that positive and negative deviations don’t cancel out (which would happen if we just summed the deviations directly). Second, squaring gives more weight to extreme deviations, making variance sensitive to outliers. Third, the squared form has beautiful mathematical properties that simplify many derivations. While we could use absolute deviations instead (this gives the “mean absolute deviation”), the squared form is more tractable mathematically and appears naturally in many statistical contexts. An Alternative Formula The definition of variance can be algebraically rearranged into a form that’s often more convenient for computation: \\begin{aligned} \\mathrm{Var}(X) &= \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\\\ &= \\mathbb{E}[X^2 - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^2] \\\\ &= \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\mathbb{E}[X] + \\mathbb{E}[X]^2 \\\\ &= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2 \\end{aligned} This gives us the memorable formula: \\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 In words: the variance equals the expected value of the square minus the square of the expected value. This computational formula is often easier to work with than the definitional formula. Properties of the Variance Operator Property 1: Non-negativity Variance is always non-negative: \\mathrm{Var}(X) \\geq 0 ."
  },
  {
    "objectID": "operators-properties#section-15",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "Since (X - \\mu)^2 \\geq 0 for all values of X , we have: \\mathrm{Var}(X) = \\mathbb{E}[(X - \\mu)^2] \\geq 0 by the non-negativity property of expectation. Property 2: Variance of a Constant The variance of a constant is zero: \\mathrm{Var}(a) = 0 ."
  },
  {
    "objectID": "operators-properties#section-16",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "\\begin{aligned} \\mathrm{Var}(a) &= \\mathbb{E}[(a - \\mathbb{E}[a])^2] \\\\ &= \\mathbb{E}[(a - a)^2] \\\\ &= \\mathbb{E}[0^2] \\\\ &= 0 \\end{aligned} This makes intuitive sense: if a variable doesn’t vary (it’s constant), its variance should be zero. Property 3: Zero Variance Implies Constant If the variance of a random variable is zero, then the variable must be constant with probability 1: \\mathrm{Var}(X) = 0 \\Rightarrow \\mathrm{P}(X = a) = 1 for some constant a ."
  },
  {
    "objectID": "operators-properties#section-17",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "Let \\mathbb{E}[X] = a for some constant a . Then: \\begin{aligned} \\mathrm{Var}(X) = 0 &\\Rightarrow \\mathbb{E}[(X - a)^2] = 0 \\\\ &\\Rightarrow (X - a)^2 = 0 \\quad \\text{(since $(X-a)^2$ cannot be negative)} \\\\ &\\Rightarrow X - a = 0 \\\\ &\\Rightarrow X = a \\end{aligned} Together, Properties 2 and 3 tell us that constants are precisely the random variables with zero variance—and vice versa. Property 4: Variance of a Sum The variance of a sum of two random variables is: \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y) where \\mathrm{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] is the covariance between X and Y ."
  },
  {
    "objectID": "operators-properties#section-18",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "\\begin{aligned} \\mathrm{Var}(X+Y) &= \\mathbb{E}[(X+Y - \\mathbb{E}[X+Y])^2] \\\\ &= \\mathbb{E}[(X+Y)^2 - 2(X+Y)\\mathbb{E}[X+Y] + (\\mathbb{E}[X+Y])^2] \\\\ &= \\mathbb{E}[(X+Y)^2] - \\mathbb{E}[X+Y]^2 \\\\ &= \\mathbb{E}[X^2] + 2\\mathbb{E}[XY] + \\mathbb{E}[Y^2] - (\\mathbb{E}[X] + \\mathbb{E}[Y])^2 \\\\ &= \\mathbb{E}[X^2] + 2\\mathbb{E}[XY] + \\mathbb{E}[Y^2] - \\mathbb{E}[X]^2 - 2\\mathbb{E}[X]\\mathbb{E}[Y] - \\mathbb{E}[Y]^2 \\\\ &= (\\mathbb{E}[X^2] - \\mathbb{E}[X]^2) + (\\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2) + 2(\\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]) \\\\ &= \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y) \\end{aligned}"
  },
  {
    "objectID": "operators-properties#section-19",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Special Case: Independent Variables",
    "text": "If X and Y are independent random variables, then \\mathrm{Cov}(X,Y) = 0 , and the formula simplifies to: \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) Similarly, for the difference of independent variables: \\mathrm{Var}(X - Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) . Property 5: Variance is Invariant to Location Shifts If a constant is added to all values of a variable, the variance is unchanged: \\mathrm{Var}(X + a) = \\mathrm{Var}(X)"
  },
  {
    "objectID": "operators-properties#section-20",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "\\begin{aligned} \\mathrm{Var}(X + a) &= \\mathrm{Var}(X) + \\mathrm{Var}(a) + 2\\mathrm{Cov}(X, a) \\\\ &= \\mathrm{Var}(X) \\end{aligned} since \\mathrm{Var}(a) = 0 and \\mathrm{Cov}(X, a) = 0 (a constant has zero covariance with any variable). This property reflects the fact that variance measures spread, not location. Shifting all values by the same amount doesn’t change how spread out they are. Property 6: Variance Under Scaling If all values are scaled by a constant, the variance is scaled by the square of that constant: \\mathrm{Var}(aX) = a^2 \\mathrm{Var}(X)"
  },
  {
    "objectID": "operators-properties#section-21",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Proof",
    "text": "\\begin{aligned} \\mathrm{Var}(aX) &= \\mathbb{E}[(aX - \\mathbb{E}[aX])^2] \\\\ &= \\mathbb{E}[(aX - a\\mathbb{E}[X])^2] \\\\ &= \\mathbb{E}[(a(X - \\mathbb{E}[X]))^2] \\\\ &= \\mathbb{E}[a^2(X - \\mathbb{E}[X])^2] \\\\ &= a^2\\mathbb{E}[(X - \\mathbb{E}[X])^2] \\\\ &= a^2 \\mathrm{Var}(X) \\end{aligned}"
  },
  {
    "objectID": "operators-properties#section-22",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: Why does variance scale with the square of the constant rather than just the constant itself?",
    "text": "Remember that variance involves squared deviations: \\mathrm{Var}(X) = \\mathbb{E}[(X-\\mu)^2] . When we scale X by a , we also scale the deviations by a : (aX - a\\mu) = a(X - \\mu) . When we square this, we get a^2(X-\\mu)^2 , which explains the a^2 factor. This property is why the standard deviation (the square root of variance) scales linearly with a : if we double all values, we double the standard deviation but quadruple the variance. Property 7: Variance of a Sum of Independent Identically Distributed Variables If Y_1, Y_2, \\ldots, Y_n are independent and identically distributed random variables, then: \\mathrm{Var}\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n \\mathrm{Var}(Y_i) = n\\mathrm{Var}(Y) where the last equality uses the fact that all the Y_i have the same variance. This property is fundamental to understanding sampling distributions and the behavior of sample means."
  },
  {
    "objectID": "operators-properties#putting-it-all-together",
    "href": "/chapter/operators-properties#putting-it-all-together",
    "title": "operators properties",
    "section": "Putting It All Together",
    "text": "Let’s work through a comprehensive example that uses both operators and their properties."
  },
  {
    "objectID": "operators-properties#section-24",
    "href": "/chapter/operators-properties",
    "title": "operators properties",
    "section": "Question: A Manufacturing Quality Control Example",
    "text": "Suppose you’re managing quality control for a manufacturing process. Each item has a production cost that’s normally distributed with mean $50 and variance $25. If an item passes inspection (which happens 90% of the time), you can sell it for $100. If it fails inspection, you must sell it at a loss for $30. You produce 100 items. What are the expected total profit and the variance of total profit? Solution: Let’s define our random variables carefully. For item i : Let C_i be the production cost (mean $50, variance $25) Let R_i be the revenue, which is $100 with probability 0.9 and $30 with probability 0.1 The profit for item i is P_i = R_i - C_i First, let’s find \\mathbb{E}[R_i] : \\mathbb{E}[R_i] = 100(0.9) + 30(0.1) = 90 + 3 = \\$93 For the expected profit on one item: \\mathbb{E}[P_i] = \\mathbb{E}[R_i - C_i] = \\mathbb{E}[R_i] - \\mathbb{E}[C_i] = 93 - 50 = \\$43 For 100 items, by linearity of expectation: \\mathbb{E}\\left[\\sum_{i=1}^{100} P_i\\right] = \\sum_{i=1}^{100} \\mathbb{E}[P_i] = 100 \\times 43 = \\$4,300 Now for the variance. First, we need \\mathrm{Var}(R_i) : \\begin{aligned} \\mathrm{Var}(R_i) &= \\mathbb{E}[R_i^2] - (\\mathbb{E}[R_i])^2 \\\\ &= [100^2(0.9) + 30^2(0.1)] - 93^2 \\\\ &= [9000 + 90] - 8649 \\\\ &= 441 \\end{aligned} For the variance of profit on one item, assuming cost and revenue are independent: \\mathrm{Var}(P_i) = \\mathrm{Var}(R_i - C_i) = \\mathrm{Var}(R_i) + \\mathrm{Var}(C_i) = 441 + 25 = 466 Finally, if items are produced independently: \\mathrm{Var}\\left(\\sum_{i=1}^{100} P_i\\right) = \\sum_{i=1}^{100} \\mathrm{Var}(P_i) = 100 \\times 466 = 46,600 Therefore, expected total profit is $4,300 with variance $46,600 (standard deviation of approximately $216)."
  },
  {
    "objectID": "operators-properties#summary",
    "href": "/chapter/operators-properties#summary",
    "title": "operators properties",
    "section": "Summary",
    "text": "The expectation and variance operators are fundamental tools in probability and statistics. The expectation operator \\mathbb{E}[\\cdot] captures the center or average of a distribution, while the variance operator \\mathrm{Var}(\\cdot) captures its spread. Key takeaways: Expectation is linear: \\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y] , regardless of dependence Variance is not linear: \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) only when X and Y are independent Adding constants doesn’t change variance: \\mathrm{Var}(X + a) = \\mathrm{Var}(X) Scaling affects variance quadratically: \\mathrm{Var}(aX) = a^2\\mathrm{Var}(X) These operators and their properties will appear repeatedly throughout your study of statistics. Mastering them now will make everything that follows much more intuitive."
  },
  {
    "objectID": "panel-data",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "",
    "text": "Panel Data Methods Panel data—repeated observations on the same individuals over time—offers researchers a powerful tool for addressing one of the most vexing problems in observational research: unobserved heterogeneity. In this chapter, we’ll explore how the longitudinal structure of panel data allows us to control for time-invariant individual characteristics that would otherwise bias our estimates. Our running example throughout this chapter will draw from the National Longitudinal Survey of Youth 1979 (NLSY79), which has followed a cohort of young Americans since 1979. We’ll focus on a fundamental question in labor economics: What is the return to education? That is, how much more do workers earn for each additional year of schooling they complete? By the end of this chapter, you will understand: Why panel data helps address omitted variable bias Fixed effects estimation and the within transformation Random effects estimation and when it’s appropriate First-differencing as an alternative to fixed effects How to implement these methods in R The key assumptions underlying each approach Why can’t we just estimate the return to education by regressing wages on years of schooling using cross-sectional data? If we simply regress wages on education using a single cross-section of workers, we face a severe omitted variable bias problem. Workers with more education may differ from workers with less education in many unobserved ways that also affect earnings—ability, motivation, family background, social networks, and so on. If these unobserved characteristics are positively correlated with both education and wages, a simple OLS regression will overstate the causal effect of education on earnings."
  },
  {
    "objectID": "panel-data#the-nlsy79-data",
    "href": "/chapter/panel-data#the-nlsy79-data",
    "title": "Panel Data Methods",
    "section": "The NLSY79 Data",
    "text": "The National Longitudinal Survey of Youth 1979 began with 12,686 respondents aged 14-22 in 1979. These individuals have been surveyed repeatedly (annually through 1994, biennially since then), providing detailed information about their education, employment, earnings, family background, and test scores. For our analysis, we’ll focus on a subset of the data: male respondents observed during their prime working years (ages 25-35). This gives us multiple observations per person, typically spanning 5-10 years. Here’s what our data structure looks like: ```{r} #| echo: true #| eval: false Load required packages library(tidyverse) library(plm) # For panel data methods library(lfe) # For high-dimensional fixed effects library(stargazer) # For nice regression tables Load NLSY data (hypothetical structure) nlsy <- read_csv(“nlsy_panel.csv”) Look at the structure head(nlsy) id year age educ logwage experience union married region 1 1986 28 12 2.45 6 0 1 NE 1 1987 29 12 2.52 7 0 1 NE 1 1988 30 12 2.58 8 1 1 NE 2 1986 27 16 2.88 3 0 0 S 2 1987 28 16 2.95 4 0 1 S 2 1988 29 16 3.02 5 0 1 S ::: {.question} What features of this data structure make it \"panel data\"? ::: ::: {.answer} Panel data has two key features visible here: 1. **Multiple individuals**: Each person has a unique identifier (`id`) 2. **Multiple time periods**: Each person appears in multiple years This creates a two-dimensional structure: we observe variation both *across* individuals and *within* individuals over time. It's this within-person variation that we'll exploit to control for unobserved individual characteristics. ::: ## The Omitted Variable Bias Problem Let's start by understanding exactly what problem panel data helps us solve. Suppose we're interested in estimating the causal effect of education on log wages. We might write down a simple model: $$ \\log(wage_{it}) = \\beta_0 + \\beta_1 educ_i + u_{it} $$ where $i$ indexes individuals and $t$ indexes time periods. The parameter $\\beta_1$ represents the r"
  },
  {
    "objectID": "panel-data#section-2",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "The Key Insight",
    "text": "The cross-sectional estimate conflates two distinct effects: The causal effect of education on wages The correlation between education and unobserved ability Panel data methods allow us to separate these two effects by exploiting the longitudinal structure of the data."
  },
  {
    "objectID": "panel-data#fixed-effects-the-within-transformation",
    "href": "/chapter/panel-data#fixed-effects-the-within-transformation",
    "title": "Panel Data Methods",
    "section": "Fixed Effects: The Within Transformation",
    "text": "The fundamental insight of fixed effects estimation is surprisingly simple: if unobserved ability doesn’t change over time, we can eliminate it by looking at changes within individuals. The Fixed Effects Model We start with a more explicit model that separates time-invariant from time-varying factors: \\log(wage_{it}) = \\beta_0 + \\beta_1 educ_{it} + \\beta_2 experience_{it} + \\beta_3 experience_{it}^2 + \\alpha_i + \\varepsilon_{it} The key addition is \\alpha_i —an individual-specific intercept that captures all time-invariant characteristics of person i . This includes: Innate ability Family background Personality traits Network effects from childhood Anything else about person i that doesn’t change over our observation period If \\alpha_i is unobserved and correlated with education, why doesn’t this cause omitted variable bias just like before? The crucial difference is that \\alpha_i doesn’t vary over time. This allows us to eliminate it through a clever transformation. If we take the time average of our equation for each individual: \\overline{\\log(wage_i)} = \\beta_0 + \\beta_1 \\overline{educ_i} + \\beta_2 \\overline{experience_i} + \\beta_3 \\overline{experience_i^2} + \\alpha_i + \\overline{\\varepsilon_i} and subtract this from the original equation, \\alpha_i disappears completely. This is called the within transformation or time-demeaning . The Within Transformation Let’s see this transformation explicitly. For each individual i , we compute the time averages: \\begin{aligned} \\overline{\\log(wage_i)} &= \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\log(wage_{it}) \\\\ \\overline{educ_i} &= \\frac{1}{T_i} \\sum_{t=1}^{T_i} educ_{it} \\end{aligned} where T_i is the number of time periods we observe individual i . Now subtract these averages from the original equation: \\log(wage_{it}) - \\overline{\\log(wage_i)} = \\beta_1(educ_{it} - \\overline{educ_i}) + \\beta_2(experience_{it} - \\overline{experience_i}) + ... + (\\varepsilon_{it} - \\overline{\\varepsilon_i}) Notice what’s missing: \\alpha_i has compl"
  },
  {
    "objectID": "panel-data#practical-considerations-and-robustness",
    "href": "/chapter/panel-data#practical-considerations-and-robustness",
    "title": "Panel Data Methods",
    "section": "Practical Considerations and Robustness",
    "text": "Clustered Standard Errors A critical issue in panel data analysis is that observations for the same individual are unlikely to be independent. Wage shocks might persist over time, leading to serial correlation in \\varepsilon_{it} . This violates the standard OLS assumption and causes our standard errors to understate uncertainty. The solution is to compute cluster-robust standard errors , clustering at the individual level: ```{r} #| echo: true #| eval: false Fixed effects with clustered standard errors library(lmtest) library(sandwich) Compute robust covariance matrix fe_vcov_cluster <- vcovHC(fe_model, type = “HC1”, cluster = “group”) Get corrected standard errors and test statistics coeftest(fe_model, vcov = fe_vcov_cluster) Coefficients: Estimate Std. Error t value Pr(>|t|) educ 0.0523 0.0189 2.767 0.00566 ** experience 0.0812 0.0132 6.152 < 2e-16 * I(experience^2) -0.0024 0.0009 -2.667 0.00766 union 0.0654 0.0221 2.959 0.00309 ** married 0.0432 0.0198 2.182 0.02912 * Notice how the clustered standard errors are larger than the default standard errors, reflecting the within-person correlation in wage shocks. This is typical in panel data applications. ::: {.callout-important} ## Always Cluster Your Standard Errors In panel data applications, you should almost always compute cluster-robust standard errors, clustering at the individual (or higher) level. Failing to do so will lead to overstated precision and too-frequent rejection of null hypotheses. This is one of the most common errors in applied panel data analysis. ::: ### Time Fixed Effects Our model so far has assumed that there are no aggregate time effects—that is, nothing systematic happens to all workers' wages in particular years. This is unrealistic. Recessions, inflation, technological change, and policy reforms affect everyone. We can add **time fixed effects** (year dummies) to control for these aggregate shocks: $$ \\log(wage_{it}) = \\beta_1 educ_{it} + \\beta_2 experience_{it} + \\beta_3 experience_{"
  },
  {
    "objectID": "panel-data#extensions-and-further-reading",
    "href": "/chapter/panel-data#extensions-and-further-reading",
    "title": "Panel Data Methods",
    "section": "Extensions and Further Reading",
    "text": "Dynamic Panel Data Our models have assumed that past wages don’t directly affect current wages (except through persistent individual effects and serially correlated shocks). But what if there’s true state dependence —where having high wages in the past directly causes high wages today? We could add a lagged dependent variable: \\log(wage_{it}) = \\rho \\log(wage_{i,t-1}) + \\beta_1 educ_{it} + ... + \\alpha_i + \\varepsilon_{it} This creates serious econometric challenges. The within transformation produces bias because \\ddot{\\log(wage_{i,t-1})} is correlated with \\ddot{\\varepsilon_{it}} by construction. Special methods like the Arellano-Bond GMM estimator are needed. Unbalanced Panels Our discussion assumed a balanced panel—the same individuals observed in all periods. Real panel datasets are typically unbalanced, with individuals entering and exiting the sample. The good news is that fixed effects and first-differencing naturally handle unbalanced panels, using all available observations. But attrition could cause selection bias if individuals’ exit depends on their wage trajectories. More on Identification We’ve focused on the mechanical aspects of panel data estimation, but the deeper questions are about identification: What variation in the data identifies our parameters? Is this the “right” variation for answering our causal question? What assumptions are required for a causal interpretation? For the NLSY education returns, we’re identifying \\beta_1 from individuals whose education changes while working. This raises questions: Are these returns generalizable to traditional students? Might education changes while working be endogenous to wage trajectories? Could there be time-varying confounders we’re not controlling for? These questions don’t have purely statistical answers. They require economic reasoning about the context and careful consideration of what variation we’re exploiting."
  },
  {
    "objectID": "panel-data#summary",
    "href": "/chapter/panel-data#summary",
    "title": "Panel Data Methods",
    "section": "Summary",
    "text": "Panel data methods offer powerful tools for addressing omitted variable bias by exploiting repeated observations on the same individuals. Here are the key takeaways:"
  },
  {
    "objectID": "panel-data#section-7",
    "href": "/chapter/panel-data",
    "title": "Panel Data Methods",
    "section": "Key Points",
    "text": "Fixed effects eliminates time-invariant unobserved heterogeneity by using within-person variation. It requires no assumptions about the relationship between \\alpha_i and regressors, but sacrifices the ability to estimate effects of time-invariant variables. Random effects uses both within- and between-person variation, gaining efficiency and allowing estimation of time-invariant effects. But it requires the strong assumption that \\alpha_i is uncorrelated with all regressors. First-differencing is an alternative to fixed effects that may be preferred when errors follow a random walk or when there are only two time periods. The Hausman test helps choose between fixed and random effects by testing whether they produce systematically different estimates. Always use cluster-robust standard errors in panel data applications to account for within-person correlation in errors. Time fixed effects should generally be included to control for aggregate time trends and shocks. The variation that identifies panel data estimates may be local —applying to specific subpopulations (like those whose treatment status changes). Extrapolation requires caution. Applied Lessons from the NLSY Our analysis of returns to education in the NLSY79 illustrates several important points: Cross-sectional estimates (10.8%) substantially overstate returns due to omitted ability bias Fixed effects estimates (5.2%) are roughly half the cross-sectional estimates, suggesting ability bias is large and positive These estimates apply specifically to workers who complete additional schooling while employed—a selected group The Hausman test strongly rejects the random effects assumption, confirming that ability is correlated with education The broader lesson: the source of identifying variation matters . Panel data methods don’t eliminate all endogeneity concerns—they only address time-invariant unobserved heterogeneity. Time-varying confounders, reverse causality, and measurement error remain potential threat"
  },
  {
    "objectID": "parameters-statistics",
    "href": "/chapter/parameters-statistics",
    "title": "Parameters and statistics",
    "section": "",
    "text": "Parameters and statistics This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "probability-distributions",
    "href": "/chapter/probability-distributions",
    "title": "Probability",
    "section": "",
    "text": "Probability In this chapter, we’ll introduce some fundamental concepts in probability theory. By the end of this chapter, you will be able to define the following: Sample space Outcome Event Probability Random variable"
  },
  {
    "objectID": "probability-distributions#what-is-probability",
    "href": "/chapter/probability-distributions#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability? Probability is a mathematical framework for quantifying uncertainty. It assigns numerical values between 0 and 1 to events, where 0 indicates impossibility and 1 indicates certainty. The sample space is the set of all possible outcomes of an experiment. For a coin flip, the sample space is \\{H, T\\} . Let’s consider a simple example. Imagine you’re flipping a fair coin. The sample space consists of two possible outcomes: heads (H) and tails (T). The probability of getting heads is: P(H) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}} = \\frac{1}{2} = 0.5"
  },
  {
    "objectID": "probability-distributions#random-variables",
    "href": "/chapter/probability-distributions#random-variables",
    "title": "Probability",
    "section": "Random Variables",
    "text": "A random variable is a function that assigns numerical values to the outcomes of a random experiment. Can you give an example of a random variable? Consider rolling a six-sided die. Let X be the random variable representing the number that appears on the top face. Then X can take values \\{1, 2, 3, 4, 5, 6\\} , each with probability 1/6 if the die is fair."
  },
  {
    "objectID": "probability-distributions#relationship-between-pdfs-and-cdfs",
    "href": "/chapter/probability-distributions#relationship-between-pdfs-and-cdfs",
    "title": "Probability",
    "section": "Relationship between PDFs and CDFs",
    "text": "The probability density function (PDF) and cumulative distribution function (CDF) are two fundamental ways of describing a probability distribution. The interactive visualization below demonstrates how these functions relate to each other. Use the dropdown menu to explore different distributions (Normal, Lognormal, and Uniform), and drag the slider to see how the PDF height at a point relates to the CDF value at that same point. As you move the slider, observe that: - The PDF ( f(Y) ) shows the height of the density at any given value - The CDF ( F(Y) ) shows the area under the PDF curve to the left of that value - The shaded region in the PDF panel corresponds exactly to the CDF value"
  },
  {
    "objectID": "probability-distributions#embedding-videos",
    "href": "/chapter/probability-distributions#embedding-videos",
    "title": "Probability",
    "section": "Embedding Videos",
    "text": "Here’s how to embed your Manim animations. Replace your_video.mp4 with the actual filename. Your browser does not support the video tag. Random Variables in Action: This animation demonstrates how a random variable maps outcomes from a sample space to numerical values."
  },
  {
    "objectID": "propensity-score",
    "href": "/chapter/propensity-score",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "Propensity Score Matching Imagine you’re tasked with evaluating whether a job training program actually helps people earn more money. You collect data on hundreds of workers—some who participated in the program and some who didn’t. You compare their earnings and find that, on average, those who went through the training actually earn less than those who didn’t. Should you conclude the program is harmful? Not so fast. The problem is that people don’t randomly stumble into job training programs. Those who seek out such programs often start from a position of disadvantage—they might have less education, weaker employment histories, or face other barriers to employment. In other words, the two groups aren’t comparable to begin with. This is the fundamental challenge of causal inference from observational data: when treatment isn’t randomly assigned, how can we estimate what would have happened to the treated individuals if they hadn’t received treatment? In this chapter, we’ll explore one elegant solution to this problem: propensity score matching . What makes propensity score matching different from simply comparing averages between treated and untreated groups? Propensity score matching explicitly accounts for the fact that treated and untreated individuals may differ systematically in their observable characteristics. Rather than comparing all treated individuals to all untreated individuals, it finds pairs (or small groups) of individuals who look similar in terms of their background characteristics but differ in whether they received treatment. This creates a more “apples-to-apples” comparison."
  },
  {
    "objectID": "propensity-score#the-national-supported-work-demonstration",
    "href": "/chapter/propensity-score#the-national-supported-work-demonstration",
    "title": "Propensity Score Matching",
    "section": "The National Supported Work Demonstration",
    "text": "To make these ideas concrete, we’ll work with data from the National Supported Work (NSW) Demonstration, a job training program implemented in the 1970s. The program provided work experience to disadvantaged workers—individuals with histories of drug use, criminal records, or long-term unemployment—in an effort to help them transition to regular employment. What makes this dataset particularly valuable for learning about causal inference is that the NSW program actually was randomized for a subset of participants. This means we know the “ground truth”—the actual causal effect of the program. We can then see how well observational methods like propensity score matching can recover this effect when we pretend we don’t have the benefit of randomization. Let’s start by looking at the data. We have information on 445 individuals: 185 who participated in the NSW program (the treated group) and 260 who did not (the control group). For each person, we observe: Outcome : Real earnings in 1978 (after the program) Pre-treatment characteristics : Age Years of education Race and ethnicity (Black, Hispanic) Marital status High school degree indicator Real earnings in 1974 (before the program) Real earnings in 1975 (before the program) Employment status in 1974 (whether earnings were zero) Employment status in 1975 (whether earnings were zero) Here’s a glimpse of what the data looks like: ```{python} #| echo: false import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyBboxPatch import seaborn as sns Set random seed for reproducibility np.random.seed(42) Load or create the LaLonde NSW dataset For demonstration, I’ll create a simplified version In practice, you would load the actual dataset Create NSW experimental data n_treated = 185 n_control = 260 Treatment group (disadvantaged background) treated_data = { ‘treat’: np.ones(n_treated), ‘age’: np.random.normal(25, 7, n_treated), ‘educ’: np.random.normal(10, 2, n_treated), ‘black’"
  },
  {
    "objectID": "propensity-score#assessing-balance-after-matching",
    "href": "/chapter/propensity-score#assessing-balance-after-matching",
    "title": "Propensity Score Matching",
    "section": "Assessing Balance After Matching",
    "text": "The key test of whether matching worked is whether it achieved balance—that is, whether the matched treated and control groups now look similar in terms of their pre-treatment characteristics. Let’s check: ```{python} #| echo: false # Create matched sample matched_treated_idx = matches[‘treated_idx’].values matched_control_idx = matches[‘control_idx’].values matched_treated = df_obs.loc[matched_treated_idx] matched_control = df_obs.loc[matched_control_idx] Balance table for matched sample print(“After Matching:”) balance_data_matched = [] for var in covariates: treated_val = matched_treated[var].mean() control_val = matched_control[var].mean() diff = treated_val - control_val # Also compute standardized difference pooled_sd = np.sqrt((matched_treated[var].std()**2 + matched_control[var].std()**2) / 2) std_diff = diff / pooled_sd if pooled_sd > 0 else 0 balance_data_matched.append({ 'Variable': var, 'Treated': f'{treated_val:.2f}', 'Control': f'{control_val:.2f}', 'Difference': f'{diff:.2f}', 'Std. Diff.': f'{std_diff:.3f}' }) balance_df_matched = pd.DataFrame(balance_data_matched) print(balance_df_matched.to_string(index=False)) Much better! The standardized differences are now much smaller. A common rule of thumb is that standardized differences should be less than 0.1 (or sometimes 0.25) for adequate balance. While not perfect, matching has substantially reduced the imbalance between treated and control groups. ::: {.question} What is a standardized difference, and why do we use it instead of just looking at raw differences? ::: ::: {.answer} A standardized difference expresses the difference between groups in units of standard deviations. It's calculated as the difference in means divided by the pooled standard deviation. We use it because it's scale-invariant—a difference of 2 years in age means something very different from a difference of $2,000 in earnings. By standardizing, we can assess balance consistently across variables measured in different units. ::: "
  },
  {
    "objectID": "propensity-score#estimating-the-treatment-effect",
    "href": "/chapter/propensity-score#estimating-the-treatment-effect",
    "title": "Propensity Score Matching",
    "section": "Estimating the Treatment Effect",
    "text": "Now that we have a matched sample with good balance, we can estimate the treatment effect. The simplest approach is to compare average outcomes between the matched treated and control groups: ```{python} #| echo: false # Estimate treatment effect on matched sample matched_treated_outcome = matched_treated[‘re78’].mean() matched_control_outcome = matched_control[‘re78’].mean() matched_effect = matched_treated_outcome - matched_control_outcome print(f”Effect Estimates:“) print(f”{‘Method’:<30} {‘Estimate’:>12}“) print(f”{‘-’*42}“) print(f”{‘Experimental benchmark’:<30} ${naive_effect:>11,.2f}“) print(f”{‘Naive (PSID controls)’:<30} ${naive_effect_obs:>11,.2f}“) print(f”{‘Propensity score matching’:<30} ${matched_effect:>11,.2f}“) ``` The propensity score matching estimate is much closer to the experimental benchmark than the naive comparison! This demonstrates the power of matching: by creating comparable groups, we can recover estimates that approximate what we would have found in a randomized experiment. Why isn’t the propensity score matching estimate exactly equal to the experimental benchmark? There are several reasons: (1) Matching only balances observed characteristics—if there are important unobserved differences between NSW participants and PSID controls, matching won’t eliminate that bias. (2) Even with the same data, different matching methods (nearest neighbor vs. kernel, different calipers, etc.) can produce slightly different estimates. (3) The experimental benchmark itself has sampling variability. The key point is that matching gets us much closer to the truth than naive comparisons."
  },
  {
    "objectID": "propensity-score#the-fundamental-assumption-unconfoundedness",
    "href": "/chapter/propensity-score#the-fundamental-assumption-unconfoundedness",
    "title": "Propensity Score Matching",
    "section": "The Fundamental Assumption: Unconfoundedness",
    "text": "All of this analysis rests on a critical assumption called unconfoundedness or selection on observables . This assumption states that, conditional on the observed covariates X , treatment assignment is independent of potential outcomes: (Y_1, Y_0) \\perp \\text{Treat} \\mid X In plain English: once we account for all the observed characteristics, there are no remaining systematic differences between treated and control groups that affect outcomes. This is a strong assumption, and it’s fundamentally untestable. We can check whether we’ve achieved balance on observed characteristics, but we can never know whether there are unobserved confounders lurking in the background. When is the unconfoundedness assumption most plausible? The assumption is most credible when: (1) We have rich data on pre-treatment characteristics that are likely to affect both treatment assignment and outcomes. (2) We understand the treatment assignment process well enough to know what variables matter. (3) The treatment decision is based primarily on factors we can observe. In the NSW example, if individuals selected into the program based solely on observable characteristics like employment history and demographics, unconfoundedness is plausible. If they also selected based on unobservable factors like motivation or family support, we may still have bias."
  },
  {
    "objectID": "propensity-score#sensitivity-analysis-how-robust-are-our-results",
    "href": "/chapter/propensity-score#sensitivity-analysis-how-robust-are-our-results",
    "title": "Propensity Score Matching",
    "section": "Sensitivity Analysis: How Robust Are Our Results?",
    "text": "Given that we can never be certain about unconfoundedness, it’s important to conduct sensitivity analyses. These ask: how strong would unobserved confounding need to be to change our conclusions? One approach, developed by Rosenbaum (2002), examines how much the odds of treatment would need to differ between matched individuals to overturn our findings. If only a small amount of confounding could change our conclusions, we should be cautious. If it would take substantial confounding, we can be more confident. Another approach is to examine whether our results are stable when we: - Use different matching methods - Change the caliper width - Include or exclude specific covariates - Trim observations with extreme propensity scores Robust findings that hold across multiple specifications are more credible than fragile results that change dramatically with small methodological choices."
  },
  {
    "objectID": "propensity-score#when-to-use-propensity-score-matching",
    "href": "/chapter/propensity-score#when-to-use-propensity-score-matching",
    "title": "Propensity Score Matching",
    "section": "When to Use Propensity Score Matching",
    "text": "Propensity score matching is a powerful tool, but it’s not always the best choice. Here’s when it works well: Use PSM when: - You have rich pre-treatment covariate data - You believe selection is primarily on observables - You need to assess and demonstrate balance - You have reasonable overlap in covariate distributions - You want an intuitive, transparent analysis Consider alternatives when: - You have limited covariate data (unconfoundedness less plausible) - Overlap is very poor (few good matches available) - You have panel data with pre-treatment outcomes (difference-in-differences may be better) - You have an instrumental variable (IV estimation may be better) - You need to model the outcome function carefully (regression adjustment may be better)"
  },
  {
    "objectID": "propensity-score#extensions-and-variations",
    "href": "/chapter/propensity-score#extensions-and-variations",
    "title": "Propensity Score Matching",
    "section": "Extensions and Variations",
    "text": "The basic propensity score matching framework we’ve covered can be extended in several ways: Matching with replacement : Each control can be matched to multiple treated units, which improves balance but reduces efficiency. Matching with multiple controls : Each treated unit is matched to k controls (e.g., k=3 ) and the treatment effect is the difference between the treated unit’s outcome and the average outcome of its matches. Kernel matching and local linear matching : Instead of discrete matches, use weighted averages of all controls, with weights depending on propensity score distance. Doubly robust estimation : Combine propensity score matching with regression adjustment. This approach is “doubly robust” in that it yields consistent estimates if either the propensity score model or the outcome regression model is correctly specified (though not necessarily both). Covariate balancing propensity score (CBPS) : Instead of just maximizing likelihood, estimate propensity scores to directly optimize covariate balance. Each of these extensions involves tradeoffs between bias and variance, and the choice depends on the specific application."
  },
  {
    "objectID": "propensity-score#practical-guidelines",
    "href": "/chapter/propensity-score#practical-guidelines",
    "title": "Propensity Score Matching",
    "section": "Practical Guidelines",
    "text": "Based on the LaLonde example and broader research, here are some practical guidelines for implementing propensity score matching: Start with descriptive analysis : Examine covariate distributions before matching to understand the selection process and assess overlap. Choose covariates carefully : Include variables that affect both treatment assignment and outcomes. Avoid including post-treatment variables or instruments. Check for common support : Trim observations with extreme propensity scores or use calipers to enforce overlap. Assess balance explicitly : Use standardized differences and visual diagnostics like love plots. Be transparent about choices : Report results under multiple specifications to demonstrate robustness. Acknowledge limitations : Discuss the unconfoundedness assumption and conduct sensitivity analyses. Compare to other methods : If possible, compare PSM estimates to results from other causal inference methods as a robustness check."
  },
  {
    "objectID": "propensity-score#conclusion",
    "href": "/chapter/propensity-score#conclusion",
    "title": "Propensity Score Matching",
    "section": "Conclusion",
    "text": "Propensity score matching provides an elegant solution to the challenge of causal inference from observational data. By summarizing many covariates into a single score and using it to create balanced comparison groups, we can approximate the conditions of a randomized experiment—at least with respect to observed characteristics. The LaLonde dataset beautifully illustrates both the power and the limitations of this approach. When we have good overlap and rich covariate data, matching can recover estimates close to experimental benchmarks. But matching is only as good as the data we have: it cannot control for unobserved confounders, and it requires sufficient overlap to find good comparisons. As you apply these methods to your own data, remember that propensity score matching is a tool, not a magic wand. It requires careful implementation, thorough diagnostics, and honest acknowledgment of assumptions. Used thoughtfully, it can help us learn about causal effects from observational data. Used carelessly, it can create a false sense of confidence in potentially biased estimates. The next chapter will explore related methods for causal inference from observational data, including inverse probability weighting, difference-in-differences, and regression discontinuity designs. Each has its own strengths and weaknesses, and understanding the full toolkit allows us to choose the right tool for each problem."
  },
  {
    "objectID": "propensity-score#further-reading",
    "href": "/chapter/propensity-score#further-reading",
    "title": "Propensity Score Matching",
    "section": "Further Reading",
    "text": "Original propensity score paper : Rosenbaum, P. R., & Rubin, D. B. (1983). “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika , 70(1), 41-55. LaLonde’s evaluation : LaLonde, R. J. (1986). “Evaluating the Econometric Evaluations of Training Programs with Experimental Data.” American Economic Review , 76(4), 604-620. Practical guide : Caliendo, M., & Kopeinig, S. (2008). “Some Practical Guidance for the Implementation of Propensity Score Matching.” Journal of Economic Surveys , 22(1), 31-72. Modern causal inference : Imbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction . Cambridge University Press."
  },
  {
    "objectID": "summary-statistics",
    "href": "/chapter/summary-statistics",
    "title": "Summary statistics",
    "section": "",
    "text": "Summary statistics This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-mean-large",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "",
    "text": "Testing a claim about a population mean In this chapter, we’ll develop a comprehensive understanding of hypothesis testing through a detailed worked example. We’ll build the theoretical foundation step by step, introducing key concepts like standard error, test statistics, and p-values along the way. By the end of this chapter, you will understand how to conduct hypothesis tests for population means, interpret their results, and recognize the crucial differences between large and small sample tests."
  },
  {
    "objectID": "testing-mean-large#the-problem-evaluating-a-new-curriculum",
    "href": "/chapter/testing-mean-large#the-problem-evaluating-a-new-curriculum",
    "title": "Testing a claim about a population mean",
    "section": "The Problem: Evaluating a New Curriculum",
    "text": "Let’s begin with a concrete problem that will guide our exploration of hypothesis testing. Suppose we’re trying to improve the logical ability of students through a new curriculum. The old curriculum, which has been in use for many years, produces an average test score of 80 points. We’ve developed a new curriculum and trained a large group of students using this approach. The central question we want to answer is: Is the new curriculum more effective at raising average test scores? To investigate this question, we randomly sample 38 students from those trained under the new curriculum and record their test scores. Our sample yields a mean score of 83 points. Our data: Test scores from 38 randomly selected students"
  },
  {
    "objectID": "testing-mean-large#section-2",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🤔 Initial Observation",
    "text": "While our sample mean of 83 is higher than the old curriculum mean of 80, we cannot immediately conclude that the population mean of all students trained under the new curriculum exceeds 80. Why not? Because our sample is just one of many possible samples we could have drawn, and sample means vary due to random sampling. The Logic of Hypothesis Testing Frequentist hypothesis testing operates on a principle analogous to proof by contradiction in mathematics. We temporarily assume the opposite of what we hope to demonstrate, then show that this assumption leads to implausible results. The nature of hypothesis testing: the probabilistic equivalent of proof by contradiction The intuition is straightforward: we set up a hypothesis about a population parameter, assume it’s correct, then calculate the conditional probability of observing our sample data. If this probability is sufficiently small, we reject the hypothesis. Intuition underlying frequentist hypothesis testing"
  },
  {
    "objectID": "testing-mean-large#understanding-standard-error",
    "href": "/chapter/testing-mean-large#understanding-standard-error",
    "title": "Testing a claim about a population mean",
    "section": "Understanding Standard Error",
    "text": "Before we can conduct a proper hypothesis test, we need to understand a crucial concept: standard error ."
  },
  {
    "objectID": "testing-mean-large#section-4",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 What is Standard Error?",
    "text": "Standard error measures how far, on average , a sample mean deviates from the population mean across repeated samples. It quantifies the precision of our estimator. Mathematically, the standard error of the sample mean is: SE = \\frac{\\sigma}{\\sqrt{n}} where \\sigma is the population standard deviation and n is the sample size. The Concept of Repeated Sampling To truly understand standard error, we need to embrace a core principle of frequentist statistics: repeated sampling . Imagine we could draw not just one sample of 38 students, but millions of such samples from our population. Each sample would give us a different sample mean. Here’s the remarkable thing: if we computed all these sample means and calculated their average, that average would equal the true population mean. This property is called unbiasedness , and it’s why we use the sample mean as our estimator. But these individual sample means would vary around the population mean. The standard error tells us the typical size of this variation. In our example, with a sample size of 38 and a calculated standard error of 1.64 points, we know that a typical sample mean deviates from the population mean by about 1.64 points."
  },
  {
    "objectID": "testing-mean-large#section-5",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Key Insight",
    "text": "Smaller standard error = More precision = Greater reliability When the standard error is small, we can be more confident that our single observed sample mean is close to the true population mean. The standard error decreases as sample size increases, which is why larger samples give us more reliable estimates. The Relationship: Population Variance to Sample Mean Variance There’s a fundamental relationship connecting the population variance to the variance of the sample mean: \\text{Var}(\\bar{Y}) = \\frac{\\sigma^2}{n} Taking the square root of both sides gives us the standard error formula. This relationship tells us that: The variance of sample means is smaller than the population variance This variance decreases as sample size increases The relationship is inverse with sample size (doubling n doesn’t double precision)"
  },
  {
    "objectID": "testing-mean-large#the-three-stages-of-hypothesis-testing",
    "href": "/chapter/testing-mean-large#the-three-stages-of-hypothesis-testing",
    "title": "Testing a claim about a population mean",
    "section": "The Three Stages of Hypothesis Testing",
    "text": "Now that we understand standard error, we can proceed with our hypothesis test. We’ll work through this systematically in three stages. We will follow a three-stage process to test hypotheses Stage 1: Formulating the Hypotheses Stage I: Setup The first step in any hypothesis test is to clearly state what we’re testing. We need two competing hypotheses. Expressing Our Claim in English Elucidate claim and its complement in English Claim: Students trained under the new curriculum will score, on average, higher than 80 on the test. Complement: Students trained under the new curriculum will not score, on average, higher than 80 on the test. The Law of the Excluded Middle In formulating our hypotheses, we’re invoking a fundamental principle of logic: the law of the excluded middle . The law of the excluded middle This law states that a statement is either true or false—there is no middle ground between truth and falsity. Either the new curriculum improves scores beyond 80, or it doesn’t. Our job is to determine which is more likely given our data. Symbolic Representation Express claim and its complement symbolically Let the mean score of students trained under the new curriculum be \\mu . Claim: \\mu > 80 Complement: \\mu \\leq 80 Assigning Null and Alternative Hypotheses Specify null and alternative hypotheses Null Hypothesis ( H_0 ): The population mean test score under the new curriculum is less than or equal to 80. H_0: \\mu \\leq 80 Alternative Hypothesis ( H_A ): The population mean test score under the new curriculum exceeds 80. H_A: \\mu > 80 The null hypothesis represents the status quo or the claim we’re trying to find evidence against. The alternative hypothesis represents what we hope to demonstrate with our data. By convention, we assign the complement of our claim to the null hypothesis—this is what we will attempt to falsify. We also need to choose a significance level \\alpha , which represents our tolerance for making a Type I error (rejecting a true null hypothe"
  },
  {
    "objectID": "testing-mean-large#section-7",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "📊 Why This Setup?",
    "text": "Notice that our null hypothesis includes the equality. This is a one-sided test because we’re only interested in whether the new curriculum is better , not just different. If we cared about any difference (better or worse), we’d use a two-sided test. Understanding Type I and Type II Errors Before proceeding, we must acknowledge that hypothesis testing involves the possibility of error. There are two types of errors we might make: Anticipating the possibility of erring Type I Error: Rejecting a true null hypothesis (false positive) Type II Error: Failing to reject a false null hypothesis (false negative) It’s crucial to understand that we cannot make both errors simultaneously: We cannot make both errors simultaneously Each error is associated with a unique decision If we reject the null, we can make only a Type I error If we don’t reject the null, we can make only a Type II error Choosing the Significance Level We also need to choose a significance level \\alpha , which represents our tolerance for making a Type I error (rejecting a true null hypothesis). The level of significance is the largest probability of making a Type I error that a researcher is willing to tolerate In many academic papers, the level of significance is set at either 5% or 1%. But where do these specific values come from? Why are these specific values used commonly? The answer involves both history and convention. The story begins with an afternoon tea party and R.A. Fischer. The Lady Tasting Tea Fischer’s work on experimental design, inspired by a colleague who claimed she could tell whether milk was added before or after tea, led to the development of significance testing as we know it today. Stage 2: Estimating the Sampling Distribution In this stage, we need to characterize the distribution of our test statistic under the assumption that the null hypothesis is true. Step 1: Choose an Estimator We use the sample mean \\bar{Y} as our estimator of the population mean \\mu . Our observed value is "
  },
  {
    "objectID": "testing-mean-large#section-8",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 The Sampling Distribution",
    "text": "We now have a complete picture of the sampling distribution under H_0 : \\bar{Y} \\sim N(80, 1.64^2) This means if the null hypothesis is true, sample means from repeated samples would be normally distributed around 80 with a standard deviation of 1.64. Visualizing the Distribution Imagine a bell curve centered at 80. This represents all possible sample means we could observe if the true population mean were 80. Some sample means would be less than 80, some greater, but they’d cluster around 80 with most values falling within a few standard errors of the center. Our observed sample mean of 83 lies to the right of this center. The question is: is it far enough to the right that we should doubt the null hypothesis? Stage 3: Computing the Test Statistic and P-value To answer our question, we need to standardize our observed value and determine how unusual it is. The Test Statistic: Zeta (ζ) We define a test statistic called zeta (ζ) as: \\zeta = \\frac{\\bar{Y} - \\mu_0}{SE} where \\mu_0 is the hypothesized population mean under the null (80 in our case). This standardization accomplishes two things: 1. It converts our result to a unit-free measure 2. It tells us how many standard errors our observed mean is from the hypothesized mean"
  },
  {
    "objectID": "testing-mean-large#section-9",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Interpretation of ζ",
    "text": "The value of ζ represents the number of standard deviations (or standard errors) that the observed sample mean is from the hypothesized population mean. If our observed sample had a mean of 83 kg, the population mean were 80 kg, and the standard error were 1.64 kg, then: \\zeta = \\frac{83 - 80}{1.64} = 1.83 The “kg” units cancel out, leaving us with a pure number: 1.83 standard errors above the hypothesized mean. Calculating Our Test Statistic For our problem: \\zeta = \\frac{83 - 80}{1.64} = \\frac{3}{1.64} \\approx 1.83 Our observed sample mean is 1.83 standard errors above the hypothesized mean of 80. The Distribution of Zeta When the sample size is large and we know (or can estimate) the population standard deviation, the test statistic ζ follows a standard normal distribution (also called a Z-distribution). This is the same as the Z-scores you may have encountered before. \\zeta \\sim N(0, 1) Computing the P-value The p-value answers the question: “If the null hypothesis were true, what is the probability of observing a test statistic as extreme as or more extreme than what we actually observed?” For our one-sided test: p\\text{-value} = P(\\zeta \\geq 1.83 \\mid H_0 \\text{ is true}) Using a standard normal table or software, we find: p\\text{-value} \\approx 0.034 \\text{ or } 3.4\\% Making the Decision We compare our p-value to our significance level: - p-value = 3.4% - \\alpha = 4% Since the p-value (3.4%) is less than our significance level (4%), we reject the null hypothesis ."
  },
  {
    "objectID": "testing-mean-large#section-10",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "📊 Conclusion",
    "text": "We have sufficient evidence at the 4% significance level to conclude that the new curriculum improves average test scores beyond 80. The probability of observing a sample mean as high as 83 (or higher) purely by chance, if the true population mean were 80 or less, is only 3.4%."
  },
  {
    "objectID": "testing-mean-large#visual-interpretation",
    "href": "/chapter/testing-mean-large#visual-interpretation",
    "title": "Testing a claim about a population mean",
    "section": "Visual Interpretation",
    "text": "Let’s visualize what we’ve done. Picture the sampling distribution under the null hypothesis: a normal curve centered at 80 with standard deviation 1.64. Our observed sample mean of 83 falls in the right tail of this distribution. The p-value is the area under this curve to the right of 83—it represents how much of the distribution lies at or beyond our observed value. This area is relatively small (3.4%), indicating that our observation would be quite unusual if the null hypothesis were true."
  },
  {
    "objectID": "testing-mean-large#the-small-sample-case-when-n-30",
    "href": "/chapter/testing-mean-large#the-small-sample-case-when-n-30",
    "title": "Testing a claim about a population mean",
    "section": "The Small Sample Case: When n < 30",
    "text": "Everything we’ve done so far assumes a large sample (typically n \\geq 30 ). But what happens when we have a small sample? The mathematics changes in an important way. The Problem with Small Samples Consider the same problem, but now suppose we only have n = 24 students in our sample. The sample mean is still 83, and the sample standard deviation is still 10.1. The key difference: when we use the sample standard deviation s to estimate the population standard deviation \\sigma , we introduce additional uncertainty. This uncertainty becomes problematic when the sample size is small. William Gosset’s T-Distribution In the early 1900s, William Sealy Gosset (writing under the pseudonym “Student” because his employer, Guinness Brewery, didn’t allow employees to publish) discovered that for small samples, the test statistic doesn’t follow a normal distribution—it follows a t-distribution . The test statistic is still calculated the same way: \\zeta = \\frac{\\bar{Y} - \\mu_0}{s/\\sqrt{n}} But now, instead of following a Z-distribution, ζ follows a t-distribution with \\nu = n-1 degrees of freedom : \\zeta \\sim t_{\\nu}"
  },
  {
    "objectID": "testing-mean-large#section-13",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "💡 Why Degrees of Freedom?",
    "text": "We lose one degree of freedom because we used one piece of information from our sample to estimate the population standard deviation. We “expended” one observation to estimate the mean, which we then used to calculate the standard deviation. In our example with n = 24 , we have \\nu = 23 degrees of freedom. Properties of the T-Distribution The t-distribution looks similar to the normal distribution—it’s symmetric and bell-shaped—but it has heavier tails . This reflects the additional uncertainty from estimating the standard deviation. Key properties: 1. As the degrees of freedom increase, the t-distribution approaches the normal distribution 2. For small degrees of freedom, the tails are much heavier than the normal 3. By \\nu \\approx 30 , the t-distribution is virtually indistinguishable from the normal Comparing the Two Ratios Let’s clarify the distinction between two similar-looking ratios: Ratio A (with known σ): \\text{Andrew} = \\frac{\\bar{Y} - \\mu_0}{\\sigma/\\sqrt{n}} Ratio B (with estimated s): \\text{Ben} = \\frac{\\bar{Y} - \\mu_0}{s/\\sqrt{n}}"
  },
  {
    "objectID": "testing-mean-large#section-14",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🤔 Question",
    "text": "Which ratio fluctuates more from sample to sample—Andrew or Ben? Answer: Ben fluctuates more because both the numerator AND the denominator are random variables. In Andrew, only the numerator ( \\bar{Y} ) varies; the denominator ( \\sigma/\\sqrt{n} ) is a known constant. In Ben, both \\bar{Y} and s vary from sample to sample, creating additional volatility. This extra volatility is exactly what the t-distribution accounts for. Small Sample Analysis: Our Example Let’s return to our curriculum problem with the small sample of 24 students: n = 24 \\bar{y} = 83 s = 10.1 SE = 10.1/\\sqrt{24} \\approx 2.06 Test statistic: \\zeta = \\frac{83 - 80}{2.06} \\approx 1.46 This time, ζ follows a t-distribution with 23 degrees of freedom. Looking up this value in a t-table or using software: p\\text{-value} \\approx 0.08 \\text{ or } 8\\% The Decision Changes Now our p-value (8%) exceeds our significance level (4%). We fail to reject the null hypothesis ."
  },
  {
    "objectID": "testing-mean-large#section-15",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "⚠️ Critical Insight",
    "text": "Notice what happened: With the same sample mean (83) and the same sample standard deviation (10.1), we reached opposite conclusions depending on our sample size! Large sample ( n=38 ): Reject H_0 (p = 3.4%) Small sample ( n=24 ): Fail to reject H_0 (p = 8%) The difference lies in the additional uncertainty from estimating σ with a small sample. The t-distribution’s heavier tails mean we need more extreme evidence to reject the null hypothesis. When to Use Each Distribution Use the Z-distribution (normal) when: - Sample size is large ( n \\geq 30 ) - Population standard deviation σ is known (rare in practice) Use the t-distribution when: - Sample size is small ( n < 30 ) - Population standard deviation σ is unknown and must be estimated from the sample In practice, many statisticians use the t-distribution for all tests involving estimated standard deviations, regardless of sample size. As the degrees of freedom increase, the t-distribution becomes virtually identical to the normal, so using the t-distribution is a conservative choice that’s always appropriate."
  },
  {
    "objectID": "testing-mean-large#summary-the-hypothesis-testing-framework",
    "href": "/chapter/testing-mean-large#summary-the-hypothesis-testing-framework",
    "title": "Testing a claim about a population mean",
    "section": "Summary: The Hypothesis Testing Framework",
    "text": "Let’s review the complete process we’ve developed: Stage 1: Set Up 1. State the null and alternative hypotheses 2. Choose a significance level α 3. Identify the test type (one-sided or two-sided) Stage 2: Characterize the Sampling Distribution 1. Select an appropriate estimator 2. Use theory (CLT) to establish its distribution 3. Estimate the parameters of this distribution 4. Visualize the distribution under H_0 Stage 3: Test and Decide 1. Calculate the test statistic (ζ) 2. Determine its distribution (Z or t) 3. Compute the p-value 4. Compare p-value to α and make a decision 5. State your conclusion in context"
  },
  {
    "objectID": "testing-mean-large#section-17",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🔄 The Theoretical Foundation",
    "text": "Notice how our hypothesis testing framework rests on a foundation of theoretical results: Markov’s Inequality → proves Chebyshev’s Inequality Chebyshev’s Inequality → proves the Law of Large Numbers Law of Large Numbers → proves the Central Limit Theorem Central Limit Theorem → justifies the normality of sampling distributions Unbiasedness → tells us where distributions are centered Standard Error Formula → quantifies sampling variability Each piece plays a crucial role in the edifice we’ve constructed."
  },
  {
    "objectID": "testing-mean-large#looking-ahead-two-sample-tests-and-causality",
    "href": "/chapter/testing-mean-large#looking-ahead-two-sample-tests-and-causality",
    "title": "Testing a claim about a population mean",
    "section": "Looking Ahead: Two-Sample Tests and Causality",
    "text": "The hypothesis test we’ve developed compares a population mean to a known constant (80). While this is valuable for understanding the mechanics of hypothesis testing, it’s relatively rare in practice. More commonly, we want to compare two groups : a control group and a treatment group. This leads to two-sample tests, which will be our next topic. Two-sample tests look very similar mechanically to what we’ve done here, with a crucial philosophical difference: they allow us to investigate causality . By comparing a treatment group to a control group, we can begin to assess whether an intervention has a causal effect on an outcome. The foundation you’ve built here—understanding sampling distributions, standard errors, test statistics, and p-values—will carry forward directly to these more powerful tests. The transition is a relatively small mechanical extension, but it opens the door to answering causal questions."
  },
  {
    "objectID": "testing-mean-large#section-19",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "🎯 Key Takeaways",
    "text": "Standard error measures the precision of an estimator across repeated samples Hypothesis testing provides a formal framework for making decisions under uncertainty The Central Limit Theorem justifies using normal distributions for large samples P-values quantify how unusual our observed data would be if H_0 were true Small samples require the t-distribution to account for additional uncertainty Larger samples provide more reliable estimates and more powerful tests Always visualize your distributions to develop intuition about your tests"
  },
  {
    "objectID": "testing-mean-large#section-21",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 1: Understanding Standard Error",
    "text": "Suppose you have two samples from the same population: - Sample A: n = 25 , s = 15 - Sample B: n = 100 , s = 15 Which sample provides more precise estimates of the population mean? Calculate the standard error for each and explain the difference. Answer: Sample B provides more precise estimates. For Sample A: SE_A = 15/\\sqrt{25} = 15/5 = 3 For Sample B: SE_B = 15/\\sqrt{100} = 15/10 = 1.5 Sample B has a standard error half the size of Sample A, even though they have the same sample standard deviation. The fourfold increase in sample size (from 25 to 100) results in a twofold increase in precision (standard error is halved). This demonstrates the principle that larger samples yield more reliable estimates."
  },
  {
    "objectID": "testing-mean-large#section-22",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 2: Interpreting Test Statistics",
    "text": "A researcher calculates a test statistic of ζ = 2.5 for a large sample test. Explain what this value means in plain language, and describe where this value would fall on a standard normal distribution. Answer: A test statistic of ζ = 2.5 means that the observed sample mean is 2.5 standard errors above the hypothesized population mean under the null hypothesis. On a standard normal distribution (bell curve), this value falls in the right tail. Approximately 99.4% of the distribution lies to the left of 2.5 standard deviations from the mean, so only about 0.6% lies beyond this point. This suggests the observation would be quite unusual if the null hypothesis were true. For a one-sided test, the p-value would be approximately 0.006 or 0.6%, which would lead to rejection of the null at common significance levels."
  },
  {
    "objectID": "testing-mean-large#section-23",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 3: Large vs. Small Sample Tests",
    "text": "You conduct the same hypothesis test twice: once with a sample size of 100 and once with a sample size of 20. In both cases, you observe the same sample mean and sample standard deviation. Will the p-values be the same? Why or why not? Answer: No, the p-values will not be the same, and the small sample will generally produce a larger p-value. With n = 100 , we use the Z-distribution (or t-distribution with 99 df, which is virtually identical). The standard error will be relatively small: SE = s/\\sqrt{100} = s/10 . With n = 20 , we must use the t-distribution with 19 degrees of freedom, which has heavier tails than the normal. Additionally, the standard error will be larger: SE = s/\\sqrt{20} \\approx s/4.47 . Two effects compound: 1. The larger standard error makes the test statistic smaller 2. The t-distribution with few degrees of freedom makes any given test statistic less significant Both effects work against rejecting the null hypothesis, resulting in a larger p-value for the small sample test."
  },
  {
    "objectID": "testing-mean-large#section-24",
    "href": "/chapter/testing-mean-large",
    "title": "Testing a claim about a population mean",
    "section": "Question 4: Choosing Significance Levels",
    "text": "Explain why a researcher might choose a more stringent significance level (say, α = 0.01) rather than a more lenient one (say, α = 0.10). What are the tradeoffs involved? Answer: The choice of significance level involves a tradeoff between Type I and Type II errors: Choosing a more stringent α (like 0.01): - Benefit: Lower risk of Type I error (falsely rejecting a true null hypothesis) - Cost: Higher risk of Type II error (failing to reject a false null hypothesis) - When appropriate: When the cost of a false positive is high (e.g., approving an unsafe drug, implementing an expensive program that doesn’t work) Choosing a more lenient α (like 0.10): - Benefit: Lower risk of Type II error (more power to detect real effects) - Cost: Higher risk of Type I error - When appropriate: When we want to detect potentially important effects for further investigation, or when the cost of a false negative is high In fields where the consequences of a Type I error are severe (like medical research or engineering safety), researchers typically use stricter significance levels. In exploratory research or preliminary studies, more lenient levels might be acceptable."
  },
  {
    "objectID": "testing-mean-small",
    "href": "/chapter/testing-mean-small",
    "title": "Claim about a population mean (small sample)",
    "section": "",
    "text": "Claim about a population mean (small sample) This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-multiple-means",
    "href": "/chapter/testing-multiple-means",
    "title": "Claim about multiple population means",
    "section": "",
    "text": "Claim about multiple population means This chapter is currently under development. Content coming soon."
  },
  {
    "objectID": "testing-two-means",
    "href": "/chapter/testing-two-means",
    "title": "Claims about two population means",
    "section": "",
    "text": "Claims about two population means This chapter is currently under development. Content coming soon."
  }
]